from sqlalchemy import create_engine, text
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from datetime import datetime, timedelta
import psycopg2
import logging
import pandas as pd
import os
import sys
import importlib.util

# Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
BASE_URL = "https://irecommend.ru"
START_URL = "https://irecommend.ru/catalog/reviews/939-13393"  # Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ ÑÑ‚Ñƒ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ
TOTAL_PAGES = 2

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logging.basicConfig(level=logging.INFO)

def load_parser_module():
    """Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°"""
    try:
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ DAG
        parser_path = "/opt/airflow/parsers/irecommend_parser.py"
        
        logging.info(f"ğŸ” Ğ˜Ñ‰ĞµĞ¼ Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ¿Ğ¾ Ğ¿ÑƒÑ‚Ğ¸: {parser_path}")
        
        if not os.path.exists(parser_path):
            raise FileNotFoundError(f"Ğ¤Ğ°Ğ¹Ğ» Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {parser_path}")
        
        # Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ
        spec = importlib.util.spec_from_file_location("irecommend_parser", parser_path)
        parser_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(parser_module)
        
        logging.info("âœ… ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½")
        return parser_module
        
    except Exception as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°: {e}")
        raise

def collect_data(**kwargs):
    """Ğ¡Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² CSV"""
    try:
        ti = kwargs['ti']
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°
        parser_module = load_parser_module()
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°
        parser = parser_module.ReviewParser(BASE_URL)
        
        logging.info("ğŸ”„ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°...")
        
        # Ğ—ĞĞœĞ•ĞĞ˜Ğ¢Ğ• Ğ­Ğ¢Ğ£ Ğ¡Ğ¢Ğ ĞĞšĞ£: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ· Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°
        reviews = parser.scrape_reviews(start_url=START_URL, pages=TOTAL_PAGES)  # Ğ˜Ğ—ĞœĞ•ĞĞ•ĞĞ˜Ğ• Ğ—Ğ”Ğ•Ğ¡Ğ¬
        
        if not reviews:
            logging.warning("âš ï¸ ĞŸĞ°Ñ€ÑĞµÑ€ Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ")
            return "no_data_collected"
        
        logging.info(f"âœ… Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²: {len(reviews)}")
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² CSV
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"reviews_{timestamp}.csv"
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ğ¿Ğ°Ğ¿ĞºÑƒ dags/files (ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚)
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'files')
        os.makedirs(output_dir, exist_ok=True)
        
        csv_path = os.path.join(output_dir, csv_filename)
        
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°
        parser.save_to_csv(reviews, csv_path)
        
        logging.info(f"ğŸ’¾ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ²: {csv_path}")
        
        # ĞŸĞµÑ€ĞµĞ´Ğ°ĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ Ñ‡ĞµÑ€ĞµĞ· XCom
        ti.xcom_push(key='csv_path', value=csv_path)
        
        return f"collected_{len(reviews)}_reviews"
        
    except Exception as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
        raise

def init_db():
    """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    try:
        connection = psycopg2.connect(
            database='airflow', 
            user='airflow', 
            password='airflow',
            host='postgres',
            port=5432
        )
        logging.info("âœ… ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾")
        
        query = "SELECT * FROM parser.reviews LIMIT 10"
        df = pd.read_sql(sql=query, con=connection)
        logging.info(f"Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ‘Ğ”:\n{df}")
            
        return "database_connection_success"
        
    except Exception as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
        raise

def save_data_to_database(**kwargs):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    try:
        ti = kwargs['ti']
        csv_path = ti.xcom_pull(task_ids='collect_data', key='csv_path')
        
        if not csv_path or not os.path.exists(csv_path):
            logging.error("âŒ Ğ¤Ğ°Ğ¹Ğ» Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
            return "Ğ¤Ğ°Ğ¹Ğ» Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½"
            
        logging.info(f'ğŸ“ Ğ¤Ğ°Ğ¹Ğ» Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸: {csv_path}')

        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğº Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        engine = create_engine('postgresql+psycopg2://airflow:airflow@postgres:5432/airflow')
        
        # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· CSV
        data = pd.read_csv(csv_path)
        logging.info(f"ğŸ“Š Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· CSV ({len(data)} ÑÑ‚Ñ€Ğ¾Ğº):")
        logging.info(f"ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸: {list(data.columns)}")
        
        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        data['scraped_at'] = pd.to_datetime(data['scraped_at'])

        data['combined_data'] = pd.to_datetime(data['date_created'] + ' ' + data['time_created'])

        max_date_from_table = pd.read_sql(sql = "select max(combined_created) from parser.reviews", con = engine)

        max_date = max_date_from_table['max'].iloc[0]

        data = data[
            (data['combined_data'] > max_date)
        ]

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ğ±Ğ°Ğ·Ñƒ
        data.to_sql(
            'reviews', 
            engine, 
            schema='parser', 
            if_exists='append', 
            index=False,
            method='multi'
        )
        
        logging.info(f"âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {len(data)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ parser.raw_reviews")

        return f'Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {len(data)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² Ğ‘Ğ”'
        
    except Exception as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ‘Ğ”: {e}")
        return f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ‘Ğ”: {str(e)}"


with DAG(
    'parser_collect_data',
    description='ĞŸĞ°Ñ€ÑĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ iRecommend',
    schedule_interval='0 2 * * *',  # ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ² 02:00,
    catchup=False,
    start_date = datetime(2025, 10, 18),
    tags=['parsing', 'data_collection'],
) as dag:
    
    start_task = EmptyOperator(task_id='start')
    
    init_db_task = PythonOperator(
        task_id='init_db', 
        python_callable=init_db,
        execution_timeout=timedelta(minutes=5)
    )
    
    collect_data_task = PythonOperator(
        task_id='collect_data',
        python_callable=collect_data,
        execution_timeout=timedelta(minutes=30),  # ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ½ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ
        provide_context=True
    )
    
    insert_data_task = PythonOperator(
        task_id='insert_data',
        python_callable=save_data_to_database,
        provide_context=True
    )

    
    end_task = EmptyOperator(task_id='end')

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
    start_task >> init_db_task >> collect_data_task >> insert_data_task >> end_task