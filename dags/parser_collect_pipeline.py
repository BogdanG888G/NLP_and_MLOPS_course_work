from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from datetime import datetime, timedelta
import psycopg2
import logging
import pandas as pd
import os
import sys
import importlib.util
from sqlalchemy import create_engine

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
BASE_URL = "https://irecommend.ru"
START_URL = "https://irecommend.ru/catalog/reviews/939-13393"
TOTAL_PAGES = 1  # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logging.basicConfig(level=logging.INFO)

def load_parser_module():
    """Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°"""
    try:
        # ĞŸÑƒÑ‚ÑŒ Ğº Ğ¿Ğ°Ñ€ÑĞµÑ€Ñƒ Ğ² ĞºĞ¾Ñ€Ğ½Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° (Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ñ‹ÑˆĞµ dags)
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        parser_path = os.path.join(project_root, "parsers", "irecommend_parser.py")
        
        logging.info(f"ğŸ” Ğ˜Ñ‰ĞµĞ¼ Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ¿Ğ¾ Ğ¿ÑƒÑ‚Ğ¸: {parser_path}")
        
        if not os.path.exists(parser_path):
            # ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ - Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² ĞºĞ¾Ñ€Ğ½Ğµ
            parser_path = os.path.join(project_root, "irecommend_parser.py")
            logging.info(f"ğŸ” ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ: {parser_path}")
            
        if not os.path.exists(parser_path):
            raise FileNotFoundError(f"Ğ¤Ğ°Ğ¹Ğ» Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {parser_path}")
        
        # Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ
        spec = importlib.util.spec_from_file_location("irecommend_parser", parser_path)
        parser_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(parser_module)
        
        logging.info("âœ… ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½")
        return parser_module
        
    except Exception as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°: {e}")
        raise

def collect_data(**kwargs):
    """Ğ¡Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² CSV"""
    try:
        ti = kwargs['ti']
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°
        parser_module = load_parser_module()
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°
        parser = parser_module.ReviewParser(BASE_URL)
        
        logging.info(f"ğŸ”„ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ {START_URL}, ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {TOTAL_PAGES}")
        
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´ scrape_reviews Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²
        reviews = parser.scrape_reviews(start_url=START_URL, pages=TOTAL_PAGES)
        
        if not reviews:
            logging.warning("âš ï¸ ĞŸĞ°Ñ€ÑĞµÑ€ Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ")
            return "no_data_collected"
        
        logging.info(f"âœ… Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²: {len(reviews)}")
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² CSV
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"reviews_{timestamp}.csv"
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ğ¿Ğ°Ğ¿ĞºÑƒ dags/files (ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚)
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'files')
        os.makedirs(output_dir, exist_ok=True)
        
        csv_path = os.path.join(output_dir, csv_filename)
        
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°
        success = parser.save_to_csv(reviews, csv_path)
        
        if success:
            logging.info(f"ğŸ’¾ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ²: {csv_path}")
            
            # ĞŸĞµÑ€ĞµĞ´Ğ°ĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ Ñ‡ĞµÑ€ĞµĞ· XCom
            ti.xcom_push(key='csv_path', value=csv_path)
            
            return f"collected_{len(reviews)}_reviews"
        else:
            logging.error("âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ² CSV")
            return "csv_save_failed"
        
    except Exception as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
        raise


def save_data_to_database(**kwargs):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    try:
        ti = kwargs['ti']
        csv_path = ti.xcom_pull(task_ids='collect_data', key='csv_path')
        
        if not csv_path or not os.path.exists(csv_path):
            logging.error("âŒ Ğ¤Ğ°Ğ¹Ğ» Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
            return "data_file_not_found"
            
        logging.info(f'ğŸ“ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°: {csv_path}')

        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğº Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        engine = create_engine('postgresql+psycopg2://airflow:airflow@postgres:5432/airflow')
        
        # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· CSV
        data = pd.read_csv(csv_path, encoding='utf-8')
        logging.info(f"ğŸ“Š Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(data)} ÑÑ‚Ñ€Ğ¾Ğº Ğ¸Ğ· CSV")
        logging.info(f"ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸: {list(data.columns)}")
        
        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        data['scraped_at'] = pd.to_datetime(data['scraped_at'], errors='coerce')
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ combined_created Ğ¸Ğ· date_created Ğ¸ time_created
        if 'date_created' in data.columns and 'time_created' in data.columns:
            data['combined_created'] = pd.to_datetime(
                data['date_created'] + ' ' + data['time_created'], 
                errors='coerce',
                format='%d.%m.%Y %H:%M'
            )
        else:
            data['combined_created'] = pd.NaT
        '''
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ°Ñ‚Ñƒ Ğ¸Ğ· Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            max_date_query = "SELECT MAX(combined_created) as max_date FROM parser.reviews"
            max_date_from_table = pd.read_sql(sql=max_date_query, con=engine)
            max_date = max_date_from_table['max_date'].iloc[0]
            
            logging.info(f"ğŸ“… ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ° Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ: {max_date}")
            
            # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            if max_date is not pd.NaT and max_date is not None:
                data = data[data['combined_created'] > max_date]
                logging.info(f"ğŸ†• ĞĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: {len(data)}")
            else:
                logging.info("ğŸ¯ Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ¿ÑƒÑÑ‚Ğ°Ñ Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ° Ğ½Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ°, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²ÑĞµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸")
        except Exception as e:
            logging.warning(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ°Ñ‚Ñ‹: {e}")
            logging.info("ğŸ”„ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²ÑĞµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸")
        
        if len(data) == 0:
            logging.info("â„¹ï¸ ĞĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ")
            return "no_new_data"'''
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ğ±Ğ°Ğ·Ñƒ
        data.to_sql(
            'reviews', 
            engine, 
            schema='parser', 
            if_exists='append', 
            index=False
        )
        
        logging.info(f"âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {len(data)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ parser.reviews")
        
        # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»
        try:
            os.remove(csv_path)
            logging.info(f"ğŸ§¹ Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ» {csv_path} ÑƒĞ´Ğ°Ğ»ĞµĞ½")
        except:
            logging.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ» {csv_path}")
        
        return f'saved_{len(data)}_records_to_db'
        
    except Exception as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ‘Ğ”: {e}")
        return f"db_save_error: {str(e)}"

with DAG(
    'irecommend_parser_dag',
    start_date=datetime(2025, 1, 18),
    description='ĞŸĞ°Ñ€ÑĞµÑ€ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ñ iRecommend',
    schedule_interval='0 2 * * *',  
    catchup=False,
    tags=['parsing', 'irecommend', 'reviews'],
) as dag:
    
    start_task = EmptyOperator(task_id='start')
    
    collect_data_task = PythonOperator(
        task_id='collect_data',
        python_callable=collect_data,
        execution_timeout=timedelta(minutes=30),
        provide_context=True
    )
    
    insert_data_task = PythonOperator(
        task_id='insert_data',
        python_callable=save_data_to_database,
        provide_context=True
    )
    
    end_task = EmptyOperator(task_id='end')

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
    start_task >> collect_data_task >> insert_data_task >> end_task