{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152b6ce7",
   "metadata": {},
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "019791d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Smart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Smart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pymorphy3\n",
    "import re\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "from functools import lru_cache\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SpatialDropout1D, LSTM, BatchNormalization, Dense, Bidirectional, Embedding, Dropout\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac263a",
   "metadata": {},
   "source": [
    "# –°—á–∏—Ç—ã–≤–∞–Ω–∏–µ —Å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eefafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql+psycopg2://airflow:airflow@localhost:5433/airflow')\n",
    "\n",
    "query = 'select * from parser.reviews where id > 113'\n",
    "df = pd.read_sql(sql=query, con=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78795b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>combined_created</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>experience</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>verdict</th>\n",
       "      <th>review_url</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Dr.Diesel –°lassic –ì–æ—Ä—è—á–∏–π —Å—ã—Ä</td>\n",
       "      <td>Pacha1985</td>\n",
       "      <td>5</td>\n",
       "      <td>29.07.2025</td>\n",
       "      <td>09:50</td>\n",
       "      <td>2025-07-29 09:50:00</td>\n",
       "      <td>–†–£–°–°–ö–ê–† —Ä–µ—à–∏–ª —Å–º–µ–Ω–∏—Ç—å –º–∞—Å–∫—É –ø–æ –¥–∏–∑–∞–π–Ω—É –≤—ã—Å—Ç–∞–≤–ª...</td>\n",
       "      <td>–ö–∞–∫ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ —á—Ç–æ –≤ —Å–∫–æ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ ...</td>\n",
       "      <td>–ö–∞–∫ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ —á—Ç–æ –≤ —Å–∫–æ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ ...</td>\n",
       "      <td>None</td>\n",
       "      <td>–í–∫—É—Å–Ω–æ | –í—ã–≥–ª—è–¥—è—Ç —Å–∏–º–ø–∞—Ç–∏—á–Ω–æ</td>\n",
       "      <td>None</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>https://irecommend.ru/content/russkar-reshil-s...</td>\n",
       "      <td>2025-10-30 07:39:13</td>\n",
       "      <td>2025-10-30 07:42:04.616239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays ¬´–ó–∞–ø–µ—á—ë–Ω–Ω—ã–π –∫–∞–º–∞–º–±–µ—Ä —Å...</td>\n",
       "      <td>Inessa_Pavlovna</td>\n",
       "      <td>4</td>\n",
       "      <td>28.07.2025</td>\n",
       "      <td>10:41</td>\n",
       "      <td>2025-07-28 10:41:00</td>\n",
       "      <td>–ù–∞ –∑–∞–ø–µ—á–µ–Ω–Ω—ã–π –∫–∞–º–∞–º–±–µ—Ä —Å –≥—Ä—É—à–µ–π –Ω–µ –ø–æ—Ö–æ–∂–µ, –Ω–æ ...</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç –≤—Å–µ–º! –ü–æ–ø—Ä–æ–±–æ–≤–∞–ª–∞ –Ω–æ–≤–∏–Ω–∫—É –õ–µ–π—Å –∑–∞–ø–µ—á–µ–Ω—ã...</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç –≤—Å–µ–º!\\n–ü–æ–ø—Ä–æ–±–æ–≤–∞–ª–∞ –Ω–æ–≤–∏–Ω–∫—É –õ–µ–π—Å –∑–∞–ø–µ—á–µ–Ω...</td>\n",
       "      <td>None</td>\n",
       "      <td>–ê–ø–ø–µ—Ç–∏—Ç–Ω—ã–µ –∏ —Ö—Ä—É—Å—Ç—è—â–∏–µ | –ù–µ–∂–Ω—ã–π –≤–∫—É—Å | –ù–µ—Ç –Ω–µ–ø...</td>\n",
       "      <td>–ó–∞—è–≤–ª–µ–Ω–Ω—ã–π –≤–∫—É—Å –Ω–µ –≤—ã—Ä–∞–∂–µ–Ω | –ù–µ—Ç –≥—Ä—É—à–∏ | –¶–µ–Ω–∞</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>https://irecommend.ru/content/na-zamechennyi-k...</td>\n",
       "      <td>2025-10-30 07:39:13</td>\n",
       "      <td>2025-10-30 07:42:04.616239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>–ß–∏–ø—Å—ã –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å  ¬´–†—É—Å—Å–∫–∞—è –ö–∞—Ä—Ç–æ—à–∫–∞¬ª...</td>\n",
       "      <td>Ommyaak</td>\n",
       "      <td>5</td>\n",
       "      <td>28.07.2025</td>\n",
       "      <td>09:49</td>\n",
       "      <td>2025-07-28 09:49:00</td>\n",
       "      <td>–ß–∏–ø—Å—ã ¬´–†—É—Å—Å–∫–∞—è –ö–∞—Ä—Ç–æ—à–∫–∞¬ª ‚Äî –∫–æ–≥–¥–∞ —Ö–æ—á–µ—Ç—Å—è —á–µ–≥–æ-...</td>\n",
       "      <td>None</td>\n",
       "      <td>–î–æ–±—Ä—ã–π –¥–µ–Ω—å, –¥–æ—Ä–æ–≥–∏–µ —á–∏—Ç–∞—Ç–µ–ª–∏!\\n–í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º ...</td>\n",
       "      <td>–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑</td>\n",
       "      <td>–í –ø–∞—á–∫–µ –º–Ω–æ–≥–æ —á–∏–ø—Å–æ–≤ | –í–∫—É—Å –∫—Ä–∞–±–∞ | –í–∫—É—Å–Ω–æ | –í...</td>\n",
       "      <td>None</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>https://irecommend.ru/content/chipsy-russkaya-...</td>\n",
       "      <td>2025-10-30 07:39:13</td>\n",
       "      <td>2025-10-30 07:42:04.616239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ –ú–æ—Å–∫–∞—Ä—Ç PopTato Grilled rib...</td>\n",
       "      <td>–õ–∏—Å–µ–Ω–æ–∫99</td>\n",
       "      <td>4</td>\n",
       "      <td>27.07.2025</td>\n",
       "      <td>16:38</td>\n",
       "      <td>2025-07-27 16:38:00</td>\n",
       "      <td>–ß–∏–ø—Å—ã —Å –Ω–µ–æ–±—ã—á–Ω—ã–º –≤–∫—É—Å–æ–º \"–†—ë–±–µ—Ä –≤ –≥–ª–∞–∑—É—Ä–∏ –∏ –ø–µ...</td>\n",
       "      <td>–í—Å–µ–º –¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä –°–µ–≥–æ–¥–Ω—è —è —Ö–æ—á—É –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –æ—Ç...</td>\n",
       "      <td>–í—Å–µ–º –¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä üëãüèª\\n–°–µ–≥–æ–¥–Ω—è —è —Ö–æ—á—É –ø–æ–¥–µ–ª–∏—Ç—å—Å...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>https://irecommend.ru/content/chipsy-s-neobych...</td>\n",
       "      <td>2025-10-30 07:39:13</td>\n",
       "      <td>2025-10-30 07:42:04.616239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Bruto Snacks ‚Äú–ß—ë—Ä–Ω—ã–π –ø–µ—Ä–µ—Ü ...</td>\n",
       "      <td>Nikita_fighter for justice</td>\n",
       "      <td>4</td>\n",
       "      <td>26.07.2025</td>\n",
       "      <td>10:53</td>\n",
       "      <td>2025-07-26 10:53:00</td>\n",
       "      <td>üåøüçüü•§ –ß–∏–ø—Å—ã –±–µ–∑ –∫–æ–ª—ã - –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä!! 100% –Ω–∞...</td>\n",
       "      <td>–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –¥–æ—Ä–æ–≥–∏–µ —á–∏—Ç–∞—Ç–µ–ª–∏!</td>\n",
       "      <td>–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –¥–æ—Ä–æ–≥–∏–µ —á–∏—Ç–∞—Ç–µ–ª–∏! üåû\\n–°–µ–≥–æ–¥–Ω—è —Ä–∞—Å...</td>\n",
       "      <td>–æ–¥–∏–Ω —Ä–∞–∑</td>\n",
       "      <td>–ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤–∫—É—Å–∞ | –ö—Ä–∞—Ñ—Ç–æ–≤–∞—è —É–ø...</td>\n",
       "      <td>None</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>https://irecommend.ru/content/chipsy-bez-koly-...</td>\n",
       "      <td>2025-10-30 07:39:13</td>\n",
       "      <td>2025-10-30 07:42:04.616239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>119</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Fix price \"–ë–µ–∫–æ–Ω—á–∏–∫–∏\"</td>\n",
       "      <td>ps2008_com</td>\n",
       "      <td>1</td>\n",
       "      <td>25.07.2025</td>\n",
       "      <td>21:57</td>\n",
       "      <td>2025-07-25 21:57:00</td>\n",
       "      <td>–ö–æ—à–º–∞—Ä</td>\n",
       "      <td>–ö—É–ø–∏–ª–∏ —ç—Ç–∏ —á–∏–ø—Å—ã –≤ –§–∏–∫—Å –ü—Ä–∞–π—Å–µ –≤ –ö—É—Ä—Å–∫–µ. –û—Ç–∫—Ä—ã...</td>\n",
       "      <td>–ö—É–ø–∏–ª–∏ —ç—Ç–∏ —á–∏–ø—Å—ã –≤ –§–∏–∫—Å –ü—Ä–∞–π—Å–µ –≤ –ö—É—Ä—Å–∫–µ. –û—Ç–∫—Ä—ã...</td>\n",
       "      <td>–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑</td>\n",
       "      <td>None</td>\n",
       "      <td>–í—Ç–æ—Ä–∞—è –∫—É–ø–ª–µ–Ω–Ω–∞—è –ø–∞—á–∫–∞ –Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–∞—Å—å | –ó–∞–ø–∞—Ö ...</td>\n",
       "      <td>–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>https://irecommend.ru/content/koshmar-n10864194</td>\n",
       "      <td>2025-10-30 07:39:13</td>\n",
       "      <td>2025-10-30 07:42:04.616239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>120</td>\n",
       "      <td>–ß–∏–ø—Å—ã –û–Ω–µ–≥–∞ Just Brutal –ü–µ—Ä–µ—Ü —Ö–∞–ª–∞–ø–µ–Ω—å–æ –∏ —Å—ã—Ä</td>\n",
       "      <td>Travel Bears</td>\n",
       "      <td>3</td>\n",
       "      <td>25.07.2025</td>\n",
       "      <td>16:26</td>\n",
       "      <td>2025-07-25 16:26:00</td>\n",
       "      <td>–û—á–µ–Ω—å –æ—Å—Ç—Ä—ã–µ —á–∏–ø—Å—ã, –Ω–æ –ª—é–±–∏—Ç–µ–ª—è–º –ø–æ—Ö—Ä—É—Å—Ç–µ—Ç—å —Å ...</td>\n",
       "      <td>–°–Ω–∞—á–∞–ª–∞ —è –ø–æ–¥—É–º–∞–ª–∞: ¬´–ù–∞–∑–≤–∞–Ω–∏–µ-—Ç–æ –∫–∞–∫–æ–µ –ø—Ä–∏–¥—É–º–∞...</td>\n",
       "      <td>–°–Ω–∞—á–∞–ª–∞ —è –ø–æ–¥—É–º–∞–ª–∞: ¬´–ù–∞–∑–≤–∞–Ω–∏–µ-—Ç–æ –∫–∞–∫–æ–µ –ø—Ä–∏–¥—É–º–∞...</td>\n",
       "      <td>–æ–¥–∏–Ω —Ä–∞–∑</td>\n",
       "      <td>–ù–µ –∂–∏—Ä–Ω—ã–µ | –ü–æ—á—Ç–∏ –Ω–µ –±—ã–ª–æ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö –≤ –ø–∞—á–∫–µ</td>\n",
       "      <td>–ù–∏—á–µ–≥–æ —Å—ã—Ä–Ω–æ–≥–æ –Ω–µ—Ç –Ω–∏ –≤–æ –≤–∫—É—Å–µ, –Ω–∏ –≤ –∑–∞–ø–∞—Ö–µ | ...</td>\n",
       "      <td>–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>https://irecommend.ru/content/ochen-ostrye-chi...</td>\n",
       "      <td>2025-10-30 07:39:13</td>\n",
       "      <td>2025-10-30 07:42:04.616239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                       product_name  \\\n",
       "0  114   –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Dr.Diesel –°lassic –ì–æ—Ä—è—á–∏–π —Å—ã—Ä   \n",
       "1  115  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays ¬´–ó–∞–ø–µ—á—ë–Ω–Ω—ã–π –∫–∞–º–∞–º–±–µ—Ä —Å...   \n",
       "2  116  –ß–∏–ø—Å—ã –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å  ¬´–†—É—Å—Å–∫–∞—è –ö–∞—Ä—Ç–æ—à–∫–∞¬ª...   \n",
       "3  117  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ –ú–æ—Å–∫–∞—Ä—Ç PopTato Grilled rib...   \n",
       "4  118  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Bruto Snacks ‚Äú–ß—ë—Ä–Ω—ã–π –ø–µ—Ä–µ—Ü ...   \n",
       "5  119           –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Fix price \"–ë–µ–∫–æ–Ω—á–∏–∫–∏\"   \n",
       "6  120      –ß–∏–ø—Å—ã –û–Ω–µ–≥–∞ Just Brutal –ü–µ—Ä–µ—Ü —Ö–∞–ª–∞–ø–µ–Ω—å–æ –∏ —Å—ã—Ä   \n",
       "\n",
       "                       author  rating date_created time_created  \\\n",
       "0                   Pacha1985       5   29.07.2025        09:50   \n",
       "1             Inessa_Pavlovna       4   28.07.2025        10:41   \n",
       "2                     Ommyaak       5   28.07.2025        09:49   \n",
       "3                   –õ–∏—Å–µ–Ω–æ–∫99       4   27.07.2025        16:38   \n",
       "4  Nikita_fighter for justice       4   26.07.2025        10:53   \n",
       "5                  ps2008_com       1   25.07.2025        21:57   \n",
       "6                Travel Bears       3   25.07.2025        16:26   \n",
       "\n",
       "     combined_created                                              title  \\\n",
       "0 2025-07-29 09:50:00  –†–£–°–°–ö–ê–† —Ä–µ—à–∏–ª —Å–º–µ–Ω–∏—Ç—å –º–∞—Å–∫—É –ø–æ –¥–∏–∑–∞–π–Ω—É –≤—ã—Å—Ç–∞–≤–ª...   \n",
       "1 2025-07-28 10:41:00  –ù–∞ –∑–∞–ø–µ—á–µ–Ω–Ω—ã–π –∫–∞–º–∞–º–±–µ—Ä —Å –≥—Ä—É—à–µ–π –Ω–µ –ø–æ—Ö–æ–∂–µ, –Ω–æ ...   \n",
       "2 2025-07-28 09:49:00  –ß–∏–ø—Å—ã ¬´–†—É—Å—Å–∫–∞—è –ö–∞—Ä—Ç–æ—à–∫–∞¬ª ‚Äî –∫–æ–≥–¥–∞ —Ö–æ—á–µ—Ç—Å—è —á–µ–≥–æ-...   \n",
       "3 2025-07-27 16:38:00  –ß–∏–ø—Å—ã —Å –Ω–µ–æ–±—ã—á–Ω—ã–º –≤–∫—É—Å–æ–º \"–†—ë–±–µ—Ä –≤ –≥–ª–∞–∑—É—Ä–∏ –∏ –ø–µ...   \n",
       "4 2025-07-26 10:53:00  üåøüçüü•§ –ß–∏–ø—Å—ã –±–µ–∑ –∫–æ–ª—ã - –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä!! 100% –Ω–∞...   \n",
       "5 2025-07-25 21:57:00                                             –ö–æ—à–º–∞—Ä   \n",
       "6 2025-07-25 16:26:00  –û—á–µ–Ω—å –æ—Å—Ç—Ä—ã–µ —á–∏–ø—Å—ã, –Ω–æ –ª—é–±–∏—Ç–µ–ª—è–º –ø–æ—Ö—Ä—É—Å—Ç–µ—Ç—å —Å ...   \n",
       "\n",
       "                                         teaser_text  \\\n",
       "0  –ö–∞–∫ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ —á—Ç–æ –≤ —Å–∫–æ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ ...   \n",
       "1  –ü—Ä–∏–≤–µ—Ç –≤—Å–µ–º! –ü–æ–ø—Ä–æ–±–æ–≤–∞–ª–∞ –Ω–æ–≤–∏–Ω–∫—É –õ–µ–π—Å –∑–∞–ø–µ—á–µ–Ω—ã...   \n",
       "2                                               None   \n",
       "3  –í—Å–µ–º –¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä –°–µ–≥–æ–¥–Ω—è —è —Ö–æ—á—É –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –æ—Ç...   \n",
       "4                    –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –¥–æ—Ä–æ–≥–∏–µ —á–∏—Ç–∞—Ç–µ–ª–∏!   \n",
       "5  –ö—É–ø–∏–ª–∏ —ç—Ç–∏ —á–∏–ø—Å—ã –≤ –§–∏–∫—Å –ü—Ä–∞–π—Å–µ –≤ –ö—É—Ä—Å–∫–µ. –û—Ç–∫—Ä—ã...   \n",
       "6  –°–Ω–∞—á–∞–ª–∞ —è –ø–æ–¥—É–º–∞–ª–∞: ¬´–ù–∞–∑–≤–∞–Ω–∏–µ-—Ç–æ –∫–∞–∫–æ–µ –ø—Ä–∏–¥—É–º–∞...   \n",
       "\n",
       "                                           full_text     experience  \\\n",
       "0  –ö–∞–∫ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ —á—Ç–æ –≤ —Å–∫–æ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ ...           None   \n",
       "1  –ü—Ä–∏–≤–µ—Ç –≤—Å–µ–º!\\n–ü–æ–ø—Ä–æ–±–æ–≤–∞–ª–∞ –Ω–æ–≤–∏–Ω–∫—É –õ–µ–π—Å –∑–∞–ø–µ—á–µ–Ω...           None   \n",
       "2  –î–æ–±—Ä—ã–π –¥–µ–Ω—å, –¥–æ—Ä–æ–≥–∏–µ —á–∏—Ç–∞—Ç–µ–ª–∏!\\n–í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º ...  –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑   \n",
       "3  –í—Å–µ–º –¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä üëãüèª\\n–°–µ–≥–æ–¥–Ω—è —è —Ö–æ—á—É –ø–æ–¥–µ–ª–∏—Ç—å—Å...           None   \n",
       "4  –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –¥–æ—Ä–æ–≥–∏–µ —á–∏—Ç–∞—Ç–µ–ª–∏! üåû\\n–°–µ–≥–æ–¥–Ω—è —Ä–∞—Å...       –æ–¥–∏–Ω —Ä–∞–∑   \n",
       "5  –ö—É–ø–∏–ª–∏ —ç—Ç–∏ —á–∏–ø—Å—ã –≤ –§–∏–∫—Å –ü—Ä–∞–π—Å–µ –≤ –ö—É—Ä—Å–∫–µ. –û—Ç–∫—Ä—ã...  –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑   \n",
       "6  –°–Ω–∞—á–∞–ª–∞ —è –ø–æ–¥—É–º–∞–ª–∞: ¬´–ù–∞–∑–≤–∞–Ω–∏–µ-—Ç–æ –∫–∞–∫–æ–µ –ø—Ä–∏–¥—É–º–∞...       –æ–¥–∏–Ω —Ä–∞–∑   \n",
       "\n",
       "                                              pluses  \\\n",
       "0                       –í–∫—É—Å–Ω–æ | –í—ã–≥–ª—è–¥—è—Ç —Å–∏–º–ø–∞—Ç–∏—á–Ω–æ   \n",
       "1  –ê–ø–ø–µ—Ç–∏—Ç–Ω—ã–µ –∏ —Ö—Ä—É—Å—Ç—è—â–∏–µ | –ù–µ–∂–Ω—ã–π –≤–∫—É—Å | –ù–µ—Ç –Ω–µ–ø...   \n",
       "2  –í –ø–∞—á–∫–µ –º–Ω–æ–≥–æ —á–∏–ø—Å–æ–≤ | –í–∫—É—Å –∫—Ä–∞–±–∞ | –í–∫—É—Å–Ω–æ | –í...   \n",
       "3                                               None   \n",
       "4  –ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤–∫—É—Å–∞ | –ö—Ä–∞—Ñ—Ç–æ–≤–∞—è —É–ø...   \n",
       "5                                               None   \n",
       "6        –ù–µ –∂–∏—Ä–Ω—ã–µ | –ü–æ—á—Ç–∏ –Ω–µ –±—ã–ª–æ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö –≤ –ø–∞—á–∫–µ   \n",
       "\n",
       "                                             minuses         verdict  \\\n",
       "0                                               None     —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç   \n",
       "1      –ó–∞—è–≤–ª–µ–Ω–Ω—ã–π –≤–∫—É—Å –Ω–µ –≤—ã—Ä–∞–∂–µ–Ω | –ù–µ—Ç –≥—Ä—É—à–∏ | –¶–µ–Ω–∞     —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç   \n",
       "2                                               None     —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç   \n",
       "3                                               None     —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç   \n",
       "4                                               None     —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç   \n",
       "5  –í—Ç–æ—Ä–∞—è –∫—É–ø–ª–µ–Ω–Ω–∞—è –ø–∞—á–∫–∞ –Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–∞—Å—å | –ó–∞–ø–∞—Ö ...  –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç   \n",
       "6  –ù–∏—á–µ–≥–æ —Å—ã—Ä–Ω–æ–≥–æ –Ω–µ—Ç –Ω–∏ –≤–æ –≤–∫—É—Å–µ, –Ω–∏ –≤ –∑–∞–ø–∞—Ö–µ | ...  –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç   \n",
       "\n",
       "                                          review_url          scraped_at  \\\n",
       "0  https://irecommend.ru/content/russkar-reshil-s... 2025-10-30 07:39:13   \n",
       "1  https://irecommend.ru/content/na-zamechennyi-k... 2025-10-30 07:39:13   \n",
       "2  https://irecommend.ru/content/chipsy-russkaya-... 2025-10-30 07:39:13   \n",
       "3  https://irecommend.ru/content/chipsy-s-neobych... 2025-10-30 07:39:13   \n",
       "4  https://irecommend.ru/content/chipsy-bez-koly-... 2025-10-30 07:39:13   \n",
       "5    https://irecommend.ru/content/koshmar-n10864194 2025-10-30 07:39:13   \n",
       "6  https://irecommend.ru/content/ochen-ostrye-chi... 2025-10-30 07:39:13   \n",
       "\n",
       "                  created_at  \n",
       "0 2025-10-30 07:42:04.616239  \n",
       "1 2025-10-30 07:42:04.616239  \n",
       "2 2025-10-30 07:42:04.616239  \n",
       "3 2025-10-30 07:42:04.616239  \n",
       "4 2025-10-30 07:42:04.616239  \n",
       "5 2025-10-30 07:42:04.616239  \n",
       "6 2025-10-30 07:42:04.616239  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9befa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –§–∞–π–ª label_studio_import.json —Å–æ–∑–¥–∞–Ω\n",
      "üìÑ –ó–∞–¥–∞—á –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏: 99\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è Label Studio\n",
    "def prepare_label_studio_data(df, filename='label_studio_import.json'):\n",
    "    tasks = []\n",
    "    \n",
    "    for text in df['full_text']:\n",
    "        if pd.notna(text) and str(text).strip():\n",
    "            task = {\n",
    "                \"data\": {\n",
    "                    \"text\": str(text).strip()\n",
    "                }\n",
    "            }\n",
    "            tasks.append(task)\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tasks, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ –§–∞–π–ª {filename} —Å–æ–∑–¥–∞–Ω\")\n",
    "    print(f\"üìÑ –ó–∞–¥–∞—á –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏: {len(tasks)}\")\n",
    "    return tasks\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞\n",
    "tasks = prepare_label_studio_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2aebf",
   "metadata": {},
   "source": [
    "# –°—á–∏—Ç—ã–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ LabelStudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b43698a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>aspect_sentiment</th>\n",
       "      <th>annotator</th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>lead_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>133</td>\n",
       "      <td>[{'start': 70, 'end': 95, 'text': '–≤–∫—É—Å –±—ã–ª —Ä–µ...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-10-29 07:30:59.870298+00:00</td>\n",
       "      <td>2025-10-29 07:30:59.870298+00:00</td>\n",
       "      <td>260.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö\\nüëã\\n–ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª...</td>\n",
       "      <td>134</td>\n",
       "      <td>[{'start': 0, 'end': 16, 'text': '–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é ...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2025-10-29 07:42:58.168204+00:00</td>\n",
       "      <td>2025-10-29 07:42:58.168204+00:00</td>\n",
       "      <td>711.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–í—Å–µ—Ö –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é!\\n–°–µ–≥–æ–¥–Ω—è —É –º–µ–Ω—è\\n–≤ –æ—Ç–∑—ã–≤–µ —á–∏...</td>\n",
       "      <td>135</td>\n",
       "      <td>[{'start': 0, 'end': 17, 'text': '–í—Å–µ—Ö –ø—Ä–∏–≤–µ—Ç—Å...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2025-10-29 07:57:24.798896+00:00</td>\n",
       "      <td>2025-10-29 07:57:24.798896+00:00</td>\n",
       "      <td>865.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–í—Å–µ–º –¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä üëãüèª\\n–°–µ–≥–æ–¥–Ω—è —è —Ö–æ—á—É –ø–æ–¥–µ–ª–∏—Ç—å—Å...</td>\n",
       "      <td>136</td>\n",
       "      <td>[{'start': 105, 'end': 161, 'text': '–ù–æ–≤–∏–Ω–∫—É –æ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-10-29 08:03:21.322228+00:00</td>\n",
       "      <td>2025-10-29 08:03:21.322228+00:00</td>\n",
       "      <td>355.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö –∑–∞–≥–ª—è–Ω—É–≤—à–∏—Ö!\\n–ó–∏–º–∞ –µ—â–µ –Ω–µ –Ω–∞—Å...</td>\n",
       "      <td>137</td>\n",
       "      <td>[{'start': 30, 'end': 135, 'text': '–ó–∏–º–∞ –µ—â–µ –Ω...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2025-10-29 08:13:05.088957+00:00</td>\n",
       "      <td>2025-10-29 08:13:05.088957+00:00</td>\n",
       "      <td>582.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Å–æ –≤–∫—É—Å–æ–º...</td>\n",
       "      <td>274</td>\n",
       "      <td>[{'start': 178, 'end': 245, 'text': '–¶–µ–Ω–∞ –¥–∞–ª–µ...</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>2025-11-06 11:01:50.629915+00:00</td>\n",
       "      <td>2025-11-06 11:01:50.629915+00:00</td>\n",
       "      <td>96.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Big Bon Snack Box –°—ã—Ä —Å —Ö—Ä—É...</td>\n",
       "      <td>275</td>\n",
       "      <td>[{'start': 0, 'end': 223, 'text': '–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ...</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>2025-11-06 11:04:09.268267+00:00</td>\n",
       "      <td>2025-11-06 11:04:09.268267+00:00</td>\n",
       "      <td>137.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>–í—Å–µ–º –¥–æ–±—Ä–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫!\\n–Ø —Ä–µ–¥–∫–æ –ø–æ–∫—É–ø–∞—é —á...</td>\n",
       "      <td>276</td>\n",
       "      <td>[{'start': 258, 'end': 320, 'text': '–Ø –Ω–µ –æ—à–∏–±...</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>2025-11-06 11:08:12.838134+00:00</td>\n",
       "      <td>2025-11-06 11:08:12.838134+00:00</td>\n",
       "      <td>206.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å –Ω—Ä–∞–≤–∏—Ç—Å—è –º–Ω–µ —Å–≤–æ–∏–º —Ä–∞–∑–Ω–æ–æ...</td>\n",
       "      <td>277</td>\n",
       "      <td>[{'start': 0, 'end': 53, 'text': '–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫...</td>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>2025-11-06 11:10:52.420490+00:00</td>\n",
       "      <td>2025-11-06 11:10:52.420490+00:00</td>\n",
       "      <td>158.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö –∑–∞–≥–ª—è–Ω—É–≤—à–∏—Ö\\n–ß–∞—Å—Ç–æ —Ö–æ–∂—É –∑–∞ –ø—Ä...</td>\n",
       "      <td>278</td>\n",
       "      <td>[{'start': 0, 'end': 171, 'text': '–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é...</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>2025-11-06 11:15:36.556914+00:00</td>\n",
       "      <td>2025-11-06 11:15:36.556914+00:00</td>\n",
       "      <td>282.973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   id  \\\n",
       "0   –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...  133   \n",
       "1   –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö\\nüëã\\n–ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª...  134   \n",
       "2   –í—Å–µ—Ö –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é!\\n–°–µ–≥–æ–¥–Ω—è —É –º–µ–Ω—è\\n–≤ –æ—Ç–∑—ã–≤–µ —á–∏...  135   \n",
       "3   –í—Å–µ–º –¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä üëãüèª\\n–°–µ–≥–æ–¥–Ω—è —è —Ö–æ—á—É –ø–æ–¥–µ–ª–∏—Ç—å—Å...  136   \n",
       "4   –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö –∑–∞–≥–ª—è–Ω—É–≤—à–∏—Ö!\\n–ó–∏–º–∞ –µ—â–µ –Ω–µ –Ω–∞—Å...  137   \n",
       "..                                                ...  ...   \n",
       "29  –ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Å–æ –≤–∫—É—Å–æ–º...  274   \n",
       "30  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Big Bon Snack Box –°—ã—Ä —Å —Ö—Ä—É...  275   \n",
       "31  –í—Å–µ–º –¥–æ–±—Ä–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫!\\n–Ø —Ä–µ–¥–∫–æ –ø–æ–∫—É–ø–∞—é —á...  276   \n",
       "32  –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å –Ω—Ä–∞–≤–∏—Ç—Å—è –º–Ω–µ —Å–≤–æ–∏–º —Ä–∞–∑–Ω–æ–æ...  277   \n",
       "33  –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö –∑–∞–≥–ª—è–Ω—É–≤—à–∏—Ö\\n–ß–∞—Å—Ç–æ —Ö–æ–∂—É –∑–∞ –ø—Ä...  278   \n",
       "\n",
       "                                     aspect_sentiment  annotator  \\\n",
       "0   [{'start': 70, 'end': 95, 'text': '–≤–∫—É—Å –±—ã–ª —Ä–µ...          1   \n",
       "1   [{'start': 0, 'end': 16, 'text': '–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é ...          1   \n",
       "2   [{'start': 0, 'end': 17, 'text': '–í—Å–µ—Ö –ø—Ä–∏–≤–µ—Ç—Å...          1   \n",
       "3   [{'start': 105, 'end': 161, 'text': '–ù–æ–≤–∏–Ω–∫—É –æ...          1   \n",
       "4   [{'start': 30, 'end': 135, 'text': '–ó–∏–º–∞ –µ—â–µ –Ω...          1   \n",
       "..                                                ...        ...   \n",
       "29  [{'start': 178, 'end': 245, 'text': '–¶–µ–Ω–∞ –¥–∞–ª–µ...          1   \n",
       "30  [{'start': 0, 'end': 223, 'text': '–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ...          1   \n",
       "31  [{'start': 258, 'end': 320, 'text': '–Ø –Ω–µ –æ—à–∏–±...          1   \n",
       "32  [{'start': 0, 'end': 53, 'text': '–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫...          1   \n",
       "33  [{'start': 0, 'end': 171, 'text': '–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é...          1   \n",
       "\n",
       "    annotation_id                       created_at  \\\n",
       "0               7 2025-10-29 07:30:59.870298+00:00   \n",
       "1               8 2025-10-29 07:42:58.168204+00:00   \n",
       "2               9 2025-10-29 07:57:24.798896+00:00   \n",
       "3              10 2025-10-29 08:03:21.322228+00:00   \n",
       "4              11 2025-10-29 08:13:05.088957+00:00   \n",
       "..            ...                              ...   \n",
       "29            149 2025-11-06 11:01:50.629915+00:00   \n",
       "30            150 2025-11-06 11:04:09.268267+00:00   \n",
       "31            151 2025-11-06 11:08:12.838134+00:00   \n",
       "32            152 2025-11-06 11:10:52.420490+00:00   \n",
       "33            153 2025-11-06 11:15:36.556914+00:00   \n",
       "\n",
       "                         updated_at  lead_time  \n",
       "0  2025-10-29 07:30:59.870298+00:00    260.484  \n",
       "1  2025-10-29 07:42:58.168204+00:00    711.878  \n",
       "2  2025-10-29 07:57:24.798896+00:00    865.725  \n",
       "3  2025-10-29 08:03:21.322228+00:00    355.591  \n",
       "4  2025-10-29 08:13:05.088957+00:00    582.876  \n",
       "..                              ...        ...  \n",
       "29 2025-11-06 11:01:50.629915+00:00     96.827  \n",
       "30 2025-11-06 11:04:09.268267+00:00    137.540  \n",
       "31 2025-11-06 11:08:12.838134+00:00    206.062  \n",
       "32 2025-11-06 11:10:52.420490+00:00    158.764  \n",
       "33 2025-11-06 11:15:36.556914+00:00    282.973  \n",
       "\n",
       "[142 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0     [{'start': 70, 'end': 95, 'text': '–≤–∫—É—Å –±—ã–ª —Ä–µ...\n",
       "1     [{'start': 0, 'end': 16, 'text': '–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é ...\n",
       "2     [{'start': 0, 'end': 17, 'text': '–í—Å–µ—Ö –ø—Ä–∏–≤–µ—Ç—Å...\n",
       "3     [{'start': 105, 'end': 161, 'text': '–ù–æ–≤–∏–Ω–∫—É –æ...\n",
       "4     [{'start': 30, 'end': 135, 'text': '–ó–∏–º–∞ –µ—â–µ –Ω...\n",
       "                            ...                        \n",
       "29    [{'start': 178, 'end': 245, 'text': '–¶–µ–Ω–∞ –¥–∞–ª–µ...\n",
       "30    [{'start': 0, 'end': 223, 'text': '–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ...\n",
       "31    [{'start': 258, 'end': 320, 'text': '–Ø –Ω–µ –æ—à–∏–±...\n",
       "32    [{'start': 0, 'end': 53, 'text': '–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫...\n",
       "33    [{'start': 0, 'end': 171, 'text': '–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é...\n",
       "Name: aspect_sentiment, Length: 142, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations_1 = pd.read_json(\"project-8-at-2025-11-01-11-43-483e368c.json\")\n",
    "df_annotations_2 = pd.read_json(\"project-9-at-2025-11-01-11-51-0428f45f.json\")\n",
    "df_annotations_3 = pd.read_json(\"project-10-at-2025-11-06-14-16-bb9bc9cd.json\")\n",
    "\n",
    "df_annotations = pd.concat([df_annotations_1, df_annotations_2, df_annotations_3])\n",
    "\n",
    "display(df_annotations)\n",
    "\n",
    "df_annotations['aspect_sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7e1a6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>–£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>–ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>–ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span           label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   –í–ö–£–°_POSITIVE\n",
       "1     –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...               O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...               O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞               O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...             ...\n",
       "2063  ‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...   –í–ö–£–°_POSITIVE\n",
       "2064             –£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥  –ü–ê–ß–ö–ê_POSITIVE\n",
       "2065                       –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.   –í–ö–£–°_POSITIVE\n",
       "2066                              –ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!   –í–ö–£–°_POSITIVE\n",
       "2067  –ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...               O\n",
       "\n",
       "[2068 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['span', 'label'])\n",
    "\n",
    "for mark in df_annotations['aspect_sentiment']:\n",
    "    for param in range(len(mark)):\n",
    "        span = mark[param]['text']\n",
    "        label = mark[param]['labels'][0]\n",
    "\n",
    "        df.loc[len(df)] = [span, label]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "951a214b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "–í–ö–£–°_POSITIVE        481\n",
       "–¢–ï–ö–°–¢–£–†–ê_POSITIVE    302\n",
       "–í–ö–£–°_NEGATIVE        265\n",
       "O                    259\n",
       "–í–ö–£–°_NEUTRAL         236\n",
       "–ü–ê–ß–ö–ê_POSITIVE       156\n",
       "–¢–ï–ö–°–¢–£–†–ê_NEUTRAL     134\n",
       "–ü–ê–ß–ö–ê_NEUTRAL        100\n",
       "–¢–ï–ö–°–¢–£–†–ê_NEGATIVE     99\n",
       "–ü–ê–ß–ö–ê_NEGATIVE        36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f186f5e",
   "metadata": {},
   "source": [
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2342e673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä –Ω–µ —Ä–∞–¥ –æ—á–µ–Ω—å –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Ö–æ—Ç–µ—Ç—å –ø–æ–¥—Ä—É–∂–∏—Ç—å—Å—è –≤—ã\n"
     ]
    }
   ],
   "source": [
    "lemmatizator = MorphAnalyzer(lang = 'ru')\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('russian'))\n",
    "stop_words = [i for i in stop_words if i not in ['–Ω–µ', '–Ω–µ—Ç']]\n",
    "\n",
    "@lru_cache(maxsize=100_000)\n",
    "def lemmatization(text):\n",
    "    return lemmatizator.parse(text)[0].normal_form\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    sentence = []\n",
    "\n",
    "    for i in tokens:\n",
    "        if i not in stop_words:\n",
    "            sentence.append(lemmatization(i))\n",
    "\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "print(preprocess_text('     –ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –Ø –Ω–µ —Ä–∞–¥ –æ—á–µ–Ω—å –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—Å—è –∏ —Ö–æ—á—É –ø–æ–¥—Ä—É–∂–∏—Ç—å—Å—è —Å –≤–∞–º–∏'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce95f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('–Ω–µ—Ç' in stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da642ec",
   "metadata": {},
   "source": [
    "# –ú–æ–¥–µ–ª—å 1: –ü—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f6fbddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "      <td>–µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "      <td>–∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>–£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "      <td>–ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>–ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "      <td>–ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>–ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...</td>\n",
       "      <td>O</td>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ n—á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span           label  \\\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   –í–ö–£–°_POSITIVE   \n",
       "1     –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...               O   \n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...               O   \n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞               O   \n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   –í–ö–£–°_NEGATIVE   \n",
       "...                                                 ...             ...   \n",
       "2063  ‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...   –í–ö–£–°_POSITIVE   \n",
       "2064             –£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥  –ü–ê–ß–ö–ê_POSITIVE   \n",
       "2065                       –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.   –í–ö–£–°_POSITIVE   \n",
       "2066                              –ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!   –í–ö–£–°_POSITIVE   \n",
       "2067  –ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...               O   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π  \n",
       "1     –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å ...  \n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...  \n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞  \n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π  \n",
       "...                                                 ...  \n",
       "2063  –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞...  \n",
       "2064                     –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥  \n",
       "2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å  \n",
       "2066                              –ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å  \n",
       "2067  –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ n—á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ...  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_text'] = df['span'].apply(preprocess_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "968204c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text']\n",
    "y = df['label']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0eeb24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "(1447, 78)\n",
      "(621, 78)\n"
     ]
    }
   ],
   "source": [
    "max_size = 0\n",
    "\n",
    "for i in df['cleaned_text']:\n",
    "    max_size = max(max_size, len(i.split()))\n",
    "\n",
    "print(max_size)\n",
    "\n",
    "vectoraizer = TfidfVectorizer(max_features=max_size, ngram_range=(1, 4))\n",
    "\n",
    "X_train_tfidf = vectoraizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectoraizer.transform(X_test)\n",
    "\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a489b622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score —Ä–∞–≤–µ–Ω 0.4216428958364395\n",
      "['–í–ö–£–°_POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(f'f1_score —Ä–∞–≤–µ–Ω {f1_score(y_pred, y_test, average='weighted')}')\n",
    "\n",
    "inf = model.predict((vectoraizer.transform([preprocess_text('–≤–∫—É—Å –Ω–µ –æ—á–µ–Ω—å')])))\n",
    "print(encoder.inverse_transform(inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2917fc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "–í–ö–£–°_POSITIVE        481\n",
       "–¢–ï–ö–°–¢–£–†–ê_POSITIVE    302\n",
       "–í–ö–£–°_NEGATIVE        265\n",
       "O                    259\n",
       "–í–ö–£–°_NEUTRAL         236\n",
       "–ü–ê–ß–ö–ê_POSITIVE       156\n",
       "–¢–ï–ö–°–¢–£–†–ê_NEUTRAL     134\n",
       "–ü–ê–ß–ö–ê_NEUTRAL        100\n",
       "–¢–ï–ö–°–¢–£–†–ê_NEGATIVE     99\n",
       "–ü–ê–ß–ö–ê_NEGATIVE        36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4226693",
   "metadata": {},
   "source": [
    "# –ú–æ–¥–µ–ª—å 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e35390ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π\n",
      "[4, 575, 176]\n"
     ]
    }
   ],
   "source": [
    "tokenizator = Tokenizer(num_words = 5000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', oov_token = 'O')\n",
    "\n",
    "\n",
    "tokenizator.fit_on_texts(X)\n",
    "\n",
    "X_sequences = tokenizator.texts_to_sequences(X)\n",
    "\n",
    "print(X[0])\n",
    "print(X_sequences[0])\n",
    "\n",
    "max_len_seq = max(X_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66a25a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_seq = 0\n",
    "\n",
    "for i in X_sequences:\n",
    "    max_len_seq = max(max_len_seq, len(i))\n",
    "\n",
    "max_len_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dbca4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    4,  575,  176],\n",
       "       [   0,    0,    0, ...,  577,  344,  477],\n",
       "       [   0,    0,    0, ...,  143, 1333,    7],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  212,  251,    4],\n",
       "       [   0,    0,    0, ...,    0,  116,   68],\n",
       "       [   0,    0,    0, ..., 2800, 2801, 2802]],\n",
       "      shape=(2068, 78), dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_seq = pad_sequences(X_sequences, maxlen = max_len_seq)\n",
    "X_new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7721497",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new_seq, y, test_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "985eb746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  84,  14,  10],\n",
       "       [  0,   0,   0, ...,  26, 904, 249],\n",
       "       [  0,   0,   0, ...,   3, 608,   9],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   4,  86,  81],\n",
       "       [  0,   0,   0, ...,  62, 954,   4],\n",
       "       [  0,   0,   0, ..., 211,  37,  74]], shape=(620, 78), dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1d9d658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=max_len_seq),\n",
    "    SpatialDropout1D(0.3),\n",
    "    Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eda7381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 487ms/step - accuracy: 0.1915 - loss: 3.2596 - val_accuracy: 0.1371 - val_loss: 3.1816\n",
      "Epoch 2/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 203ms/step - accuracy: 0.2177 - loss: 3.0570 - val_accuracy: 0.1371 - val_loss: 3.0209\n",
      "Epoch 3/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 205ms/step - accuracy: 0.2157 - loss: 2.8959 - val_accuracy: 0.1371 - val_loss: 2.9006\n",
      "Epoch 4/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 200ms/step - accuracy: 0.2117 - loss: 2.7876 - val_accuracy: 0.1371 - val_loss: 2.7971\n",
      "Epoch 5/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 192ms/step - accuracy: 0.2036 - loss: 2.6761 - val_accuracy: 0.1371 - val_loss: 2.7172\n",
      "Epoch 6/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - accuracy: 0.2238 - loss: 2.5815 - val_accuracy: 0.1371 - val_loss: 2.6400\n",
      "Epoch 7/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - accuracy: 0.2339 - loss: 2.4766 - val_accuracy: 0.1371 - val_loss: 2.5598\n",
      "Epoch 8/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 203ms/step - accuracy: 0.2319 - loss: 2.3478 - val_accuracy: 0.1371 - val_loss: 2.4423\n",
      "Epoch 9/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 211ms/step - accuracy: 0.2762 - loss: 2.1860 - val_accuracy: 0.1855 - val_loss: 2.3761\n",
      "Epoch 10/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - accuracy: 0.2923 - loss: 2.1029 - val_accuracy: 0.1694 - val_loss: 2.3413\n",
      "Epoch 11/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 187ms/step - accuracy: 0.2923 - loss: 2.0241 - val_accuracy: 0.1532 - val_loss: 2.3248\n",
      "Epoch 12/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 203ms/step - accuracy: 0.3105 - loss: 1.9622 - val_accuracy: 0.1452 - val_loss: 2.3922\n",
      "Epoch 13/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 193ms/step - accuracy: 0.3266 - loss: 1.8654 - val_accuracy: 0.1452 - val_loss: 2.4120\n",
      "Epoch 14/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - accuracy: 0.3669 - loss: 1.8278 - val_accuracy: 0.1694 - val_loss: 2.3214\n",
      "Epoch 15/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 197ms/step - accuracy: 0.3730 - loss: 1.7298 - val_accuracy: 0.1694 - val_loss: 2.3814\n",
      "Epoch 16/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 197ms/step - accuracy: 0.4052 - loss: 1.6330 - val_accuracy: 0.1855 - val_loss: 2.4468\n",
      "Epoch 17/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 197ms/step - accuracy: 0.4315 - loss: 1.5991 - val_accuracy: 0.1774 - val_loss: 2.4695\n",
      "Epoch 18/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 189ms/step - accuracy: 0.4758 - loss: 1.5156 - val_accuracy: 0.2016 - val_loss: 2.4726\n",
      "Epoch 19/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.4980 - loss: 1.4903 - val_accuracy: 0.2016 - val_loss: 2.5134\n",
      "Epoch 20/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 197ms/step - accuracy: 0.4415 - loss: 1.4674 - val_accuracy: 0.1774 - val_loss: 2.6442\n",
      "Epoch 21/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 191ms/step - accuracy: 0.4617 - loss: 1.4652 - val_accuracy: 0.1935 - val_loss: 2.6959\n",
      "Epoch 22/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 197ms/step - accuracy: 0.5121 - loss: 1.3611 - val_accuracy: 0.1694 - val_loss: 2.9404\n",
      "Epoch 23/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.5161 - loss: 1.3671 - val_accuracy: 0.1774 - val_loss: 2.7446\n",
      "Epoch 24/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 203ms/step - accuracy: 0.5000 - loss: 1.3595 - val_accuracy: 0.2258 - val_loss: 2.6199\n",
      "Epoch 25/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - accuracy: 0.4879 - loss: 1.3249 - val_accuracy: 0.2177 - val_loss: 2.7458\n",
      "Epoch 26/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 197ms/step - accuracy: 0.5565 - loss: 1.2726 - val_accuracy: 0.2097 - val_loss: 3.1196\n",
      "Epoch 27/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - accuracy: 0.5423 - loss: 1.2381 - val_accuracy: 0.2016 - val_loss: 2.9292\n",
      "Epoch 28/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 190ms/step - accuracy: 0.5806 - loss: 1.1902 - val_accuracy: 0.2097 - val_loss: 3.0782\n",
      "Epoch 29/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.5968 - loss: 1.1617 - val_accuracy: 0.2016 - val_loss: 3.4503\n",
      "Epoch 30/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 193ms/step - accuracy: 0.5827 - loss: 1.1116 - val_accuracy: 0.1935 - val_loss: 3.3123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20a3d47ccd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 30, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ba34bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ spatial_dropout1d_1             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m128\u001b[0m)        ‚îÇ       \u001b[38;5;34m640,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ spatial_dropout1d_1             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m128\u001b[0m)        ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m128\u001b[0m)        ‚îÇ        \u001b[38;5;34m98,816\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ        \u001b[38;5;34m41,216\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ         \u001b[38;5;34m4,160\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ         \u001b[38;5;34m2,080\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ           \u001b[38;5;34m330\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> (9.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,359,808\u001b[0m (9.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">786,602</span> (3.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m786,602\u001b[0m (3.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,206</span> (6.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,573,206\u001b[0m (6.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a4ced6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step\n",
      "F1-score: 0.2390\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)  # –ë–µ—Ä–µ–º –∫–ª–∞—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é\n",
    "\n",
    "print(f'F1-score: {f1_score(y_test, y_pred, average=\"weighted\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1766b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.46106568 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.43371472 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.77415114 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "text = '–í–∫—É—Å –≥–æ–≤–Ω–æ, –Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª—Å—è'\n",
    "text = preprocess_text(text)\n",
    "\n",
    "new_text = vectoraizer.transform([text]).todense()\n",
    "\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a012c8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_10\n",
      "Received: inputs=('Tensor(shape=(1, 78))',)\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "üìù –¢–µ–∫—Å—Ç: '–≤–∫—É—Å –≥–æ–≤–Ω–æ –Ω–µ –ø–æ–Ω—Ä–∞–≤–∏—Ç—å—Å—è'\n",
      "üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: ['–¢–ï–ö–°–¢–£–†–ê_POSITIVE']\n",
      "üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: 80.01%\n",
      "\n",
      "üèÜ –¢–æ–ø-3 –∫–ª–∞—Å—Å–∞:\n",
      "  1. –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 80.01%\n",
      "  2. –í–ö–£–°_NEUTRAL: 6.81%\n",
      "  3. –í–ö–£–°_NEGATIVE: 5.37%\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict([new_text])\n",
    "predicted_class = np.argmax(predict, axis=1)\n",
    "class_name = encoder.inverse_transform(predicted_class)\n",
    "\n",
    "print(f\"üìù –¢–µ–∫—Å—Ç: '{text}'\")\n",
    "print(f\"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {class_name}\")\n",
    "print(f\"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.max(predict[0]):.2%}\")\n",
    "\n",
    "# –ü–æ–∫–∞–∂–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "print(\"\\nüèÜ –¢–æ–ø-3 –∫–ª–∞—Å—Å–∞:\")\n",
    "probs = predict[0]\n",
    "top_3_indices = np.argsort(probs)[-3:][::-1]  # –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–ø-3\n",
    "\n",
    "for i, idx in enumerate(top_3_indices):\n",
    "    class_n = encoder.classes_[idx]\n",
    "    prob = probs[idx]\n",
    "    print(f\"  {i+1}. {class_n}: {prob:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e4f6f",
   "metadata": {},
   "source": [
    "## –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056b8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—É—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\n",
      "label\n",
      "–í–ö–£–°_POSITIVE        481\n",
      "–¢–ï–ö–°–¢–£–†–ê_POSITIVE    302\n",
      "–í–ö–£–°_NEGATIVE        265\n",
      "O                    259\n",
      "–í–ö–£–°_NEUTRAL         236\n",
      "–ü–ê–ß–ö–ê_POSITIVE       156\n",
      "–¢–ï–ö–°–¢–£–†–ê_NEUTRAL     134\n",
      "–ü–ê–ß–ö–ê_NEUTRAL        100\n",
      "–¢–ï–ö–°–¢–£–†–ê_NEGATIVE     99\n",
      "–ü–ê–ß–ö–ê_NEGATIVE        36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "–ü–æ—Å–ª–µ –æ–≤–µ—Ä—Å—ç–º–ø–ª–∏–Ω–≥–∞:\n",
      "label\n",
      "–í–ö–£–°_POSITIVE        481\n",
      "–ü–ê–ß–ö–ê_NEUTRAL        400\n",
      "–¢–ï–ö–°–¢–£–†–ê_NEGATIVE    396\n",
      "–¢–ï–ö–°–¢–£–†–ê_POSITIVE    302\n",
      "–í–ö–£–°_NEGATIVE        265\n",
      "O                    259\n",
      "–í–ö–£–°_NEUTRAL         236\n",
      "–ü–ê–ß–ö–ê_NEGATIVE       186\n",
      "–ü–ê–ß–ö–ê_POSITIVE       156\n",
      "–¢–ï–ö–°–¢–£–†–ê_NEUTRAL     134\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "print(\"–¢–µ–∫—É—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω—ã–µ –∫–ª–∞—Å—Å—ã\n",
    "minority_classes = ['–ü–ê–ß–ö–ê_NEGATIVE', '–¢–ï–ö–°–¢–£–†–ê_NEGATIVE', '–ü–ê–ß–ö–ê_NEUTRAL']\n",
    "\n",
    "# –û–≤–µ—Ä—Å—ç–º–ø–ª–∏–Ω–≥ –¥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∫–ª–∞—Å—Å–∞\n",
    "dfs_resampled = []\n",
    "for class_name in minority_classes:\n",
    "    class_df = df[df['label'] == class_name]\n",
    "    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤ 2-3 —Ä–∞–∑–∞ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω—ã–µ –∫–ª–∞—Å—Å—ã\n",
    "    target_size = max(len(class_df) * 3, 150)  # –º–∏–Ω–∏–º—É–º 150 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "    if len(class_df) < target_size:\n",
    "        resampled_df = resample(class_df, \n",
    "                              replace=True, \n",
    "                              n_samples=target_size, \n",
    "                              random_state=42)\n",
    "        \n",
    "        dfs_resampled.append(resampled_df)\n",
    "\n",
    "# –°–æ–±–∏—Ä–∞–µ–º –æ–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "df_balanced = pd.concat([df] + dfs_resampled, ignore_index=True)\n",
    "print(\"\\n–ü–æ—Å–ª–µ –æ–≤–µ—Ä—Å—ç–º–ø–ª–∏–Ω–≥–∞:\")\n",
    "print(df_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf6daead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def simple_augmentation(text, n_variants=2):\n",
    "    \"\"\"–ü—Ä–æ—Å—Ç–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–º–µ–Ω–æ–π —Å–∏–Ω–æ–Ω–∏–º–æ–≤\"\"\"\n",
    "    synonyms = {\n",
    "        '–≤–∫—É—Å': ['–ø—Ä–∏–≤–∫—É—Å', '–∞—Ä–æ–º–∞—Ç', '–ø–æ—Å–ª–µ–≤–∫—É—Å–∏–µ'],\n",
    "        '–ø–∞—á–∫–∞': ['—É–ø–∞–∫–æ–≤–∫–∞', '–ø–∞–∫–µ—Ç–∏–∫', '—Ç—É–±–∞', '–ø–∞–∫–µ—Ç'],\n",
    "        '—Ç–µ–∫—Å—Ç—É—Ä–∞': ['—Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å', '–Ω–æ—Ç–∫–∏', '–æ—â—É—â–µ–Ω–∏–µ'],\n",
    "        '–æ—Ç–ª–∏—á–Ω—ã–π': ['–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–π', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π', '–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–π'],\n",
    "        '–ø–ª–æ—Ö–æ–π': ['—É–∂–∞—Å–Ω—ã–π', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π', '–Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–π'],\n",
    "        '–æ—Å—Ç—Ä—ã–π': ['–ø–∏–∫–∞–Ω—Ç–Ω—ã–π', '–ø—Ä—è–Ω—ã–π', '–∂–≥—É—á–∏–π'],\n",
    "        '–æ—Ç—Å—Ç–æ–π': ['–Ω–µ –æ—á–µ–Ω—å']\n",
    "    }\n",
    "    \n",
    "    augmented = []\n",
    "    for _ in range(n_variants):\n",
    "        words = text.split()\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word.lower() in synonyms and random.random() > 0.7:\n",
    "                new_words.append(random.choice(synonyms[word.lower()]))\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        augmented.append(' '.join(new_words))\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –∫ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω—ã–º –∫–ª–∞—Å—Å–∞–º\n",
    "augmented_data = []\n",
    "for label in minority_classes:\n",
    "    samples = df[df['label'] == label]\n",
    "    for _, row in samples.iterrows():\n",
    "        augmented_texts = simple_augmentation(row['span'])\n",
    "        for aug_text in augmented_texts:\n",
    "            augmented_data.append({'span': aug_text, 'label': row['label']})\n",
    "\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "df_extended = pd.concat([df, df_augmented], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58b9aa5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "      <td>–µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>–í–Ω—É—Ç—Ä–∏ —á–∏–ø—Å—ã —É–ø–∞–∫–æ–≤–∞–Ω—ã –≤ —Ñ–æ–ª—å–≥—É</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEUTRAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>–ß–∏–ø—Å—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ, –Ω–∞ –∫–æ—Ç...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEUTRAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>–ß–∏–ø—Å—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ, –Ω–∞ –∫–æ—Ç...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEUTRAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>–ù–∞ –ø–∞—á–∫–µ –∏–∑–æ–±—Ä–∞–∂—ë–Ω–æ –ø–ª–∞–º—è, —á—Ç–æ –±—É–∫–≤–∞–ª—å–Ω–æ –∫—Ä–∏—á–∏...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEUTRAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>–ù–∞ –ø–∞—á–∫–µ –∏–∑–æ–±—Ä–∞–∂—ë–Ω–æ –ø–ª–∞–º—è, —á—Ç–æ –±—É–∫–≤–∞–ª—å–Ω–æ –∫—Ä–∏—á–∏...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEUTRAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2538 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span          label  \\\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π  –í–ö–£–°_POSITIVE   \n",
       "1     –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...              O   \n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...              O   \n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞              O   \n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ  –í–ö–£–°_NEGATIVE   \n",
       "...                                                 ...            ...   \n",
       "2533                    –í–Ω—É—Ç—Ä–∏ —á–∏–ø—Å—ã —É–ø–∞–∫–æ–≤–∞–Ω—ã –≤ —Ñ–æ–ª—å–≥—É  –ü–ê–ß–ö–ê_NEUTRAL   \n",
       "2534  –ß–∏–ø—Å—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ, –Ω–∞ –∫–æ—Ç...  –ü–ê–ß–ö–ê_NEUTRAL   \n",
       "2535  –ß–∏–ø—Å—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ, –Ω–∞ –∫–æ—Ç...  –ü–ê–ß–ö–ê_NEUTRAL   \n",
       "2536  –ù–∞ –ø–∞—á–∫–µ –∏–∑–æ–±—Ä–∞–∂—ë–Ω–æ –ø–ª–∞–º—è, —á—Ç–æ –±—É–∫–≤–∞–ª—å–Ω–æ –∫—Ä–∏—á–∏...  –ü–ê–ß–ö–ê_NEUTRAL   \n",
       "2537  –ù–∞ –ø–∞—á–∫–µ –∏–∑–æ–±—Ä–∞–∂—ë–Ω–æ –ø–ª–∞–º—è, —á—Ç–æ –±—É–∫–≤–∞–ª—å–Ω–æ –∫—Ä–∏—á–∏...  –ü–ê–ß–ö–ê_NEUTRAL   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π  \n",
       "1     –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å ...  \n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...  \n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞  \n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π  \n",
       "...                                                 ...  \n",
       "2533                                                NaN  \n",
       "2534                                                NaN  \n",
       "2535                                                NaN  \n",
       "2536                                                NaN  \n",
       "2537                                                NaN  \n",
       "\n",
       "[2538 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689d803",
   "metadata": {},
   "source": [
    "# –ú–æ–¥–µ–ª—å 3: Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a9626354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model_name = 'cointegrated/rubert-tiny'\n",
    "model_name = 'cointegrated/rubert-tiny2'\n",
    "\n",
    "tokenaizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels = len(encoder.classes_), \n",
    "    id2label={i: label for i, label in enumerate(encoder.classes_)},\n",
    "    label2id={label: i for i, label in enumerate(encoder.classes_)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34b96d7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_extended[\u001b[33m'\u001b[39m\u001b[33mencoded_label\u001b[39m\u001b[33m'\u001b[39m] = encoder.transform(\u001b[43mdf_extended\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      2\u001b[39m df_extended = df_extended[[\u001b[33m'\u001b[39m\u001b[33mspan\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mencoded_label\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      3\u001b[39m df_extended = df_extended.rename(columns = {\u001b[33m'\u001b[39m\u001b[33mencoded_label\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "df_extended['encoded_label'] = encoder.transform(df_extended['label'])\n",
    "df_extended = df_extended[['span', 'encoded_label']]\n",
    "df_extended = df_extended.rename(columns = {'encoded_label': 'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6f157d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>–í–Ω—É—Ç—Ä–∏ —á–∏–ø—Å—ã —É–ø–∞–∫–æ–≤–∞–Ω—ã –≤ —Ñ–æ–ª—å–≥—É</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>–ß–∏–ø—Å—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ, –Ω–∞ –∫–æ—Ç...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>–ß–∏–ø—Å—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ, –Ω–∞ –∫–æ—Ç...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>–ù–∞ –ø–∞—á–∫–µ –∏–∑–æ–±—Ä–∞–∂—ë–Ω–æ –ø–ª–∞–º—è, —á—Ç–æ –±—É–∫–≤–∞–ª—å–Ω–æ –∫—Ä–∏—á–∏...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>–ù–∞ –ø–∞—á–∫–µ –∏–∑–æ–±—Ä–∞–∂—ë–Ω–æ –ø–ª–∞–º—è, —á—Ç–æ –±—É–∫–≤–∞–ª—å–Ω–æ –∫—Ä–∏—á–∏...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2538 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span  labels\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π       3\n",
       "1     –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...       0\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...       0\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞       0\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ       1\n",
       "...                                                 ...     ...\n",
       "2533                    –í–Ω—É—Ç—Ä–∏ —á–∏–ø—Å—ã —É–ø–∞–∫–æ–≤–∞–Ω—ã –≤ —Ñ–æ–ª—å–≥—É       5\n",
       "2534  –ß–∏–ø—Å—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ, –Ω–∞ –∫–æ—Ç...       5\n",
       "2535  –ß–∏–ø—Å—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ, –Ω–∞ –∫–æ—Ç...       5\n",
       "2536  –ù–∞ –ø–∞—á–∫–µ –∏–∑–æ–±—Ä–∞–∂—ë–Ω–æ –ø–ª–∞–º—è, —á—Ç–æ –±—É–∫–≤–∞–ª—å–Ω–æ –∫—Ä–∏—á–∏...       5\n",
       "2537  –ù–∞ –ø–∞—á–∫–µ –∏–∑–æ–±—Ä–∞–∂—ë–Ω–æ –ø–ª–∞–º—è, —á—Ç–æ –±—É–∫–≤–∞–ª—å–Ω–æ –∫—Ä–∏—á–∏...       5\n",
       "\n",
       "[2538 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0f4bf3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   span  labels\n",
      "1024  –ù–µ —á–∏—Å—Ç–æ –≤–∫—É—Å —Ä—ã–±—ã, –∫–æ–Ω–µ—á–Ω–æ, –∞ –∏–º–µ–Ω–Ω–æ —Ä—ã–±–Ω–æ-—Å–ª...       3\n",
      "950                           –Ω–∞–ø–æ–º–∏–Ω–∞–µ—Ç —à–∞—à–ª—ã—á–Ω—ã–π –≤–∫—É—Å       2\n",
      "199                       –ó–∞–ø–∞—Ö, –∫–∞–∫ –±—É–¥—Ç–æ –±—ã, —É–∫—Å—É—Å–Ω—ã–π       1\n",
      "637   –ü–∞—á–∫–∞ –±—ã–ª–∞ –≤ —Å–∏–Ω–∏—Ö –æ—Ç—Ç–µ–Ω–∫–∞—Ö, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–º–ø–∞—Ç...       6\n",
      "2441  –≠—Ç–æ –º–µ–≥–∞-—É–¥–æ–±–Ω–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ –º–æ–∂–Ω–æ –æ—Ç–∫—Ä—ã—Ç—å –æ–¥–∏–Ω...       5\n",
      "(2030, 2)\n",
      "                                                   span  labels\n",
      "2054                        –ù–µ—Ç –ø–µ—Ä–µ–∂–∞—Ä–µ–Ω–Ω—ã—Ö –∏ –≥–æ—Ä–µ–ª—ã—Ö;       9\n",
      "934   –°–∞–º–∏ —á–∏–ø—Å—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–ª—å–Ω–æ –ø–æ—Å—ã–ø–∞–Ω—ã —Å–ø–µ—Ü–∏—è–º–∏...       9\n",
      "840                              –ò —á–∏–ø—Å—ã –æ—á–µ–Ω—å —Å–æ–ª—ë–Ω—ã–µ!       1\n",
      "1280  –ö—É–ø–∏–ª–∞ –≤ –º–∞–≥–∞–∑–∏–Ω–µ ¬´–ö—Ä–∞—Å–Ω–æ–µ –∏ –ë–µ–ª–æ–µ¬ª —É–ø–∞–∫–æ–≤–∫—É —á...       0\n",
      "511   –£–≤–∏–¥–µ–≤ –Ω–∞ –ø–æ–ª–∫–µ –≤ –º–∞–≥–∞–∑–∏–Ω–µ –ø—Ä–∏–æ–±—Ä–µ–ª–∞ —Å—Ä–∞–∑—É –¥–≤–∞...       6\n",
      "(254, 2)\n",
      "                                                   span  labels\n",
      "70    –∑–∞–ø–∏—Ö–Ω—É–ª–∏ –∏ –≥–∏–≥–∞–Ω—Ç—Å–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫—Ä–∞—Ö–º–∞–ª–∞, –∏ ...       8\n",
      "2181              –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–ø–∞–¥–∞—é—Ç—Å—è –æ–±–≥–æ—Ä–µ–ª—ã–µ –∫—Ä–∞—è       7\n",
      "1403                           –î–∞–∂–µ –ø–æ—á—Ç–∏ –Ω–µ –ø–æ–ª–æ–º–∞–Ω–Ω—ã–µ       8\n",
      "26    –í —Å–æ—Å—Ç–∞–≤ —Ç–∞–∫–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞ —è –æ—Å–æ–±–æ –Ω–µ –≤—á–∏—Ç—ã–≤–∞—é—Å—å...       0\n",
      "2014  –° –¥–∞–Ω–Ω—ã–º —Å–æ—É—Å–æ–º —è –∑–Ω–∞–∫–æ–º–∞, —É –º–Ω–æ–≥–∏—Ö –æ–Ω –≤—ã–∑—ã–≤–∞–µ...       3\n",
      "(254, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_temp = train_test_split(df_extended, test_size = 0.2, stratify=df_extended['labels'])\n",
    "\n",
    "df_test, df_val = train_test_split(df_temp, test_size = 0.5, stratify = df_temp['labels'])\n",
    "\n",
    "print(df_train.head(5))\n",
    "print(df_train.shape)\n",
    "\n",
    "print(df_test.head(5))\n",
    "print(df_test.shape)\n",
    "\n",
    "print(df_val.head(5))\n",
    "print(df_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "300c2dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2030/2030 [00:00<00:00, 4382.64 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 254/254 [00:00<00:00, 6218.57 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 254/254 [00:00<00:00, 7183.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "\n",
    "def tokenize_dataset(text):\n",
    "    return tokenaizer(\n",
    "        text['span'], \n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None  # –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã!\n",
    "    )\n",
    "\n",
    "tokenized_dataset_train = dataset_train.map(tokenize_dataset, batched=True)\n",
    "tokenized_dataset_test = dataset_test.map(tokenize_dataset, batched=True)\n",
    "tokenized_dataset_val = dataset_val.map(tokenize_dataset, batched=True)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenaizer,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9c07526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11176' max='25400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11176/25400 1:06:47 < 1:25:01, 2.79 it/s, Epoch 22/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.254000</td>\n",
       "      <td>2.229800</td>\n",
       "      <td>0.125134</td>\n",
       "      <td>0.224409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.041800</td>\n",
       "      <td>1.979597</td>\n",
       "      <td>0.250906</td>\n",
       "      <td>0.358268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.707800</td>\n",
       "      <td>1.624355</td>\n",
       "      <td>0.368054</td>\n",
       "      <td>0.468504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.433300</td>\n",
       "      <td>1.406239</td>\n",
       "      <td>0.424360</td>\n",
       "      <td>0.515748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.212100</td>\n",
       "      <td>1.216569</td>\n",
       "      <td>0.544745</td>\n",
       "      <td>0.610236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.908700</td>\n",
       "      <td>1.036164</td>\n",
       "      <td>0.623522</td>\n",
       "      <td>0.673228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.673900</td>\n",
       "      <td>1.007444</td>\n",
       "      <td>0.606582</td>\n",
       "      <td>0.653543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>1.012203</td>\n",
       "      <td>0.638735</td>\n",
       "      <td>0.669291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>1.035201</td>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.696850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>1.096610</td>\n",
       "      <td>0.694860</td>\n",
       "      <td>0.712598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>1.256465</td>\n",
       "      <td>0.680939</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>1.434889</td>\n",
       "      <td>0.690364</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>1.492686</td>\n",
       "      <td>0.714959</td>\n",
       "      <td>0.724409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.089100</td>\n",
       "      <td>1.613840</td>\n",
       "      <td>0.714902</td>\n",
       "      <td>0.720472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>1.743551</td>\n",
       "      <td>0.708942</td>\n",
       "      <td>0.708661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>1.879876</td>\n",
       "      <td>0.714703</td>\n",
       "      <td>0.716535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.855701</td>\n",
       "      <td>0.718153</td>\n",
       "      <td>0.724409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>1.949013</td>\n",
       "      <td>0.707404</td>\n",
       "      <td>0.708661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>2.027800</td>\n",
       "      <td>0.700283</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.089464</td>\n",
       "      <td>0.702352</td>\n",
       "      <td>0.712598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.083334</td>\n",
       "      <td>0.710339</td>\n",
       "      <td>0.724409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>2.239777</td>\n",
       "      <td>0.697698</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11176, training_loss=0.565885606228188, metrics={'train_runtime': 4008.6614, 'train_samples_per_second': 25.32, 'train_steps_per_second': 6.336, 'total_flos': 17220183179184.0, 'train_loss': 0.565885606228188, 'epoch': 22.0})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='models/',\n",
    "                                  overwrite_output_dir = True,\n",
    "                                  logging_dir='logs/',\n",
    "                                  num_train_epochs=50,\n",
    "                                  learning_rate=3e-5,\n",
    "                                  per_device_train_batch_size=4,   \n",
    "                                  eval_strategy='epoch',\n",
    "                                  save_strategy='epoch',\n",
    "                                  warmup_ratio=0.1,\n",
    "                                  lr_scheduler_type='cosine',\n",
    "                                  metric_for_best_model='f1-score',\n",
    "                                  weight_decay=0.2,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_total_limit=2,\n",
    "                                  max_grad_norm=1.0,\n",
    "                                  logging_steps=100)\n",
    "\n",
    "def eval_metrcis(eval_pred):\n",
    "\n",
    "    predictions, label = eval_pred\n",
    "    predictions = np.argmax(predictions, axis = 1)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'f1-score': f1_score(label, predictions, average='weighted'),\n",
    "        'accuracy': accuracy_score(label, predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(model, \n",
    "                  args=training_args, \n",
    "                  train_dataset=tokenized_dataset_train,\n",
    "                  eval_dataset=tokenized_dataset_val,\n",
    "                  compute_metrics=eval_metrcis,\n",
    "                  data_collator=data_collator,\n",
    "                  callbacks=[EarlyStoppingCallback(early_stopping_patience=5)])\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b5e84f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1645994186401367,\n",
       " 'eval_f1-score': 0.664011547133012,\n",
       " 'eval_accuracy': 0.6732283464566929,\n",
       " 'eval_runtime': 1.0707,\n",
       " 'eval_samples_per_second': 237.236,\n",
       " 'eval_steps_per_second': 29.888,\n",
       " 'epoch': 22.0}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3badd7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./best_model_tokenaiser\\\\tokenizer_config.json',\n",
       " './best_model_tokenaiser\\\\special_tokens_map.json',\n",
       " './best_model_tokenaiser\\\\vocab.txt',\n",
       " './best_model_tokenaiser\\\\added_tokens.json',\n",
       " './best_model_tokenaiser\\\\tokenizer.json')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('./best_model_transformer')\n",
    "tokenaizer.save_pretrained('./best_model_tokenaiser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "92037bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline('text-classification',\n",
    "                 model = './best_model_transformer',\n",
    "                 tokenizer = './best_model_tokenaiser'\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "531cb60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '–í–ö–£–°_NEGATIVE', 'score': 0.998602569103241}]\n"
     ]
    }
   ],
   "source": [
    "result = model(['–ù–µ –æ—á–µ–Ω—å –≤–∫—É—Å–Ω–æ'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2acc7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '–í–ö–£–°_POSITIVE', 'score': 0.9997544884681702}, {'label': '–í–ö–£–°_NEGATIVE', 'score': 0.9996160268783569}, {'label': '–¢–ï–ö–°–¢–£–†–ê_NEGATIVE', 'score': 0.9885003566741943}, {'label': '–í–ö–£–°_NEGATIVE', 'score': 0.9967984557151794}, {'label': '–í–ö–£–°_NEGATIVE', 'score': 0.9990449547767639}]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"–ß–∏–ø—Å—ã –æ–±–ª–∞–¥–∞—é—Ç —è—Ä–∫–∏–º –≤–∫—É—Å–æ–º –∏ –∞—Ä–æ–º–∞—Ç–æ–º –±–µ–∫–æ–Ω–æ\",\n",
    "    \"–Ø –±—ã –Ω–µ —Å–∫–∞–∑–∞–ª, —á—Ç–æ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ, –∞ —Å–∫–æ—Ä–µ–µ —Ç–∞–∫–æ–≥–æ, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—à—å –æ–∂–∏–¥–∞—Ç—å –æ—Ç —á–∏–ø—Å\", \n",
    "    \"–ù–æ –µ—Å—Ç—å –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –∏ –∂–∏—Ä–Ω—ã–π –º–∏–Ω—É—Å: —á–∏–ø—Å—ã –ø–µ—Ä–µ—Å–æ–ª–µ–Ω—ã\",\n",
    "    \"–°–æ–ª–∏ —Ä–µ–∞–ª—å–Ω–æ –º–æ–≥–ª–æ –±—ã –º–µ–Ω—å—à–µ. –û–Ω–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç –≥–ª—É—à–∏—Ç—å –≤–∫—É—Å; –Ω–∞—á–∏–Ω–∞–µ—Ç —Ö–æ—Ç–µ—Ç—å—Å—è –ø–∏—Ç—å\",\n",
    "    \"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —É—Å–∏–ª–∏—Ç–µ–ª–µ–π –≤–∫—É—Å–∞ –∏ –∞—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–æ–≤, —á—Ç–æ –Ω–µ –∏–¥—ë—Ç –Ω–∞ –ø–æ–ª—å–∑—É –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞\"\n",
    "]\n",
    "\n",
    "predictions = model(test_texts)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
