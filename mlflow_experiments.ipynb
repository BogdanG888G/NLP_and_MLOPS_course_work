{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05f1f49",
   "metadata": {},
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Smart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import *\n",
    "\n",
    "import joblib\n",
    "import cloudpickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from functools import lru_cache\n",
    "\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import re\n",
    "import emoji\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558efd8c",
   "metadata": {},
   "source": [
    "# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e78b22",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8ba75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!   mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a3e922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ —Å–µ—Ä–≤–µ—Ä—É, –∑–∞–ø—É—â–µ–Ω–Ω—ã–º –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º —Ö–æ—Å—Ç–µ\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27692760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–≤–µ—Å—å –ø—Ä–∏–≤–µ—Ç –∫–∞–∫–æ–π –Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–π –º–µ—Å—Ç–æ –Ω–µ—Ç\n"
     ]
    }
   ],
   "source": [
    "analyzer = MorphAnalyzer(lang = 'ru')\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "stop_words_cleaned = []\n",
    "\n",
    "for i in stop_words:\n",
    "    if i != '–Ω–µ' and i != '–Ω–µ—Ç' and i != '–Ω–∏–∫–æ–≥–¥–∞':\n",
    "        stop_words_cleaned.append(i)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100_000)\n",
    "def lemmatization(text):\n",
    "    return analyzer.parse(text)[0].normal_form\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\bn(\\w+)', r'\\1', text)  # n—á–∏–ø—Å—ã -> —á–∏–ø—Å—ã\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    sentence = []\n",
    "\n",
    "    for i in text_tokens:\n",
    "        if i not in stop_words_cleaned:\n",
    "            sentence.append(lemmatization(i))\n",
    "\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "\n",
    "print(preprocess_text('–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ö–∞–∫–æ–µ –∂–µ –Ω–µ–ø—Ä–∏—è—Ç–Ω–æ–µ –º–µ—Å—Ç–æ, –Ω–µ—Ç?\\n'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb52f206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>–∏—Ç–æ–≥—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞—Å–∞...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>—É –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>–∫ –ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span           label\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...               O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...               O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞               O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π   –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...             ...\n",
       "2063  –∏—Ç–æ–≥—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞—Å–∞...   –í–ö–£–°_POSITIVE\n",
       "2064                   —É –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥  –ü–ê–ß–ö–ê_POSITIVE\n",
       "2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å   –í–ö–£–°_POSITIVE\n",
       "2066                            –∫ –ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å   –í–ö–£–°_POSITIVE\n",
       "2067  –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ ...               O\n",
       "\n",
       "[2068 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run First dataset at: http://127.0.0.1:8080/#/experiments/0/runs/f1f4e9f64a7e46eab3e2d6c8742e14a6\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='First dataset'):\n",
    "    \n",
    "    mlflow.set_tag('Dataset_version', '1.0.0')\n",
    "\n",
    "    df_annotations_1 = pd.read_json(\"project-8-at-2025-11-01-11-43-483e368c.json\")\n",
    "    df_annotations_2 = pd.read_json(\"project-9-at-2025-11-01-11-51-0428f45f.json\")\n",
    "    df_annotations_3 = pd.read_json(\"project-10-at-2025-11-06-14-16-bb9bc9cd.json\")\n",
    "\n",
    "    df_annotations = pd.concat([df_annotations_1, df_annotations_2, df_annotations_3])\n",
    "\n",
    "    df = pd.DataFrame(columns = ['span', 'label'])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for param in range(len(mark)):\n",
    "            span = mark[param]['text']\n",
    "            label = mark[param]['labels'][0]\n",
    "\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "    df['span'] = df['span'].apply(preprocess_text)\n",
    "    \n",
    "    \n",
    "    display(df)\n",
    "\n",
    "\n",
    "    file = df.to_csv('First_version.csv')\n",
    "    \n",
    "    \n",
    "    mlflow.log_artifact('First_version.csv', 'datasets')\n",
    "\n",
    "    with open('preprocess_text_first.pkl', 'wb') as f: \n",
    "        preprocess_pickle = cloudpickle.dump(preprocess_text, f)\n",
    "\n",
    "    mlflow.log_artifact('preprocess_text_first.pkl', 'functions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99a6cf",
   "metadata": {},
   "source": [
    "## –í—Ç–æ—Ä–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91dcc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_only(text):\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # –ó–∞–º–µ–Ω—è–µ–º —ç–º–æ–¥–∑–∏\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã\n",
    "    text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c7fb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>—É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>–∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span           label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   –í–ö–£–°_POSITIVE\n",
       "1     —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...               O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...               O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞               O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...             ...\n",
       "2063  check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...   –í–ö–£–°_POSITIVE\n",
       "2064             —É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥  –ü–ê–ß–ö–ê_POSITIVE\n",
       "2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å   –í–ö–£–°_POSITIVE\n",
       "2066                               –∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é   –í–ö–£–°_POSITIVE\n",
       "2067  –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...               O\n",
       "\n",
       "[2068 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Second dataset at: http://127.0.0.1:8080/#/experiments/0/runs/fe1cee81afdc40208ab2bc72e140f051\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = 'Second dataset'):\n",
    "    \n",
    "    mlflow.set_tag('Dataset_version', '2.0.0')\n",
    "\n",
    "    df_annotations_1 = pd.read_json(\"project-8-at-2025-11-01-11-43-483e368c.json\")\n",
    "    df_annotations_2 = pd.read_json(\"project-9-at-2025-11-01-11-51-0428f45f.json\")\n",
    "    df_annotations_3 = pd.read_json(\"project-10-at-2025-11-06-14-16-bb9bc9cd.json\")\n",
    "\n",
    "    df_annotations = pd.concat([df_annotations_1, df_annotations_2, df_annotations_3])\n",
    "\n",
    "    df = pd.DataFrame(columns = ['span', 'label'])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for param in range(len(mark)):\n",
    "            span = mark[param]['text']\n",
    "            label = mark[param]['labels'][0]\n",
    "\n",
    "            df.loc[len(df)] = [span, label]\n",
    "    \n",
    "    df['span'] = df['span'].apply(clean_text_only)\n",
    "\n",
    "    file = df.to_csv('Second_version.csv')\n",
    "    \n",
    "    display(df)\n",
    "\n",
    "    mlflow.log_artifact('Second_version.csv', 'datasets')\n",
    "\n",
    "\n",
    "    with open('preprocess_text_second.pkl', 'wb') as f:\n",
    "        cloudpickle.dump(clean_text_only, f)\n",
    "\n",
    "    mlflow.log_artifact('preprocess_text_second.pkl', 'functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0766fa",
   "metadata": {},
   "source": [
    "## –¢—Ä–µ—Ç–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b861f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>–£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>–ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>–ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span           label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   –í–ö–£–°_POSITIVE\n",
       "1     –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...               O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...               O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞               O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...             ...\n",
       "2063  ‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...   –í–ö–£–°_POSITIVE\n",
       "2064             –£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥  –ü–ê–ß–ö–ê_POSITIVE\n",
       "2065                       –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.   –í–ö–£–°_POSITIVE\n",
       "2066                              –ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!   –í–ö–£–°_POSITIVE\n",
       "2067  –ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...               O\n",
       "\n",
       "[2068 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Third dataset at: http://127.0.0.1:8080/#/experiments/0/runs/85b7f73a97c046b1939da0fc3eb2a90c\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = 'Third dataset'):\n",
    "    \n",
    "    mlflow.set_tag('Dataset_version', '3.0.0')\n",
    "\n",
    "    df_annotations_1 = pd.read_json(\"project-8-at-2025-11-01-11-43-483e368c.json\")\n",
    "    df_annotations_2 = pd.read_json(\"project-9-at-2025-11-01-11-51-0428f45f.json\")\n",
    "    df_annotations_3 = pd.read_json(\"project-10-at-2025-11-06-14-16-bb9bc9cd.json\")\n",
    "\n",
    "    df_annotations = pd.concat([df_annotations_1, df_annotations_2, df_annotations_3])\n",
    "\n",
    "    df = pd.DataFrame(columns = ['span', 'label'])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for param in range(len(mark)):\n",
    "            span = mark[param]['text']\n",
    "            label = mark[param]['labels'][0]\n",
    "\n",
    "            df.loc[len(df)] = [span, label]\n",
    "    \n",
    "    file = df.to_csv('Third_version.csv')\n",
    "    \n",
    "    display(df)\n",
    "\n",
    "    mlflow.log_artifact('Third_version.csv', 'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790af86c",
   "metadata": {},
   "source": [
    "# –≠–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–æ–¥–µ–ª—è–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b384f",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793612b",
   "metadata": {},
   "source": [
    "–ë—É–¥–µ–º –æ–±—É—á–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å: \"–ù–∞–∏–≤–Ω—ã–π –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\" –∏–∑ `Sklearn`, –±—É–¥–µ–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ 3 –≤–µ—Ä—Å–∏—è—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø–æ–π–º–µ–º, –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ —Å–µ–±—è –ø–æ–∫–∞–∂–µ—Ç –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ–º –∏–ª–∏ –∏–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead861b4",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e81f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>–∏—Ç–æ–≥—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞—Å–∞...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>—É –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2065</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2066</td>\n",
       "      <td>–∫ –ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               span  \\\n",
       "0              0                              –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   \n",
       "1              1  —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...   \n",
       "2              2  —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...   \n",
       "3              3                          –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞   \n",
       "4              4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π   \n",
       "...          ...                                                ...   \n",
       "2063        2063  –∏—Ç–æ–≥—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞—Å–∞...   \n",
       "2064        2064                   —É –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥   \n",
       "2065        2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å   \n",
       "2066        2066                            –∫ –ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å   \n",
       "2067        2067  –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ ...   \n",
       "\n",
       "               label  \n",
       "0      –í–ö–£–°_POSITIVE  \n",
       "1                  O  \n",
       "2                  O  \n",
       "3                  O  \n",
       "4      –í–ö–£–°_NEGATIVE  \n",
       "...              ...  \n",
       "2063   –í–ö–£–°_POSITIVE  \n",
       "2064  –ü–ê–ß–ö–ê_POSITIVE  \n",
       "2065   –í–ö–£–°_POSITIVE  \n",
       "2066   –í–ö–£–°_POSITIVE  \n",
       "2067               O  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.32907258187514044\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.4033816425120773\n",
      "üèÉ View run first_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/5b744dd079fa4e4ebdbc52836203b4ce\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = 'first_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', '1.0.0')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=['0'],  # Default —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç ID = 0\n",
    "        filter_string=\"tags.mlflow.runName = 'First dataset'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    \n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "    files = client.list_artifacts(first_dataset_latest_run_id, 'datasets')\n",
    "\n",
    "    dataframe = files[0].path\n",
    "\n",
    "    dataframe_path = client.download_artifacts(first_dataset_latest_run_id, dataframe)\n",
    "\n",
    "    df = pd.read_csv(dataframe_path)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectoraizer = TfidfVectorizer(lowercase=True,\n",
    "                                  analyzer = 'word',\n",
    "                                  max_features=5000,\n",
    "                                  ngram_range=(1, 4),\n",
    "                                  min_df=2,\n",
    "                                  max_df=0.9\n",
    "                                  )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST/VAL\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectoraizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectoraizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1_score(y_test, y_pred, average='weighted')}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "    joblib.dump(model, 'naive_bayes.pkl')\n",
    "    joblib.dump(vectoraizer, 'tfidfvectoraizer.pkl')\n",
    "    joblib.dump(encoder, 'labelencoder.pkl')\n",
    "    \n",
    "    mlflow.log_artifact('tfidfvectoraizer.pkl', 'models')\n",
    "    mlflow.log_artifact('labelencoder.pkl', 'models')\n",
    "    mlflow.log_artifact('naive_bayes.pkl', 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ddbdb",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93388f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5b744dd079fa4e4ebdbc52836203b4ce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.23it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 21.18it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 20.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"—è –ø—Ä–∏–≤–ª–µ—á—å —Ç–∞–∫–æ–π —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "['–í–ö–£–°_POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(experiment_ids=['0'],\n",
    "                         filter_string='attribute.run_name = \"first_model_experiment\"',\n",
    "                         order_by=['attribute.end_time desc'])\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "\n",
    "print(latest_run_model_id)\n",
    "\n",
    "model_run = client.get_run(latest_run_model_id)\n",
    "\n",
    "artifacts = client.list_artifacts(latest_run_model_id, 'models')\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–∏–º –º–æ–¥–µ–ª—è–º –∏–∑ MLFlow\n",
    "\n",
    "vectoraizer_file = client.download_artifacts(latest_run_model_id, 'models/tfidfvectoraizer.pkl')\n",
    "bayes_file = client.download_artifacts(latest_run_model_id, 'models/naive_bayes.pkl')\n",
    "encoder_file = client.download_artifacts(latest_run_model_id, 'models/labelencoder.pkl')\n",
    "\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ MLFlow\n",
    "\n",
    "\n",
    "latest_run_dataset = client.search_runs(experiment_ids=['0'],\n",
    "                                        filter_string='attributes.run_name=\"First dataset\"',\n",
    "                                        order_by=['attributes.end_time desc'])\n",
    "\n",
    "\n",
    "latest_run_dataset_id = latest_run_dataset[0].info.run_id\n",
    "\n",
    "art_loc = client.download_artifacts(latest_run_dataset_id, 'functions/preprocess_text_first.pkl')\n",
    "\n",
    "\n",
    "# –ò–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö –≤ –≤–∏–¥–µ –≤—ã–ø–æ–ª–Ω—è–µ–º–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞\n",
    "\n",
    "vectoraizer = joblib.load(vectoraizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "text = '–ú–µ–Ω—è –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ'\n",
    "\n",
    "preprocessed_text = preprocess_func(text)\n",
    "\n",
    "done_text = vectoraizer.transform([preprocessed_text])\n",
    "    \n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(encoder.inverse_transform(label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3439a",
   "metadata": {},
   "source": [
    "### –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ca78c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>—É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2065</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2066</td>\n",
       "      <td>–∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               span  \\\n",
       "0              0                          –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   \n",
       "1              1  —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...   \n",
       "2              2  —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...   \n",
       "3              3             —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞   \n",
       "4              4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   \n",
       "...          ...                                                ...   \n",
       "2063        2063  check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...   \n",
       "2064        2064             —É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥   \n",
       "2065        2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å   \n",
       "2066        2066                               –∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é   \n",
       "2067        2067  –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...   \n",
       "\n",
       "               label  \n",
       "0      –í–ö–£–°_POSITIVE  \n",
       "1                  O  \n",
       "2                  O  \n",
       "3                  O  \n",
       "4      –í–ö–£–°_NEGATIVE  \n",
       "...              ...  \n",
       "2063   –í–ö–£–°_POSITIVE  \n",
       "2064  –ü–ê–ß–ö–ê_POSITIVE  \n",
       "2065   –í–ö–£–°_POSITIVE  \n",
       "2066   –í–ö–£–°_POSITIVE  \n",
       "2067               O  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.31295994125933585\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.37922705314009664\n",
      "üèÉ View run second_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/448ebe798ba94371abc1f9c5491c9de0\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = 'second_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', '2.0.0')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=['0'],  # Default —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç ID = 0\n",
    "        filter_string=\"tags.mlflow.runName = 'Second dataset'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    \n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "    files = client.list_artifacts(first_dataset_latest_run_id, 'datasets')\n",
    "\n",
    "    dataframe = files[0].path\n",
    "\n",
    "    dataframe_path = client.download_artifacts(first_dataset_latest_run_id, dataframe)\n",
    "\n",
    "    df = pd.read_csv(dataframe_path)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectoraizer = TfidfVectorizer(lowercase=True,\n",
    "                                  analyzer = 'word',\n",
    "                                  max_features=5000,\n",
    "                                  ngram_range=(1, 4),\n",
    "                                  min_df=2,\n",
    "                                  max_df=0.9\n",
    "                                  )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST/VAL\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectoraizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectoraizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1_score(y_test, y_pred, average='weighted')}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "    joblib.dump(model, 'naive_bayes.pkl')\n",
    "    joblib.dump(vectoraizer, 'tfidfvectoraizer.pkl')\n",
    "    joblib.dump(encoder, 'labelencoder.pkl')\n",
    "    \n",
    "    mlflow.log_artifact('tfidfvectoraizer.pkl', 'models')\n",
    "    mlflow.log_artifact('labelencoder.pkl', 'models')\n",
    "    mlflow.log_artifact('naive_bayes.pkl', 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfbf25",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47f598e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448ebe798ba94371abc1f9c5491c9de0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.24s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.83it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 24.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–º–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ loudly_crying_face\"\n",
      "['–í–ö–£–°_POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(experiment_ids=['0'],\n",
    "                         filter_string='attribute.run_name = \"second_model_experiment\"',\n",
    "                         order_by=['attribute.end_time desc'])\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "\n",
    "print(latest_run_model_id)\n",
    "\n",
    "model_run = client.get_run(latest_run_model_id)\n",
    "\n",
    "artifacts = client.list_artifacts(latest_run_model_id, 'models')\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–∏–º –º–æ–¥–µ–ª—è–º –∏–∑ MLFlow\n",
    "\n",
    "vectoraizer_file = client.download_artifacts(latest_run_model_id, 'models/tfidfvectoraizer.pkl')\n",
    "bayes_file = client.download_artifacts(latest_run_model_id, 'models/naive_bayes.pkl')\n",
    "encoder_file = client.download_artifacts(latest_run_model_id, 'models/labelencoder.pkl')\n",
    "\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ MLFlow\n",
    "\n",
    "\n",
    "latest_run_dataset = client.search_runs(experiment_ids=['0'],\n",
    "                                        filter_string='attributes.run_name=\"Second dataset\"',\n",
    "                                        order_by=['attributes.end_time desc'])\n",
    "\n",
    "\n",
    "latest_run_dataset_id = latest_run_dataset[0].info.run_id\n",
    "\n",
    "art_loc = client.download_artifacts(latest_run_dataset_id, 'functions/preprocess_text_second.pkl')\n",
    "\n",
    "\n",
    "# –ò–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö –≤ –≤–∏–¥–µ –≤—ã–ø–æ–ª–Ω—è–µ–º–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞\n",
    "\n",
    "vectoraizer = joblib.load(vectoraizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "text = '–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠'\n",
    "\n",
    "preprocessed_text = preprocess_func(text)\n",
    "\n",
    "done_text = vectoraizer.transform([preprocessed_text])\n",
    "    \n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(encoder.inverse_transform(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231b62e",
   "metadata": {},
   "source": [
    "### –¢—Ä–µ—Ç–∏–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d475c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>–£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2065</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2066</td>\n",
       "      <td>–ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>–ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               span  \\\n",
       "0              0                          –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   \n",
       "1              1  –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...   \n",
       "2              2  —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...   \n",
       "3              3             —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞   \n",
       "4              4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   \n",
       "...          ...                                                ...   \n",
       "2063        2063  ‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...   \n",
       "2064        2064             –£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥   \n",
       "2065        2065                       –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.   \n",
       "2066        2066                              –ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!   \n",
       "2067        2067  –ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...   \n",
       "\n",
       "               label  \n",
       "0      –í–ö–£–°_POSITIVE  \n",
       "1                  O  \n",
       "2                  O  \n",
       "3                  O  \n",
       "4      –í–ö–£–°_NEGATIVE  \n",
       "...              ...  \n",
       "2063   –í–ö–£–°_POSITIVE  \n",
       "2064  –ü–ê–ß–ö–ê_POSITIVE  \n",
       "2065   –í–ö–£–°_POSITIVE  \n",
       "2066   –í–ö–£–°_POSITIVE  \n",
       "2067               O  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.31762069535565496\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.38164251207729466\n",
      "üèÉ View run third_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/b8c2c44296e747ff82fc1d01712e92b3\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = 'third_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', '3.0.0')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=['0'],  # Default —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç ID = 0\n",
    "        filter_string=\"tags.mlflow.runName = 'Third dataset'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    \n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "    files = client.list_artifacts(first_dataset_latest_run_id, 'datasets')\n",
    "\n",
    "    dataframe = files[0].path\n",
    "\n",
    "    dataframe_path = client.download_artifacts(first_dataset_latest_run_id, dataframe)\n",
    "\n",
    "    df = pd.read_csv(dataframe_path)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectoraizer = TfidfVectorizer(lowercase=True,\n",
    "                                  analyzer = 'word',\n",
    "                                  max_features=5000,\n",
    "                                  ngram_range=(1, 4),\n",
    "                                  min_df=2,\n",
    "                                  max_df=0.9\n",
    "                                  )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST/VAL\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectoraizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectoraizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1_score(y_test, y_pred, average='weighted')}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "    joblib.dump(model, 'naive_bayes.pkl')\n",
    "    joblib.dump(vectoraizer, 'tfidfvectoraizer.pkl')\n",
    "    joblib.dump(encoder, 'labelencoder.pkl')\n",
    "    \n",
    "    mlflow.log_artifact('tfidfvectoraizer.pkl', 'models')\n",
    "    mlflow.log_artifact('labelencoder.pkl', 'models')\n",
    "    mlflow.log_artifact('naive_bayes.pkl', 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20a7ee",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8890d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b8c2c44296e747ff82fc1d01712e92b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–º–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ loudly_crying_face\"\n",
      "['–í–ö–£–°_POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(experiment_ids=['0'],\n",
    "                         filter_string='attribute.run_name = \"third_model_experiment\"',\n",
    "                         order_by=['attribute.end_time desc'])\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "\n",
    "print(latest_run_model_id)\n",
    "\n",
    "model_run = client.get_run(latest_run_model_id)\n",
    "\n",
    "artifacts = client.list_artifacts(latest_run_model_id, 'models')\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–∏–º –º–æ–¥–µ–ª—è–º –∏–∑ MLFlow\n",
    "\n",
    "vectoraizer_file = client.download_artifacts(latest_run_model_id, 'models/tfidfvectoraizer.pkl')\n",
    "bayes_file = client.download_artifacts(latest_run_model_id, 'models/naive_bayes.pkl')\n",
    "encoder_file = client.download_artifacts(latest_run_model_id, 'models/labelencoder.pkl')\n",
    "\n",
    "\n",
    "# –ò–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö –≤ –≤–∏–¥–µ –≤—ã–ø–æ–ª–Ω—è–µ–º–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞\n",
    "\n",
    "vectoraizer = joblib.load(vectoraizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "text = '–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠'\n",
    "\n",
    "preprocessed_text = preprocess_func(text)\n",
    "\n",
    "done_text = vectoraizer.transform([preprocessed_text])\n",
    "    \n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(encoder.inverse_transform(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205f87a",
   "metadata": {},
   "source": [
    "### –ò—Ç–æ–≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a20932c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>–í–µ—Ä—Å–∏—è</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>–û–ø–∏—Å–∞–Ω–∏–µ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 1</td>\n",
       "      <td>0.363707</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 2</td>\n",
       "      <td>0.344306</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>–û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 3</td>\n",
       "      <td>0.344139</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     –í–µ—Ä—Å–∏—è  F1-Score  Accuracy                                      –û–ø–∏—Å–∞–Ω–∏–µ\n",
       "0  –í–µ—Ä—Å–∏—è 1  0.363707  0.413043      –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
       "1  –í–µ—Ä—Å–∏—è 2  0.344306  0.391304  –û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text\n",
       "2  –í–µ—Ä—Å–∏—è 3  0.344139  0.391304                               –ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    '–í–µ—Ä—Å–∏—è': ['–í–µ—Ä—Å–∏—è 1', '–í–µ—Ä—Å–∏—è 2', '–í–µ—Ä—Å–∏—è 3'],\n",
    "    'F1-Score': [0.363707, 0.344306, 0.344139],\n",
    "    'Accuracy': [0.413043, 0.391304, 0.391304],\n",
    "    '–û–ø–∏—Å–∞–Ω–∏–µ': ['–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏', '–û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text', '–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏']\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84af634",
   "metadata": {},
   "source": [
    "## –í—Ç–æ—Ä–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2fe0c",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0abe375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>–∏—Ç–æ–≥—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞—Å–∞...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>—É –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2065</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2066</td>\n",
       "      <td>–∫ –ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               span  \\\n",
       "0              0                              –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   \n",
       "1              1  —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...   \n",
       "2              2  —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...   \n",
       "3              3                          –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞   \n",
       "4              4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π   \n",
       "...          ...                                                ...   \n",
       "2063        2063  –∏—Ç–æ–≥—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞—Å–∞...   \n",
       "2064        2064                   —É –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥   \n",
       "2065        2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å   \n",
       "2066        2066                            –∫ –ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å   \n",
       "2067        2067  –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ ...   \n",
       "\n",
       "               label  \n",
       "0      –í–ö–£–°_POSITIVE  \n",
       "1                  O  \n",
       "2                  O  \n",
       "3                  O  \n",
       "4      –í–ö–£–°_NEGATIVE  \n",
       "...              ...  \n",
       "2063   –í–ö–£–°_POSITIVE  \n",
       "2064  –ü–ê–ß–ö–ê_POSITIVE  \n",
       "2065   –í–ö–£–°_POSITIVE  \n",
       "2066   –í–ö–£–°_POSITIVE  \n",
       "2067               O  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/11/14 14:15:12 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential' has no defined input shape yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.2275 - loss: 2.1984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 206ms/step - accuracy: 0.2255 - loss: 2.1665 - val_accuracy: 0.2319 - val_loss: 2.0960\n",
      "Epoch 2/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.2287 - loss: 2.0806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.2733 - loss: 2.0092 - val_accuracy: 0.3382 - val_loss: 1.8667\n",
      "Epoch 3/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.3864 - loss: 1.7235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 150ms/step - accuracy: 0.4081 - loss: 1.6525 - val_accuracy: 0.4444 - val_loss: 1.6026\n",
      "Epoch 4/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.5375 - loss: 1.4016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.5435 - loss: 1.3561 - val_accuracy: 0.4638 - val_loss: 1.4836\n",
      "Epoch 5/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.6509 - loss: 1.0705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 181ms/step - accuracy: 0.6312 - loss: 1.0949 - val_accuracy: 0.4734 - val_loss: 1.4338\n",
      "Epoch 6/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 153ms/step - accuracy: 0.7019 - loss: 0.8826 - val_accuracy: 0.4831 - val_loss: 1.4537\n",
      "Epoch 7/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 160ms/step - accuracy: 0.7485 - loss: 0.7422 - val_accuracy: 0.4928 - val_loss: 1.4907\n",
      "Epoch 8/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 171ms/step - accuracy: 0.7987 - loss: 0.6143 - val_accuracy: 0.5121 - val_loss: 1.5313\n",
      "Epoch 9/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step - accuracy: 0.8404 - loss: 0.5175 - val_accuracy: 0.4976 - val_loss: 1.6287\n",
      "Epoch 10/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - accuracy: 0.8688 - loss: 0.4360 - val_accuracy: 0.4831 - val_loss: 1.6938\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/14 14:18:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/14 14:19:28 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Smart\\AppData\\Local\\Temp\\tmpbyukzg5w\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.20.0', 'cloudpickle==3.1.2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.4396 - loss: 2.0402\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m128\u001b[0m)        ‚îÇ       \u001b[38;5;34m256,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ        \u001b[38;5;34m98,816\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ         \u001b[38;5;34m1,290\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,068,320</span> (4.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,068,320\u001b[0m (4.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,106</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,106\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">712,214</span> (2.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m712,214\u001b[0m (2.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step\n",
      "f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.4556642959166068\n",
      "üèÉ View run first_experiment_neural_network at: http://127.0.0.1:8080/#/experiments/0/runs/bb16c224144d4b47a34f1365031d3cf4\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='first_experiment_neural_network'):\n",
    "    \n",
    "    mlflow.set_tag('LSTM', '1.0.0')\n",
    "\n",
    "    \n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    first_dataset_run = client.search_runs(experiment_ids=['0'],\n",
    "                                           filter_string = \"attributes.run_name='First dataset'\",\n",
    "                                           order_by = ['attributes.end_time desc'])\n",
    "    \n",
    "    first_dataset_run_latest = first_dataset_run[0]\n",
    "    \n",
    "    first_dataset_run_id = first_dataset_run_latest.info.run_id\n",
    "    art = client.download_artifacts(first_dataset_run_id, 'datasets/First_version.csv')\n",
    "\n",
    "    df = pd.read_csv(art)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = 2000, \n",
    "                          oov_token = '<OOV>',\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True,\n",
    "                          split=' ',\n",
    "                          char_level=False)\n",
    "\n",
    "    tokenizer.fit_on_texts(df['span'])\n",
    "\n",
    "\n",
    "    df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "    df_test, df_val = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp['label'])\n",
    "    \n",
    "\n",
    "    X_train_vec = tokenizer.texts_to_sequences(df_train['span'])\n",
    "    X_test_vec = tokenizer.texts_to_sequences(df_test['span'])\n",
    "    X_val_vec = tokenizer.texts_to_sequences(df_val['span'])\n",
    "\n",
    "\n",
    "    max_len_text = 0\n",
    "\n",
    "    for i in df['span']:\n",
    "        max_len_text = max(max_len_text, len(i.split(' ')))\n",
    "\n",
    "    print(max_len_text)\n",
    "\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_vec, max_len_text, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_vec, max_len_text, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_vec, max_len_text, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    y_train = encoder.fit_transform(df_train['label'])\n",
    "    y_test = encoder.transform(df_test['label'])\n",
    "    y_val = encoder.transform(df_val['label'])\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim = 2000, output_dim = 128, input_length = max_len_text))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.3)))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(X_train_pad, y_train, batch_size = 16, epochs = 10, validation_data=(X_val_pad, y_val))\n",
    "\n",
    "    model.evaluate(X_test_pad, y_test)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    pred_proba = model.predict(X_test_pad)\n",
    "\n",
    "    pred_class = np.argmax(pred_proba, axis = 1)\n",
    "    \n",
    "    print(f'f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {f1_score(pred_class, y_test, average = 'weighted')}')\n",
    "\n",
    "    mlflow.log_metric('f1', f1_score(pred_class, y_test, average = 'weighted'))\n",
    "\n",
    "    model.save('LSTM_ver_1.keras')\n",
    "\n",
    "    with open('tokenizer.pkl', 'wb') as f:\n",
    "        cloudpickle.dump(tokenizer, f)\n",
    "\n",
    "    \n",
    "    with open('encoder.pkl', 'wb') as f:\n",
    "        cloudpickle.dump(encoder, f)\n",
    "\n",
    "\n",
    "    mlflow.log_artifact('LSTM_ver_1.keras', 'models')\n",
    "    mlflow.log_artifact('tokenizer.pkl', 'models') \n",
    "    mlflow.log_artifact('encoder.pkl', 'models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ff08f",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8881a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.73s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.56it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.31it/s]\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 11 variables whereas the saved optimizer has 20 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987ms/step\n",
      "['O']\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "last_model_runs = client.search_runs(experiment_ids=['0'],\n",
    "                                    filter_string=\"attributes.run_name = 'first_experiment_neural_network'\",\n",
    "                                    order_by=['attributes.end_time desc'])\n",
    "\n",
    "\n",
    "last_model_run_id = last_model_runs[0].info.run_id\n",
    "\n",
    "nn_loc = client.download_artifacts(last_model_run_id, 'models/LSTM_ver_1.keras')\n",
    "encoder_loc = client.download_artifacts(last_model_run_id, 'models/encoder.pkl')\n",
    "tokenizer_loc = client.download_artifacts(last_model_run_id, 'models/tokenizer.pkl')\n",
    "\n",
    "model_keras = keras.models.load_model(nn_loc)\n",
    "encoder = joblib.load(encoder_loc)\n",
    "tokenizer = joblib.load(tokenizer_loc)\n",
    "\n",
    "\n",
    "# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏\n",
    "\n",
    "text = '–í —Ü–µ–ª–æ–º, —á–∏–ø—Å—ã –ê—à–∞–Ω –ö—Ä–∞—Å–Ω–∞—è –ø—Ç–∏—Ü–∞ –ë–∞—Ä–±–µ–∫—é –≤–ø–æ–ª–Ω–µ —Å—ä–µ–¥–æ–±–Ω—ã–µ'\n",
    "\n",
    "tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "max_len = model_keras.input_shape[1]\n",
    "padded_text = pad_sequences(tokenized_text, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "pred = model_keras.predict(padded_text)\n",
    "\n",
    "predicted_class_ind = np.argmax(pred, axis=1)\n",
    "predicted_class = encoder.inverse_transform(predicted_class_ind)\n",
    "\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d0476",
   "metadata": {},
   "source": [
    "### –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58c3bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>—É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2065</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2066</td>\n",
       "      <td>–∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               span  \\\n",
       "0              0                          –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   \n",
       "1              1  —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...   \n",
       "2              2  —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...   \n",
       "3              3             —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞   \n",
       "4              4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   \n",
       "...          ...                                                ...   \n",
       "2063        2063  check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...   \n",
       "2064        2064             —É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥   \n",
       "2065        2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å   \n",
       "2066        2066                               –∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é   \n",
       "2067        2067  –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...   \n",
       "\n",
       "               label  \n",
       "0      –í–ö–£–°_POSITIVE  \n",
       "1                  O  \n",
       "2                  O  \n",
       "3                  O  \n",
       "4      –í–ö–£–°_NEGATIVE  \n",
       "...              ...  \n",
       "2063   –í–ö–£–°_POSITIVE  \n",
       "2064  –ü–ê–ß–ö–ê_POSITIVE  \n",
       "2065   –í–ö–£–°_POSITIVE  \n",
       "2066   –í–ö–£–°_POSITIVE  \n",
       "2067               O  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/11/14 14:19:47 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential_1' has no defined input shape yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m100/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1403 - loss: 2.2957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.1808 - loss: 2.2741 - val_accuracy: 0.2657 - val_loss: 2.2133\n",
      "Epoch 2/40\n",
      "\u001b[1m 99/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2390 - loss: 2.1599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2527 - loss: 2.1250 - val_accuracy: 0.2995 - val_loss: 2.0611\n",
      "Epoch 3/40\n",
      "\u001b[1m101/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3188 - loss: 1.9072"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.3295 - loss: 1.8683 - val_accuracy: 0.3623 - val_loss: 1.9236\n",
      "Epoch 4/40\n",
      "\u001b[1m100/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3909 - loss: 1.6462"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.3888 - loss: 1.6180 - val_accuracy: 0.3671 - val_loss: 1.8543\n",
      "Epoch 5/40\n",
      "\u001b[1m 99/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4735 - loss: 1.3830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.4680 - loss: 1.3847 - val_accuracy: 0.3913 - val_loss: 1.8164\n",
      "Epoch 6/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5520 - loss: 1.2025 - val_accuracy: 0.3865 - val_loss: 1.8280\n",
      "Epoch 7/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6070 - loss: 1.0507 - val_accuracy: 0.3382 - val_loss: 1.8678\n",
      "Epoch 8/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6469 - loss: 0.9636 - val_accuracy: 0.3430 - val_loss: 1.9461\n",
      "Epoch 9/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6892 - loss: 0.8456 - val_accuracy: 0.3092 - val_loss: 2.0117\n",
      "Epoch 10/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7279 - loss: 0.7700 - val_accuracy: 0.3188 - val_loss: 2.0898\n",
      "Epoch 11/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7745 - loss: 0.6687 - val_accuracy: 0.3285 - val_loss: 2.1958\n",
      "Epoch 12/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7944 - loss: 0.6087 - val_accuracy: 0.3382 - val_loss: 2.3151\n",
      "Epoch 13/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8083 - loss: 0.5391 - val_accuracy: 0.3043 - val_loss: 2.4450\n",
      "Epoch 14/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8368 - loss: 0.4954 - val_accuracy: 0.3430 - val_loss: 2.5048\n",
      "Epoch 15/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8543 - loss: 0.4219 - val_accuracy: 0.3188 - val_loss: 2.5906\n",
      "Epoch 16/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8730 - loss: 0.3824 - val_accuracy: 0.3575 - val_loss: 2.6669\n",
      "Epoch 17/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8791 - loss: 0.3593 - val_accuracy: 0.3527 - val_loss: 2.7960\n",
      "Epoch 18/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8845 - loss: 0.3598 - val_accuracy: 0.3527 - val_loss: 2.8071\n",
      "Epoch 19/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8936 - loss: 0.3085 - val_accuracy: 0.3478 - val_loss: 2.9213\n",
      "Epoch 20/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8996 - loss: 0.2955 - val_accuracy: 0.3430 - val_loss: 3.0270\n",
      "Epoch 21/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9117 - loss: 0.2739 - val_accuracy: 0.3237 - val_loss: 3.1631\n",
      "Epoch 22/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9123 - loss: 0.2704 - val_accuracy: 0.3478 - val_loss: 3.2169\n",
      "Epoch 23/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9178 - loss: 0.2470 - val_accuracy: 0.3478 - val_loss: 3.2923\n",
      "Epoch 24/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9244 - loss: 0.2386 - val_accuracy: 0.3478 - val_loss: 3.4059\n",
      "Epoch 25/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9287 - loss: 0.2144 - val_accuracy: 0.3720 - val_loss: 3.4226\n",
      "Epoch 26/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9202 - loss: 0.2389 - val_accuracy: 0.3671 - val_loss: 3.5264\n",
      "Epoch 27/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9268 - loss: 0.2200 - val_accuracy: 0.3478 - val_loss: 3.6170\n",
      "Epoch 28/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9347 - loss: 0.2088 - val_accuracy: 0.3671 - val_loss: 3.6800\n",
      "Epoch 29/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9299 - loss: 0.2053 - val_accuracy: 0.3575 - val_loss: 3.7965\n",
      "Epoch 30/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9432 - loss: 0.1819 - val_accuracy: 0.3430 - val_loss: 3.8857\n",
      "Epoch 31/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9498 - loss: 0.1582 - val_accuracy: 0.3575 - val_loss: 3.9097\n",
      "Epoch 32/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9335 - loss: 0.1963 - val_accuracy: 0.3478 - val_loss: 3.9976\n",
      "Epoch 33/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9522 - loss: 0.1492 - val_accuracy: 0.3527 - val_loss: 4.1059\n",
      "Epoch 34/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9395 - loss: 0.1732 - val_accuracy: 0.3768 - val_loss: 4.0556\n",
      "Epoch 35/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9510 - loss: 0.1427 - val_accuracy: 0.3623 - val_loss: 4.2409\n",
      "Epoch 36/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9414 - loss: 0.1581 - val_accuracy: 0.3768 - val_loss: 4.1708\n",
      "Epoch 37/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9444 - loss: 0.1649 - val_accuracy: 0.3768 - val_loss: 4.2276\n",
      "Epoch 38/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9528 - loss: 0.1526 - val_accuracy: 0.3527 - val_loss: 4.3643\n",
      "Epoch 39/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9498 - loss: 0.1495 - val_accuracy: 0.3623 - val_loss: 4.3726\n",
      "Epoch 40/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9547 - loss: 0.1420 - val_accuracy: 0.3768 - val_loss: 4.5574\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/14 14:20:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/14 14:21:09 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Smart\\AppData\\Local\\Temp\\tmpsovyssop\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.20.0', 'cloudpickle==3.1.2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">128,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_max_pooling1d            ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ       \u001b[38;5;34m128,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)         ‚îÇ         \u001b[38;5;34m6,176\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)         ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_max_pooling1d            ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ           \u001b[38;5;34m528\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ           \u001b[38;5;34m170\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">404,624</span> (1.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m404,624\u001b[0m (1.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,874</span> (526.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m134,874\u001b[0m (526.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,750</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m269,750\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3575 - loss: 4.1014 \n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000218F239F600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000218F239F600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.352281074230904\n",
      "üèÉ View run second_experiment_neural_network at: http://127.0.0.1:8080/#/experiments/0/runs/96e203d967044ad4ac9f0267b29b3ccb\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='second_experiment_neural_network'):\n",
    "    \n",
    "    mlflow.set_tag('LSTM', '2.0.0')\n",
    "\n",
    "    \n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    first_dataset_run = client.search_runs(experiment_ids=['0'],\n",
    "                                           filter_string = \"attributes.run_name='Second dataset'\",\n",
    "                                           order_by = ['attributes.end_time desc'])\n",
    "    \n",
    "    first_dataset_run_latest = first_dataset_run[0]\n",
    "    \n",
    "    first_dataset_run_id = first_dataset_run_latest.info.run_id\n",
    "    art = client.download_artifacts(first_dataset_run_id, 'datasets/Second_version.csv')\n",
    "\n",
    "    df = pd.read_csv(art)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "        \n",
    "    # =====================================================================================================================================\n",
    "    #                                         –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = 2000, \n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True,\n",
    "                          split=' ',\n",
    "                          char_level=False,\n",
    "                          oov_token=None,\n",
    "                          analyzer=None)\n",
    "    \n",
    "    tokenizer.fit_on_texts(df['span'])\n",
    "\n",
    "    df_train, df_temp = train_test_split(df, test_size = 0.2, random_state = 42)\n",
    "    df_test, df_val = train_test_split(df_temp, test_size=0.5, random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "    X_train_vec = tokenizer.texts_to_sequences(df_train['span'])\n",
    "    X_test_vec = tokenizer.texts_to_sequences(df_test['span'])\n",
    "    X_val_vec = tokenizer.texts_to_sequences(df_val['span'])\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_vec, maxlen=100, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_vec, maxlen=100, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_vec, maxlen=100, padding='post', truncating='post')\n",
    "    \n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    y_train = encoder.fit_transform(df_train['label'])\n",
    "    y_test = encoder.transform(df_test['label'])\n",
    "    y_val = encoder.transform(df_val['label'])\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim = 2000, output_dim = 64, input_length = 100))\n",
    "\n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(Dense(16, activation = 'relu'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(X_train_pad, y_train, batch_size=16, epochs=40, validation_data = (X_val_pad, y_val))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.evaluate(X_test_pad, y_test)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    pred = model.predict(X_test_pad)\n",
    "\n",
    "    pred_class = np.argmax(pred, axis=1)\n",
    "\n",
    "    print(f'f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {f1_score(pred_class, y_test, average = 'weighted')}')\n",
    "\n",
    "    mlflow.log_metric('f1', f1_score(pred_class, y_test, average = 'weighted'))\n",
    "\n",
    "    model.save('LSTM_ver_2.keras')\n",
    "\n",
    "    joblib.dump(tokenizer, 'tokenizer.pkl')\n",
    "    joblib.dump(encoder, 'encoder.pkl')\n",
    "\n",
    "    mlflow.log_artifact('tokenizer.pkl', 'models')\n",
    "    mlflow.log_artifact('encoder.pkl', 'models')\n",
    "    mlflow.log_artifact('LSTM_ver_2.keras', 'models')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e2918",
   "metadata": {},
   "source": [
    "## –¢—Ä–µ—Ç–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74382c43",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78383437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1654/1654 [00:00<00:00, 1884.99 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207/207 [00:00<00:00, 2099.89 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207/207 [00:00<00:00, 2401.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: ['span', 'labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Val features: ['span', 'labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='414' max='414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [414/414 01:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.960500</td>\n",
       "      <td>1.936422</td>\n",
       "      <td>0.230187</td>\n",
       "      <td>0.338164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run transformers_experiment_1 at: http://127.0.0.1:8080/#/experiments/0/runs/15a40ee889404e4180950782979c694b\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='transformers_experiment_1'):\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "    \n",
    "    first_dataset_runs = client.search_runs(experiment_ids=['0'],\n",
    "                             filter_string=\"attributes.run_name = 'First dataset'\",\n",
    "                             order_by=['attributes.end_time desc'])\n",
    "    \n",
    "    need_run = first_dataset_runs[0]\n",
    "    need_run_id = need_run.info.run_id\n",
    "    art_loc = client.download_artifacts(need_run_id, 'datasets/First_version.csv')\n",
    "    df = pd.read_csv(art_loc)\n",
    "    df = df[['span', 'label']]\n",
    "    df = df.rename(columns={\"label\": \"labels\"})\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê/–¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "    \n",
    "    df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42, stratify=df['labels'])\n",
    "    df_test, df_val = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp['labels'])\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(df_train['labels'])\n",
    "    y_test = encoder.transform(df_test['labels'])\n",
    "    y_val = encoder.transform(df_val['labels'])\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "\n",
    "    mlflow.transformers.autolog()\n",
    "\n",
    "    model_name = 'cointegrated/rubert-tiny2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=len(encoder.classes_), \n",
    "        id2label={i: label for i, label in enumerate(encoder.classes_)},\n",
    "        label2id={label: i for i, label in enumerate(encoder.classes_)}\n",
    "    )\n",
    "    \n",
    "    dataset_train = Dataset.from_pandas(df_train.assign(labels=y_train))    \n",
    "    dataset_test = Dataset.from_pandas(df_test.assign(labels=y_test))    \n",
    "    dataset_val = Dataset.from_pandas(df_val.assign(labels=y_val))\n",
    "\n",
    "    def tokenize_dataset(row):\n",
    "        return tokenizer(\n",
    "            row['span'],\n",
    "            truncation=True, \n",
    "            padding=False,\n",
    "            max_length=512,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "    dataset_tokenized_train = dataset_train.map(tokenize_dataset, batched=False)\n",
    "    dataset_tokenized_test = dataset_test.map(tokenize_dataset, batched=False)\n",
    "    dataset_tokenized_val = dataset_val.map(tokenize_dataset, batched=False)\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./models/',\n",
    "        overwrite_output_dir=True,\n",
    "        logging_dir='./logs/',\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=4,   \n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type='cosine',\n",
    "        metric_for_best_model='f1-score',\n",
    "        weight_decay=0.2,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        max_grad_norm=1.0,\n",
    "        logging_steps=100\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return {\n",
    "            'f1-score': f1_score(labels, predictions, average='weighted'),\n",
    "            'accuracy': accuracy_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=dataset_tokenized_train,\n",
    "        eval_dataset=dataset_tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    final_metrics = trainer.evaluate(dataset_tokenized_test)\n",
    "    \n",
    "    model_dir = \"./Transformers_ver_1\"\n",
    "    tokenizer_dir = \"./Tokenizer_transformers_ver_1\"\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "    mlflow.log_artifacts(model_dir, \"model\")\n",
    "    mlflow.log_artifacts(tokenizer_dir, \"tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
