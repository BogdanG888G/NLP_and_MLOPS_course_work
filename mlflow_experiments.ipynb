{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05f1f49",
   "metadata": {},
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Smart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import *\n",
    "\n",
    "import joblib\n",
    "import cloudpickle\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from functools import lru_cache\n",
    "\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import re\n",
    "import emoji\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bda6b4",
   "metadata": {},
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5623ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "def load_config(config_name):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\"\"\"\n",
    "    config_path = f\"configs/{config_name}.yml\"\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "    \n",
    "    cfg = OmegaConf.load(config_path)\n",
    "    \n",
    "    if 'MLFLOW_TRACKING_URI' in os.environ:\n",
    "        cfg.mlflow.tracking_uri = os.environ['MLFLOW_TRACKING_URI']\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558efd8c",
   "metadata": {},
   "source": [
    "# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e78b22",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ba75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!   mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI: http://127.0.0.1:8080\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"base\")\n",
    "\n",
    "mlflow.set_tracking_uri(cfg.mlflow.tracking_uri)\n",
    "\n",
    "print(f\"Tracking URI: {cfg.mlflow.tracking_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27692760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å—Ö–æ–¥–Ω—ã–π: –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ö–∞–∫–æ–µ –∂–µ –Ω–µ–ø—Ä–∏—è—Ç–Ω–æ–µ –º–µ—Å—Ç–æ, –Ω–µ—Ç?\n",
      "\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π: –≤–µ—Å—å –ø—Ä–∏–≤–µ—Ç –∫–∞–∫–æ–π –Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–π –º–µ—Å—Ç–æ –Ω–µ—Ç\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"preprocess_first\")\n",
    "\n",
    "analyzer = MorphAnalyzer(lang='ru')\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "stop_words_cleaned = [\n",
    "    w for w in stop_words\n",
    "    if w not in cfg.preprocess.keep_words\n",
    "]\n",
    "\n",
    "@lru_cache(maxsize=cfg.preprocess.lru_cache_size)\n",
    "def lemmatization(text):\n",
    "    return analyzer.parse(text)[0].normal_form\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(cfg.preprocess.regex.remove_newlines, \" \", text)\n",
    "\n",
    "    text = re.sub(cfg.preprocess.regex.fix_mistyped_n, r\"\\1\", text)\n",
    "\n",
    "    text = re.sub(cfg.preprocess.regex.remove_symbols, \"\", text)\n",
    "\n",
    "    text = re.sub(cfg.preprocess.regex.collapse_spaces, \" \", text).strip()\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    result = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in stop_words_cleaned:\n",
    "            result.append(lemmatization(token))\n",
    "\n",
    "    return \" \".join(result)\n",
    "\n",
    "test_text = \"–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ö–∞–∫–æ–µ –∂–µ –Ω–µ–ø—Ä–∏—è—Ç–Ω–æ–µ –º–µ—Å—Ç–æ, –Ω–µ—Ç?\\n\"\n",
    "processed = preprocess_text(test_text)\n",
    "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π: {test_text}\")\n",
    "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π: {processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52f206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run First dataset at: http://127.0.0.1:8080/#/experiments/0/runs/8690112afeac4c7faf0867c5d30d2dd9\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"First dataset\"):\n",
    "\n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "\n",
    "    annotation_dfs = [\n",
    "        pd.read_json(path) for path in cfg.preprocess.input_files\n",
    "    ]\n",
    "\n",
    "    df_annotations = pd.concat(annotation_dfs)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"span\", \"label\"])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for entry in mark:\n",
    "            span = entry['text']\n",
    "            label = entry['labels'][0]\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "    df['span'] = df['span'].apply(preprocess_text)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    df.to_csv(cfg.preprocess.output.dataset_csv, index=False)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.dataset_csv, \"datasets\")\n",
    "\n",
    "    with open(cfg.preprocess.output.preprocess_pickle, \"wb\") as f:\n",
    "        cloudpickle.dump(preprocess_text, f)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.preprocess_pickle, \"functions\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99a6cf",
   "metadata": {},
   "source": [
    "## –í—Ç–æ—Ä–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7fb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...                  O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...                  O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504       —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                     —Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                  –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Second dataset at: http://127.0.0.1:8080/#/experiments/0/runs/ba68188717234d8ea8f01745d4816ea8\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "cfg = load_config('preprocess_second')\n",
    "\n",
    "def clean_text_only(text, cfg=None):\n",
    "    \"\"\"\n",
    "    –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "    \n",
    "    Args:\n",
    "        text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç\n",
    "        cfg: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    lowercase = getattr(cfg, 'clean_only', {}).get('lowercase', True) if cfg else True\n",
    "    replace_emoji = getattr(cfg, 'clean_only', {}).get('replace_emoji', True) if cfg else True\n",
    "    remove_punctuation = getattr(cfg, 'clean_only', {}).get('remove_punctuation', True) if cfg else True\n",
    "    remove_special_chars = getattr(cfg, 'clean_only', {}).get('remove_special_chars', True) if cfg else True\n",
    "    collapse_spaces = getattr(cfg, 'clean_only', {}).get('collapse_spaces', True) if cfg else True\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if replace_emoji:\n",
    "        text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    if remove_special_chars:\n",
    "        text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
    "    \n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    if collapse_spaces:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text_only_legacy(text):\n",
    "    \"\"\"–õ–µ–≥–∞—Å–∏ –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\"\"\"\n",
    "    return clean_text_only(text)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name='Second dataset'):\n",
    "    \n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "\n",
    "    annotation_dfs = [\n",
    "        pd.read_json(path) for path in cfg.preprocess.input_files\n",
    "    ]\n",
    "\n",
    "    df_annotations = pd.concat(annotation_dfs)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"span\", \"label\"])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for entry in mark:\n",
    "            span = entry['text']\n",
    "            label = entry['labels'][0]\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "    df['span'] = df['span'].apply(lambda x: clean_text_only(x, cfg))\n",
    "    \n",
    "    display(df)\n",
    "\n",
    "    df.to_csv(cfg.preprocess.output.dataset_csv, index=False)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.dataset_csv, \"datasets\")\n",
    "\n",
    "    clean_text_with_config = partial(clean_text_only, cfg=cfg)\n",
    "    with open(cfg.preprocess.output.preprocess_pickle, \"wb\") as f:\n",
    "        cloudpickle.dump(clean_text_with_config, f)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.preprocess_pickle, \"functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0766fa",
   "metadata": {},
   "source": [
    "## –¢—Ä–µ—Ç–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∏–Ω–æ—Ä–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b861f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minority classes: ['–ü–ê–ß–ö–ê_POSITIVE', '–¢–ï–ö–°–¢–£–†–ê_NEUTRAL', '–ü–ê–ß–ö–ê_NEUTRAL', '–¢–ï–ö–°–¢–£–†–ê_NEGATIVE', '–ü–ê–ß–ö–ê_NEGATIVE']\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π: third_experiment_dataset.csv\n",
      "üèÉ View run Third dataset at: http://127.0.0.1:8080/#/experiments/0/runs/327f7f34ba914fce83ca6c0e787e8f15\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Third dataset\"):\n",
    "\n",
    "    cfg = load_config(\"preprocess_third\")\n",
    "\n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "\n",
    "\n",
    "    annotation_dfs = [pd.read_json(path) for path in cfg.preprocess.input_files]\n",
    "    df_annotations = pd.concat(annotation_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"span\", \"label\"])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for entry in mark:\n",
    "            span = entry['text']\n",
    "            label = entry['labels'][0]\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "\n",
    "    class_counts = df['label'].value_counts()\n",
    "    minority_classes = class_counts[class_counts < class_counts.mean()].index.tolist()\n",
    "\n",
    "    print(f\"Minority classes: {minority_classes}\")\n",
    "\n",
    "\n",
    "    import random\n",
    "\n",
    "    synonyms = cfg.preprocess.augmentation.synonyms\n",
    "    n_variants = cfg.preprocess.augmentation.n_variants\n",
    "\n",
    "    def simple_augmentation(text):\n",
    "        augmented = []\n",
    "        for _ in range(n_variants):\n",
    "            words = text.split()\n",
    "            new_words = []\n",
    "            for w in words:\n",
    "                wl = w.lower()\n",
    "                if wl in synonyms and random.random() > 0.7:\n",
    "                    new_words.append(random.choice(synonyms[wl]))\n",
    "                else:\n",
    "                    new_words.append(w)\n",
    "            augmented.append(\" \".join(new_words))\n",
    "        return augmented\n",
    "\n",
    "\n",
    "    augmented_data = []\n",
    "    for label in minority_classes:\n",
    "        samples = df[df['label'] == label]\n",
    "        for _, row in samples.iterrows():\n",
    "            for aug_text in simple_augmentation(row[\"span\"]):\n",
    "                augmented_data.append({\"span\": aug_text, \"label\": label})\n",
    "\n",
    "    df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "\n",
    "    df_extended = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "\n",
    "    output_path = cfg.preprocess.output.dataset_csv\n",
    "    df_extended.to_csv(output_path, index=False)\n",
    "\n",
    "    mlflow.log_artifact(output_path, \"datasets\")\n",
    "\n",
    "    print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790af86c",
   "metadata": {},
   "source": [
    "# –≠–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–æ–¥–µ–ª—è–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b384f",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793612b",
   "metadata": {},
   "source": [
    "–ë—É–¥–µ–º –æ–±—É—á–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å: \"–ù–∞–∏–≤–Ω—ã–π –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\" –∏–∑ `Sklearn`, –±—É–¥–µ–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ 3 –≤–µ—Ä—Å–∏—è—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø–æ–π–º–µ–º, –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ —Å–µ–±—è –ø–æ–∫–∞–∂–µ—Ç –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ–º –∏–ª–∏ –∏–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead861b4",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: datasets/first_experiment_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.3869\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.4422\n",
      "–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run first_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/64117c4a566b416fb2c5adeafb5de204\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"naive_bayes_first\")\n",
    "\n",
    "with mlflow.start_run(run_name='first_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    full_dataset_path = f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\"\n",
    "    \n",
    "    try:\n",
    "        dataframe_path = client.download_artifacts(first_dataset_latest_run_id, full_dataset_path)\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {full_dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {full_dataset_path}: {e}\")\n",
    "\n",
    "        try:\n",
    "            dataframe_path = client.download_artifacts(first_dataset_latest_run_id, cfg.data.dataset_file)\n",
    "            df = pd.read_csv(dataframe_path)\n",
    "            print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {cfg.data.dataset_file}\")\n",
    "        except:\n",
    "            raise ValueError(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç: {full_dataset_path}\")\n",
    "    \n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=cfg.vectorizer.lowercase,\n",
    "        analyzer=cfg.vectorizer.analyzer,\n",
    "        max_features=cfg.vectorizer.max_features,\n",
    "        ngram_range=tuple(cfg.vectorizer.ngram_range),\n",
    "        min_df=cfg.vectorizer.min_df,\n",
    "        max_df=cfg.vectorizer.max_df\n",
    "    )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    mlflow.sklearn.autolog(disable=True)\n",
    "    \n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy:.4f}')\n",
    "\n",
    "    joblib.dump(model, cfg.model.artifacts.model)\n",
    "    joblib.dump(vectorizer, cfg.model.artifacts.vectorizer)\n",
    "    joblib.dump(encoder, cfg.model.artifacts.encoder)\n",
    "    \n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.vectorizer, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'vectorizer_max_features': cfg.vectorizer.max_features,\n",
    "        'vectorizer_ngram_range': str(cfg.vectorizer.ngram_range),\n",
    "        'test_size': cfg.training.test_size\n",
    "    })\n",
    "\n",
    "    print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ddbdb",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93388f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 64117c4a566b416fb2c5adeafb5de204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.86it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 29.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"—è –ø—Ä–∏–≤–ª–µ—á—å —Ç–∞–∫–æ–π —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_POSITIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_bayes_first\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "\n",
    "vectorizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.vectorizer}\"\n",
    "bayes_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.bayes}\" \n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "try:\n",
    "    vectorizer_file = client.download_artifacts(latest_run_model_id, vectorizer_path)\n",
    "    bayes_file = client.download_artifacts(latest_run_model_id, bayes_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "\n",
    "    try:\n",
    "        vectorizer_file = client.download_artifacts(latest_run_model_id, cfg_inference.model.vectorizer)\n",
    "        bayes_file = client.download_artifacts(latest_run_model_id, cfg_inference.model.bayes)\n",
    "        encoder_file = client.download_artifacts(latest_run_model_id, cfg_inference.model.encoder)\n",
    "        print(\"‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (fallback)\")\n",
    "    except:\n",
    "        raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –§–£–ù–ö–¶–ò–ò –ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì–ê\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_dataset = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.preprocess.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "latest_run_dataset_id = latest_run_dataset[0].info.run_id\n",
    "\n",
    "try:\n",
    "    art_loc = client.download_artifacts(latest_run_dataset_id, cfg_inference.preprocess.artifact_path)\n",
    "    print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "vectorizer = joblib.load(vectorizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "preprocessed_text = preprocess_func(text)\n",
    "done_text = vectorizer.transform([preprocessed_text])\n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {encoder.inverse_transform(label)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3439a",
   "metadata": {},
   "source": [
    "### –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ca78c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...                  O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...                  O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504       —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                     —Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                  –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.3675\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.4223\n",
      "–í—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run second_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/2414d58064a146a69e694625b0041c82\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = load_config(\"naive_bayes_second\")\n",
    "\n",
    "with mlflow.start_run(run_name='second_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "\n",
    "    try:\n",
    "        dataframe_path = client.download_artifacts(first_dataset_latest_run_id, \"datasets/second_experiment_dataset.csv\")\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        print(\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\")\n",
    "    except:\n",
    "        try:\n",
    "            files = client.list_artifacts(first_dataset_latest_run_id, cfg.data.dataset_path)\n",
    "            dataframe = files[0].path\n",
    "            dataframe_path = client.download_artifacts(first_dataset_latest_run_id, dataframe)\n",
    "            df = pd.read_csv(dataframe_path)\n",
    "            print(\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —á–µ—Ä–µ–∑ list_artifacts\")\n",
    "        except:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=cfg.vectorizer.lowercase,\n",
    "        analyzer=cfg.vectorizer.analyzer,\n",
    "        max_features=cfg.vectorizer.max_features,\n",
    "        ngram_range=tuple(cfg.vectorizer.ngram_range),\n",
    "        min_df=cfg.vectorizer.min_df,\n",
    "        max_df=cfg.vectorizer.max_df\n",
    "    )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy:.4f}')\n",
    "\n",
    "    joblib.dump(model, cfg.model.artifacts.model)\n",
    "    joblib.dump(vectorizer, cfg.model.artifacts.vectorizer)\n",
    "    joblib.dump(encoder, cfg.model.artifacts.encoder)\n",
    "    \n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.vectorizer, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'vectorizer_max_features': cfg.vectorizer.max_features,\n",
    "        'vectorizer_ngram_range': str(cfg.vectorizer.ngram_range),\n",
    "        'test_size': cfg.training.test_size\n",
    "    })\n",
    "\n",
    "    print(\"–í—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfbf25",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f598e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 2414d58064a146a69e694625b0041c82\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.57it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 22.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞...\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–º–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ loudly_crying_face\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_NEGATIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_bayes_second\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"', \n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "\n",
    "vectorizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.vectorizer}\"\n",
    "bayes_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.bayes}\"\n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\")\n",
    "try:\n",
    "    vectorizer_file = client.download_artifacts(latest_run_model_id, vectorizer_path)\n",
    "    bayes_file = client.download_artifacts(latest_run_model_id, bayes_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –§–£–ù–ö–¶–ò–ò –ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì–ê\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_dataset = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.preprocess.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_dataset:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.preprocess.run_name}\")\n",
    "\n",
    "latest_run_dataset_id = latest_run_dataset[0].info.run_id\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞...\")\n",
    "try:\n",
    "    art_loc = client.download_artifacts(latest_run_dataset_id, cfg_inference.preprocess.artifact_path)\n",
    "    print(\"–§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "vectorizer = joblib.load(vectorizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞...\")\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "preprocessed_text = preprocess_func(text)\n",
    "done_text = vectorizer.transform([preprocessed_text])\n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {encoder.inverse_transform(label)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231b62e",
   "metadata": {},
   "source": [
    "### –¢—Ä–µ—Ç–∏–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d475c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: 327f7f34ba914fce83ca6c0e787e8f15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>—Å —á–∏–ø—Å–∞–º–∏ –≤—Å–µ–≥–¥–∞ —Å—Ç–æ–∏—Ç –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ –¥–≤–µ —Ç—Ä–µ—Ç–∏ ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>–ü–ª–∞—Å—Ç–∏–∫–æ–≤–∞—è –∫—Ä—ã—à–∫–∞ –ø–æ–ø–∞–ª–∞—Å—å –æ—á–µ–Ω—å —Ç—É–≥–∞—è, –µ–ª–µ-–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>–ü–ª–∞—Å—Ç–∏–∫–æ–≤–∞—è –∫—Ä—ã—à–∫–∞ –ø–æ–ø–∞–ª–∞—Å—å –æ—á–µ–Ω—å —Ç—É–≥–∞—è, –µ–ª–µ-–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>–ù–∞–ø–æ–ª–Ω–µ–∏–Ω–µ –ø–∞—á–∫–∏ —Ç–∏–ø–∏—á–Ω–æ–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>–ù–∞–ø–æ–ª–Ω–µ–∏–Ω–µ –ø–∞—á–∫–∏ —Ç–∏–ø–∏—á–Ω–æ–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4020 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span           label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   –í–ö–£–°_POSITIVE\n",
       "1     –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...               O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...               O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞               O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...             ...\n",
       "4015  —Å —á–∏–ø—Å–∞–º–∏ –≤—Å–µ–≥–¥–∞ —Å—Ç–æ–∏—Ç –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ –¥–≤–µ —Ç—Ä–µ—Ç–∏ ...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "4016  –ü–ª–∞—Å—Ç–∏–∫–æ–≤–∞—è –∫—Ä—ã—à–∫–∞ –ø–æ–ø–∞–ª–∞—Å—å –æ—á–µ–Ω—å —Ç—É–≥–∞—è, –µ–ª–µ-–µ...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "4017  –ü–ª–∞—Å—Ç–∏–∫–æ–≤–∞—è –∫—Ä—ã—à–∫–∞ –ø–æ–ø–∞–ª–∞—Å—å –æ—á–µ–Ω—å —Ç—É–≥–∞—è, –µ–ª–µ-–µ...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "4018  –ù–∞–ø–æ–ª–Ω–µ–∏–Ω–µ –ø–∞—á–∫–∏ —Ç–∏–ø–∏—á–Ω–æ–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "4019  –ù–∞–ø–æ–ª–Ω–µ–∏–Ω–µ –ø–∞—á–∫–∏ —Ç–∏–ø–∏—á–Ω–æ–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "\n",
       "[4020 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.5517\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.5958\n",
      "–¢—Ä–µ—Ç—å—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run third_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/b5cc3981bcdd47d2ae00e73f194e41d0\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"naive_bayes_third\")\n",
    "\n",
    "with mlflow.start_run(run_name='third_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        dataframe_path = client.download_artifacts(dataset_run_id, f\"datasets/{cfg.data.dataset_file}\")\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        print(\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "        alternative_paths = [\n",
    "            cfg.data.dataset_file,\n",
    "            f\"artifacts/datasets/{cfg.data.dataset_file}\",\n",
    "            \"third_experiment_dataset.csv\"\n",
    "        ]\n",
    "        \n",
    "        for path in alternative_paths:\n",
    "            try:\n",
    "                dataframe_path = client.download_artifacts(dataset_run_id, path)\n",
    "                df = pd.read_csv(dataframe_path)\n",
    "                print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=cfg.vectorizer.lowercase,\n",
    "        analyzer=cfg.vectorizer.analyzer,\n",
    "        max_features=cfg.vectorizer.max_features,\n",
    "        ngram_range=tuple(cfg.vectorizer.ngram_range),\n",
    "        min_df=cfg.vectorizer.min_df,\n",
    "        max_df=cfg.vectorizer.max_df\n",
    "    )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy:.4f}')\n",
    "\n",
    "    joblib.dump(model, cfg.model.artifacts.model)\n",
    "    joblib.dump(vectorizer, cfg.model.artifacts.vectorizer)\n",
    "    joblib.dump(encoder, cfg.model.artifacts.encoder)\n",
    "    \n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.vectorizer, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'vectorizer_max_features': cfg.vectorizer.max_features,\n",
    "        'vectorizer_ngram_range': str(cfg.vectorizer.ngram_range),\n",
    "        'test_size': cfg.training.test_size\n",
    "    })\n",
    "\n",
    "    print(\"–¢—Ä–µ—Ç—å—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20a7ee",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: b5cc3981bcdd47d2ae00e73f194e41d0\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.34it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.17it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–º–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ loudly_crying_face\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –ü–ê–ß–ö–ê_POSITIVE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_bayes_third\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "\n",
    "vectorizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.vectorizer}\"\n",
    "bayes_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.bayes}\"\n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\")\n",
    "try:\n",
    "    vectorizer_file = client.download_artifacts(latest_run_model_id, vectorizer_path)\n",
    "    bayes_file = client.download_artifacts(latest_run_model_id, bayes_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "vectorizer = joblib.load(vectorizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "done_text = vectorizer.transform([text])\n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {encoder.inverse_transform(label)[0]}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205f87a",
   "metadata": {},
   "source": [
    "### –ò—Ç–æ–≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a20932c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>–í–µ—Ä—Å–∏—è</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>–û–ø–∏—Å–∞–Ω–∏–µ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 1</td>\n",
       "      <td>0.363707</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 2</td>\n",
       "      <td>0.344306</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>–û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 3</td>\n",
       "      <td>0.344139</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     –í–µ—Ä—Å–∏—è  F1-Score  Accuracy                                      –û–ø–∏—Å–∞–Ω–∏–µ\n",
       "0  –í–µ—Ä—Å–∏—è 1  0.363707  0.413043      –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
       "1  –í–µ—Ä—Å–∏—è 2  0.344306  0.391304  –û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text\n",
       "2  –í–µ—Ä—Å–∏—è 3  0.344139  0.391304                               –ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    '–í–µ—Ä—Å–∏—è': ['–í–µ—Ä—Å–∏—è 1', '–í–µ—Ä—Å–∏—è 2', '–í–µ—Ä—Å–∏—è 3'],\n",
    "    'F1-Score': [0.363707, 0.344306, 0.344139],\n",
    "    'Accuracy': [0.413043, 0.391304, 0.391304],\n",
    "    '–û–ø–∏—Å–∞–Ω–∏–µ': ['–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏', '–û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text', '–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏']\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84af634",
   "metadata": {},
   "source": [
    "## –í—Ç–æ—Ä–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2fe0c",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: 8690112afeac4c7faf0867c5d30d2dd9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/11/26 01:15:28 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential' has no defined input shape yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 144ms/step - accuracy: 0.1924 - loss: 2.1970 - val_accuracy: 0.2032 - val_loss: 2.1249\n",
      "Epoch 2/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 116ms/step - accuracy: 0.2832 - loss: 1.9663 - val_accuracy: 0.3187 - val_loss: 1.8309\n",
      "Epoch 3/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.4526 - loss: 1.5831 - val_accuracy: 0.4183 - val_loss: 1.6108\n",
      "Epoch 4/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - accuracy: 0.5718 - loss: 1.2474 - val_accuracy: 0.4542 - val_loss: 1.5475\n",
      "Epoch 5/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 130ms/step - accuracy: 0.6849 - loss: 0.9882 - val_accuracy: 0.4462 - val_loss: 1.5364\n",
      "Epoch 6/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 128ms/step - accuracy: 0.7443 - loss: 0.8044 - val_accuracy: 0.4582 - val_loss: 1.6953\n",
      "Epoch 7/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 122ms/step - accuracy: 0.7787 - loss: 0.6762 - val_accuracy: 0.4582 - val_loss: 1.7486\n",
      "Epoch 8/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.8170 - loss: 0.5724 - val_accuracy: 0.4422 - val_loss: 1.8520\n",
      "Epoch 9/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 119ms/step - accuracy: 0.8485 - loss: 0.4931 - val_accuracy: 0.4303 - val_loss: 2.0296\n",
      "Epoch 10/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 137ms/step - accuracy: 0.8674 - loss: 0.4263 - val_accuracy: 0.4303 - val_loss: 2.1174\n",
      "Epoch 11/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.8883 - loss: 0.3677 - val_accuracy: 0.4422 - val_loss: 2.2669\n",
      "Epoch 12/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 114ms/step - accuracy: 0.9068 - loss: 0.3271 - val_accuracy: 0.4382 - val_loss: 2.3150\n",
      "Epoch 13/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.9192 - loss: 0.2829 - val_accuracy: 0.4064 - val_loss: 2.4181\n",
      "Epoch 14/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.9217 - loss: 0.2582 - val_accuracy: 0.4382 - val_loss: 2.4972\n",
      "Epoch 15/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.9377 - loss: 0.2218 - val_accuracy: 0.4183 - val_loss: 2.5767\n",
      "Epoch 16/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 121ms/step - accuracy: 0.9327 - loss: 0.2299 - val_accuracy: 0.4064 - val_loss: 2.6018\n",
      "Epoch 17/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.9467 - loss: 0.1856 - val_accuracy: 0.4104 - val_loss: 2.7538\n",
      "Epoch 18/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 122ms/step - accuracy: 0.9521 - loss: 0.1740 - val_accuracy: 0.4343 - val_loss: 2.8200\n",
      "Epoch 19/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.9541 - loss: 0.1552 - val_accuracy: 0.4303 - val_loss: 2.9397\n",
      "Epoch 20/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.9536 - loss: 0.1515 - val_accuracy: 0.4064 - val_loss: 2.9252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 01:21:02 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: API request to endpoint /api/2.0/mlflow/logged-models failed with error code 404 != 200. Response body: '<!doctype html>\n",
      "<html lang=en>\n",
      "<title>404 Not Found</title>\n",
      "<h1>Not Found</h1>\n",
      "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n",
      "'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4701 - loss: 2.8788\n",
      "Test Accuracy: 0.4701\n",
      "Test Loss: 2.8788\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m128\u001b[0m)        ‚îÇ       \u001b[38;5;34m256,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ        \u001b[38;5;34m98,816\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ         \u001b[38;5;34m1,290\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,068,320</span> (4.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,068,320\u001b[0m (4.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,106</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,106\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">712,214</span> (2.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m712,214\u001b[0m (2.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 270ms/step\n",
      "f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.4730\n",
      "accuracy —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.4701\n",
      "‚úÖ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run first_experiment_neural_network at: http://127.0.0.1:8080/#/experiments/0/runs/bd1c61c3551940db8aa3011106254c09\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = load_config(\"neural_network_first\")\n",
    "\n",
    "with mlflow.start_run(run_name='first_experiment_neural_network'):\n",
    "    \n",
    "    mlflow.set_tag('LSTM', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "    try:\n",
    "        dataframe_path = client.download_artifacts(dataset_run_id, f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\")\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        print(\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "        try:\n",
    "            files = client.list_artifacts(dataset_run_id, cfg.data.dataset_path)\n",
    "            dataframe = files[0].path\n",
    "            dataframe_path = client.download_artifacts(dataset_run_id, dataframe)\n",
    "            df = pd.read_csv(dataframe_path)\n",
    "            print(\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —á–µ—Ä–µ–∑ list_artifacts\")\n",
    "        except Exception as e2:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ list_artifacts: {e2}\")\n",
    "            alternative_paths = [\n",
    "                cfg.data.dataset_file,\n",
    "                f\"artifacts/{cfg.data.dataset_path}/{cfg.data.dataset_file}\",\n",
    "                \"First_version.csv\"\n",
    "            ]\n",
    "            \n",
    "            for path in alternative_paths:\n",
    "                try:\n",
    "                    dataframe_path = client.download_artifacts(dataset_run_id, path)\n",
    "                    df = pd.read_csv(dataframe_path)\n",
    "                    print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df)\n",
    "    \n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=cfg.tokenizer.num_words,\n",
    "        oov_token=cfg.tokenizer.oov_token,\n",
    "        filters=cfg.tokenizer.filters,\n",
    "        lower=cfg.tokenizer.lower,\n",
    "        split=cfg.tokenizer.split,\n",
    "        char_level=cfg.tokenizer.char_level\n",
    "    )\n",
    "\n",
    "    tokenizer.fit_on_texts(df['span'])\n",
    "\n",
    "    df_train, df_temp = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    df_test, df_val = train_test_split(\n",
    "        df_temp, \n",
    "        test_size=cfg.training.val_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df_temp['label']\n",
    "    )\n",
    "\n",
    "    X_train_vec = tokenizer.texts_to_sequences(df_train['span'])\n",
    "    X_test_vec = tokenizer.texts_to_sequences(df_test['span'])\n",
    "    X_val_vec = tokenizer.texts_to_sequences(df_val['span'])\n",
    "\n",
    "    max_len_text = 0\n",
    "    for i in df['span']:\n",
    "        max_len_text = max(max_len_text, len(i.split(' ')))\n",
    "    print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {max_len_text}\")\n",
    "\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_vec, max_len_text, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_vec, max_len_text, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_vec, max_len_text, padding='post', truncating='post')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    y_train = encoder.fit_transform(df_train['label'])\n",
    "    y_test = encoder.transform(df_test['label'])\n",
    "    y_val = encoder.transform(df_val['label'])\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(\n",
    "        input_dim=cfg.tokenizer.num_words, \n",
    "        output_dim=cfg.model.embedding_dim, \n",
    "        input_length=max_len_text\n",
    "    ))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        cfg.model.lstm_units, \n",
    "        dropout=0.2, \n",
    "        recurrent_dropout=0.3\n",
    "    )))\n",
    "\n",
    "    model.add(Dropout(cfg.model.dropout_rate))\n",
    "\n",
    "    model.add(Dense(cfg.model.dense_units, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, \n",
    "        y_train, \n",
    "        batch_size=cfg.training.batch_size, \n",
    "        epochs=cfg.training.epochs, \n",
    "        validation_data=(X_val_pad, y_val)\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    pred_proba = model.predict(X_test_pad)\n",
    "    pred_class = np.argmax(pred_proba, axis=1)\n",
    "    \n",
    "    f1 = f1_score(pred_class, y_test, average='weighted')\n",
    "    accuracy = accuracy_score(pred_class, y_test)\n",
    "\n",
    "    print(f'f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "    print(f'accuracy —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {accuracy:.4f}')\n",
    "\n",
    "    mlflow.log_metric('f1_score', f1)\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "    mlflow.log_metric('test_accuracy', test_accuracy)\n",
    "    mlflow.log_metric('test_loss', test_loss)\n",
    "\n",
    "    model.save(\"LSTM_ver_1.keras\")\n",
    "\n",
    "    with open(cfg.model.artifacts.tokenizer, 'wb') as f:\n",
    "        cloudpickle.dump(tokenizer, f)\n",
    "    \n",
    "    with open(cfg.model.artifacts.encoder, 'wb') as f:\n",
    "        cloudpickle.dump(encoder, f)\n",
    "\n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.tokenizer, 'models') \n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'num_words': cfg.tokenizer.num_words,\n",
    "        'embedding_dim': cfg.model.embedding_dim,\n",
    "        'lstm_units': cfg.model.lstm_units,\n",
    "        'dropout_rate': cfg.model.dropout_rate,\n",
    "        'batch_size': cfg.training.batch_size,\n",
    "        'epochs': cfg.training.epochs\n",
    "    })\n",
    "\n",
    "    print(\"–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ff08f",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: bd1c61c3551940db8aa3011106254c09\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.73it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 73\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–í —Ü–µ–ª–æ–º, —á–∏–ø—Å—ã –ê—à–∞–Ω –ö—Ä–∞—Å–Ω–∞—è –ø—Ç–∏—Ü–∞ –ë–∞—Ä–±–µ–∫—é –≤–ø–æ–ª–Ω–µ —Å—ä–µ–¥–æ–±–Ω—ã–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_NEUTRAL\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:\n",
      "  O: 0.0332\n",
      "  –í–ö–£–°_NEGATIVE: 0.0001\n",
      "  –í–ö–£–°_NEUTRAL: 0.9660\n",
      "  –í–ö–£–°_POSITIVE: 0.0002\n",
      "  –ü–ê–ß–ö–ê_NEGATIVE: 0.0001\n",
      "  –ü–ê–ß–ö–ê_NEUTRAL: 0.0002\n",
      "  –ü–ê–ß–ö–ê_POSITIVE: 0.0000\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEGATIVE: 0.0000\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEUTRAL: 0.0001\n",
      "  –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 0.0002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_neural_network\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "model_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.model_file}\"\n",
    "tokenizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.tokenizer}\"\n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\")\n",
    "try:\n",
    "    model_file = client.download_artifacts(latest_run_model_id, model_path)\n",
    "    tokenizer_file = client.download_artifacts(latest_run_model_id, tokenizer_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "model_keras = tf.keras.models.load_model(model_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "tokenizer = joblib.load(tokenizer_file)\n",
    "\n",
    "print(\"–í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "\n",
    "tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "max_len = model_keras.input_shape[1]\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_len}\")\n",
    "\n",
    "padded_text = pad_sequences(tokenized_text, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "pred = model_keras.predict(padded_text)\n",
    "\n",
    "predicted_class_ind = np.argmax(pred, axis=1)\n",
    "predicted_class = encoder.inverse_transform(predicted_class_ind)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {predicted_class[0]}')\n",
    "print(f'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:')\n",
    "for i, prob in enumerate(pred[0]):\n",
    "    class_name = encoder.inverse_transform([i])[0]\n",
    "    print(f'  {class_name}: {prob:.4f}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d0476",
   "metadata": {},
   "source": [
    "### –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: ba68188717234d8ea8f01745d4816ea8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: datasets/second_experiment_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...                  O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...                  O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504       —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                     —Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                  –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/11/26 01:21:32 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential_1' has no defined input shape yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.1939 - loss: 2.2327 - val_accuracy: 0.1753 - val_loss: 2.2185\n",
      "Epoch 2/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2094 - loss: 2.1531 - val_accuracy: 0.1873 - val_loss: 2.1567\n",
      "Epoch 3/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.2662 - loss: 2.0085 - val_accuracy: 0.2829 - val_loss: 2.0674\n",
      "Epoch 4/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3988 - loss: 1.7389 - val_accuracy: 0.4104 - val_loss: 1.9122\n",
      "Epoch 5/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5040 - loss: 1.4663 - val_accuracy: 0.4183 - val_loss: 1.8126\n",
      "Epoch 6/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5897 - loss: 1.2436 - val_accuracy: 0.4422 - val_loss: 1.7648\n",
      "Epoch 7/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6441 - loss: 1.0564 - val_accuracy: 0.3984 - val_loss: 1.7366\n",
      "Epoch 8/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6949 - loss: 0.9225 - val_accuracy: 0.4382 - val_loss: 1.7570\n",
      "Epoch 9/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7488 - loss: 0.7733 - val_accuracy: 0.4303 - val_loss: 1.7986\n",
      "Epoch 10/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7612 - loss: 0.7324 - val_accuracy: 0.4382 - val_loss: 1.8286\n",
      "Epoch 11/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7996 - loss: 0.6354 - val_accuracy: 0.4343 - val_loss: 1.8701\n",
      "Epoch 12/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8011 - loss: 0.5709 - val_accuracy: 0.4303 - val_loss: 1.9190\n",
      "Epoch 13/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8400 - loss: 0.5036 - val_accuracy: 0.4183 - val_loss: 2.0120\n",
      "Epoch 14/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8290 - loss: 0.4983 - val_accuracy: 0.4143 - val_loss: 2.0961\n",
      "Epoch 15/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8524 - loss: 0.4441 - val_accuracy: 0.3984 - val_loss: 2.1284\n",
      "Epoch 16/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8569 - loss: 0.4259 - val_accuracy: 0.4024 - val_loss: 2.1865\n",
      "Epoch 17/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8589 - loss: 0.4202 - val_accuracy: 0.4104 - val_loss: 2.2258\n",
      "Epoch 18/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8794 - loss: 0.3834 - val_accuracy: 0.4024 - val_loss: 2.3754\n",
      "Epoch 19/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8873 - loss: 0.3470 - val_accuracy: 0.3865 - val_loss: 2.4250\n",
      "Epoch 20/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8789 - loss: 0.3423 - val_accuracy: 0.4104 - val_loss: 2.4676\n",
      "Epoch 21/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8918 - loss: 0.3123 - val_accuracy: 0.3984 - val_loss: 2.5724\n",
      "Epoch 22/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9053 - loss: 0.2960 - val_accuracy: 0.4024 - val_loss: 2.6514\n",
      "Epoch 23/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8978 - loss: 0.2968 - val_accuracy: 0.4064 - val_loss: 2.7179\n",
      "Epoch 24/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9093 - loss: 0.2843 - val_accuracy: 0.4024 - val_loss: 2.7960\n",
      "Epoch 25/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9078 - loss: 0.2829 - val_accuracy: 0.4024 - val_loss: 2.7997\n",
      "Epoch 26/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9118 - loss: 0.2655 - val_accuracy: 0.3944 - val_loss: 2.8634\n",
      "Epoch 27/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9202 - loss: 0.2603 - val_accuracy: 0.4064 - val_loss: 2.9345\n",
      "Epoch 28/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9282 - loss: 0.2419 - val_accuracy: 0.3865 - val_loss: 3.0599\n",
      "Epoch 29/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9163 - loss: 0.2363 - val_accuracy: 0.4024 - val_loss: 3.1243\n",
      "Epoch 30/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9272 - loss: 0.2279 - val_accuracy: 0.3944 - val_loss: 3.2254\n",
      "Epoch 31/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9327 - loss: 0.2101 - val_accuracy: 0.3944 - val_loss: 3.2242\n",
      "Epoch 32/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9262 - loss: 0.2167 - val_accuracy: 0.4024 - val_loss: 3.3131\n",
      "Epoch 33/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9292 - loss: 0.2146 - val_accuracy: 0.3785 - val_loss: 3.4280\n",
      "Epoch 34/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9302 - loss: 0.2184 - val_accuracy: 0.3825 - val_loss: 3.4083\n",
      "Epoch 35/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9262 - loss: 0.2153 - val_accuracy: 0.3785 - val_loss: 3.4641\n",
      "Epoch 36/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9327 - loss: 0.2058 - val_accuracy: 0.4024 - val_loss: 3.5492\n",
      "Epoch 37/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9317 - loss: 0.1912 - val_accuracy: 0.4024 - val_loss: 3.6155\n",
      "Epoch 38/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9407 - loss: 0.1717 - val_accuracy: 0.3984 - val_loss: 3.7692\n",
      "Epoch 39/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9422 - loss: 0.1778 - val_accuracy: 0.4024 - val_loss: 3.7990\n",
      "Epoch 40/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9402 - loss: 0.1722 - val_accuracy: 0.3944 - val_loss: 3.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 01:22:17 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: API request to endpoint /api/2.0/mlflow/logged-models failed with error code 404 != 200. Response body: '<!doctype html>\n",
      "<html lang=en>\n",
      "<title>404 Not Found</title>\n",
      "<h1>Not Found</h1>\n",
      "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n",
      "'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">128,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_max_pooling1d            ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ       \u001b[38;5;34m128,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)         ‚îÇ         \u001b[38;5;34m6,176\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)         ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_max_pooling1d            ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ           \u001b[38;5;34m528\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ           \u001b[38;5;34m170\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">404,624</span> (1.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m404,624\u001b[0m (1.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,874</span> (526.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m134,874\u001b[0m (526.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,750</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m269,750\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4024 - loss: 3.4085 \n",
      "Test Accuracy: 0.4024\n",
      "Test Loss: 3.4085\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 93ms/stepWARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000028E1C6F39C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.4025\n",
      "accuracy —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.4024\n",
      "‚úÖ CNN –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run second_experiment_neural_network at: http://127.0.0.1:8080/#/experiments/0/runs/7d097ddcda574eb39cb841127b8e31f3\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = load_config(\"neural_network_second\")\n",
    "\n",
    "with mlflow.start_run(run_name='second_experiment_neural_network'):\n",
    "    \n",
    "    mlflow.set_tag('CNN', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "    try:\n",
    "        dataset_path = f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\"\n",
    "        art = client.download_artifacts(dataset_run_id, dataset_path)\n",
    "        df = pd.read_csv(art)\n",
    "        print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "        alternative_paths = [\n",
    "            cfg.data.dataset_file,\n",
    "            f\"artifacts/{cfg.data.dataset_path}/{cfg.data.dataset_file}\",\n",
    "            \"Second_version.csv\"\n",
    "        ]\n",
    "        \n",
    "        for path in alternative_paths:\n",
    "            try:\n",
    "                art = client.download_artifacts(dataset_run_id, path)\n",
    "                df = pd.read_csv(art)\n",
    "                print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    tokenizer_config = {k: v for k, v in cfg.tokenizer.items() if v is not None}\n",
    "    tokenizer = Tokenizer(**tokenizer_config)\n",
    "    \n",
    "    tokenizer.fit_on_texts(df['span'])\n",
    "\n",
    "    df_train, df_temp = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state\n",
    "    )\n",
    "    \n",
    "    df_test, df_val = train_test_split(\n",
    "        df_temp, \n",
    "        test_size=cfg.training.val_size, \n",
    "        random_state=cfg.training.random_state\n",
    "    )\n",
    "\n",
    "    X_train_vec = tokenizer.texts_to_sequences(df_train['span'])\n",
    "    X_test_vec = tokenizer.texts_to_sequences(df_test['span'])\n",
    "    X_val_vec = tokenizer.texts_to_sequences(df_val['span'])\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_vec, maxlen=cfg.training.max_sequence_length, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_vec, maxlen=cfg.training.max_sequence_length, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_vec, maxlen=cfg.training.max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    y_train = encoder.fit_transform(df_train['label'])\n",
    "    y_test = encoder.transform(df_test['label'])\n",
    "    y_val = encoder.transform(df_val['label'])\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò CNN\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(\n",
    "        input_dim=cfg.tokenizer.num_words, \n",
    "        output_dim=cfg.model.embedding_dim, \n",
    "        input_length=cfg.training.max_sequence_length\n",
    "    ))\n",
    "\n",
    "    model.add(Conv1D(\n",
    "        filters=cfg.model.conv_filters,\n",
    "        kernel_size=cfg.model.conv_kernel_size,\n",
    "        activation=cfg.model.conv_activation\n",
    "    ))\n",
    "\n",
    "    model.add(Dropout(cfg.model.dropout_rate_1))\n",
    "\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(Dense(cfg.model.dense_units_1, activation=cfg.model.dense_activation_1))\n",
    "\n",
    "    model.add(Dropout(cfg.model.dropout_rate_2))\n",
    "\n",
    "    model.add(Dense(cfg.model.dense_units_2, activation=cfg.model.dense_activation_2))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, \n",
    "        y_train, \n",
    "        batch_size=cfg.training.batch_size, \n",
    "        epochs=cfg.training.epochs, \n",
    "        validation_data=(X_val_pad, y_val)\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    pred = model.predict(X_test_pad)\n",
    "    pred_class = np.argmax(pred, axis=1)\n",
    "\n",
    "    f1 = f1_score(pred_class, y_test, average='weighted')\n",
    "    accuracy = accuracy_score(pred_class, y_test)\n",
    "\n",
    "    print(f'f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "    print(f'accuracy —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {accuracy:.4f}')\n",
    "\n",
    "    mlflow.log_metric('f1_score', f1)\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "    mlflow.log_metric('test_accuracy', test_accuracy)\n",
    "    mlflow.log_metric('test_loss', test_loss)\n",
    "\n",
    "    model.save(cfg.model.artifacts.model)\n",
    "\n",
    "    joblib.dump(tokenizer, cfg.model.artifacts.tokenizer)\n",
    "    joblib.dump(encoder, cfg.model.artifacts.encoder)\n",
    "\n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.tokenizer, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'num_words': cfg.tokenizer.num_words,\n",
    "        'embedding_dim': cfg.model.embedding_dim,\n",
    "        'conv_filters': cfg.model.conv_filters,\n",
    "        'conv_kernel_size': cfg.model.conv_kernel_size,\n",
    "        'dropout_rate_1': cfg.model.dropout_rate_1,\n",
    "        'dropout_rate_2': cfg.model.dropout_rate_2,\n",
    "        'dense_units_1': cfg.model.dense_units_1,\n",
    "        'batch_size': cfg.training.batch_size,\n",
    "        'epochs': cfg.training.epochs,\n",
    "        'max_sequence_length': cfg.training.max_sequence_length\n",
    "    })\n",
    "\n",
    "    print(\"‚úÖ CNN –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d25bc",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011461e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 7d097ddcda574eb39cb841127b8e31f3\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 100\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000028E1C480FE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–í —Ü–µ–ª–æ–º, —á–∏–ø—Å—ã –ê—à–∞–Ω –ö—Ä–∞—Å–Ω–∞—è –ø—Ç–∏—Ü–∞ –ë–∞—Ä–±–µ–∫—é –≤–ø–æ–ª–Ω–µ —Å—ä–µ–¥–æ–±–Ω—ã–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_NEUTRAL\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:\n",
      "  O: 0.0063\n",
      "  –í–ö–£–°_NEGATIVE: 0.0010\n",
      "  –í–ö–£–°_NEUTRAL: 0.7726\n",
      "  –í–ö–£–°_POSITIVE: 0.0001\n",
      "  –ü–ê–ß–ö–ê_NEGATIVE: 0.0600\n",
      "  –ü–ê–ß–ö–ê_NEUTRAL: 0.1581\n",
      "  –ü–ê–ß–ö–ê_POSITIVE: 0.0019\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEGATIVE: 0.0000\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEUTRAL: 0.0001\n",
      "  –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 0.0000\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_neural_network_second\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "model_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.model_file}\"\n",
    "tokenizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.tokenizer}\"\n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\")\n",
    "try:\n",
    "    model_file = client.download_artifacts(latest_run_model_id, model_path)\n",
    "    tokenizer_file = client.download_artifacts(latest_run_model_id, tokenizer_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "model_keras = tf.keras.models.load_model(model_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "tokenizer = joblib.load(tokenizer_file)\n",
    "\n",
    "print(\"–í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "\n",
    "\n",
    "tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "\n",
    "max_len = model_keras.input_shape[1]\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_len}\")\n",
    "\n",
    "\n",
    "padded_text = pad_sequences(tokenized_text, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "pred = model_keras.predict(padded_text)\n",
    "\n",
    "\n",
    "predicted_class_ind = np.argmax(pred, axis=1)\n",
    "predicted_class = encoder.inverse_transform(predicted_class_ind)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {predicted_class[0]}')\n",
    "print(f'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:')\n",
    "for i, prob in enumerate(pred[0]):\n",
    "    class_name = encoder.inverse_transform([i])[0]\n",
    "    print(f'  {class_name}: {prob:.4f}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c696649",
   "metadata": {},
   "source": [
    "### –¢—Ä–µ—Ç–∏–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613e556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: 327f7f34ba914fce83ca6c0e787e8f15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: datasets/third_experiment_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                span          label\n",
       "0                          –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π  –í–ö–£–°_POSITIVE\n",
       "1  –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...              O\n",
       "2  —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...              O\n",
       "3             —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞              O\n",
       "4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ  –í–ö–£–°_NEGATIVE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 01:22:32 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential_2' has no defined input shape yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 85\n",
      "üìä –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\n",
      "   Train: (3216, 85), (3216,)\n",
      "   Test: (402, 85), (402,)\n",
      "   Val: (402, 85), (402,)\n",
      "Epoch 1/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 158ms/step - accuracy: 0.4667 - loss: 1.5992 - val_accuracy: 0.4577 - val_loss: 2.0455\n",
      "Epoch 2/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 151ms/step - accuracy: 0.7848 - loss: 0.6825 - val_accuracy: 0.5746 - val_loss: 1.6586\n",
      "Epoch 3/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 146ms/step - accuracy: 0.8859 - loss: 0.4023 - val_accuracy: 0.6791 - val_loss: 1.1612\n",
      "Epoch 4/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 159ms/step - accuracy: 0.9288 - loss: 0.2556 - val_accuracy: 0.6318 - val_loss: 1.3787\n",
      "Epoch 5/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 181ms/step - accuracy: 0.9443 - loss: 0.1877 - val_accuracy: 0.6667 - val_loss: 1.3948\n",
      "Epoch 6/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 167ms/step - accuracy: 0.9608 - loss: 0.1290 - val_accuracy: 0.6517 - val_loss: 1.8094\n",
      "Epoch 7/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 155ms/step - accuracy: 0.9733 - loss: 0.0943 - val_accuracy: 0.6418 - val_loss: 2.0034\n",
      "Epoch 8/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 157ms/step - accuracy: 0.9717 - loss: 0.0935 - val_accuracy: 0.6841 - val_loss: 1.9123\n",
      "Epoch 9/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 163ms/step - accuracy: 0.9832 - loss: 0.0659 - val_accuracy: 0.6866 - val_loss: 1.8061\n",
      "Epoch 10/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 159ms/step - accuracy: 0.9866 - loss: 0.0508 - val_accuracy: 0.6542 - val_loss: 2.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 01:25:33 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: API request to endpoint /api/2.0/mlflow/logged-models failed with error code 404 != 200. Response body: '<!doctype html>\n",
      "<html lang=en>\n",
      "<title>404 Not Found</title>\n",
      "<h1>Not Found</h1>\n",
      "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n",
      "'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">493,056</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ batch_normalization             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m85\u001b[0m, \u001b[38;5;34m512\u001b[0m)        ‚îÇ     \u001b[38;5;34m1,024,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            ‚îÇ       \u001b[38;5;34m493,056\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ        \u001b[38;5;34m16,448\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ batch_normalization             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ           \u001b[38;5;34m256\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ           \u001b[38;5;34m650\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,602,976</span> (17.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,602,976\u001b[0m (17.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,534,282</span> (5.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,534,282\u001b[0m (5.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,068,566</span> (11.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,068,566\u001b[0m (11.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.7189 - loss: 1.4945\n",
      "üìä Test Loss: 1.4945\n",
      "üìä Test Accuracy: 0.7189\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step\n",
      "üéØ F1-score: 0.7280\n",
      "üéØ Accuracy: 0.7189\n",
      "‚úÖ Bidirectional GRU –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run third_experiment_neural_network at: http://127.0.0.1:8080/#/experiments/0/runs/8d1d38be345646f5bccb64306e83cff6\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"neural_network_third\")\n",
    "\n",
    "with mlflow.start_run(run_name='third_experiment_neural_network'):\n",
    "    \n",
    "    mlflow.set_tag('Bidirectional_GRU', cfg.model.version)\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "    try:\n",
    "        dataset_path = f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\"\n",
    "        art = client.download_artifacts(dataset_run_id, dataset_path)\n",
    "        df = pd.read_csv(art)\n",
    "        print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "        alternative_paths = [\n",
    "            cfg.data.dataset_file,\n",
    "            f\"artifacts/{cfg.data.dataset_path}/{cfg.data.dataset_file}\",\n",
    "            \"third_experiment_dataset.csv\"\n",
    "        ]\n",
    "        \n",
    "        for path in alternative_paths:\n",
    "            try:\n",
    "                art = client.download_artifacts(dataset_run_id, path)\n",
    "                df = pd.read_csv(art)\n",
    "                print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df.head())\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    tokenizer_config = {k: v for k, v in cfg.tokenizer.items() if v is not None}\n",
    "    tokenizer = Tokenizer(**tokenizer_config)\n",
    "    \n",
    "    tokenizer.fit_on_texts(df['span'])\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(df['span'])\n",
    "\n",
    "    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_len}\")\n",
    "\n",
    "    X = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï –¢–ê–†–ì–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(df['label'])\n",
    "    num_classes = len(encoder.classes_)\n",
    "    \n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/TEST/VAL\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, \n",
    "        train_size=1-cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size=cfg.training.val_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    print(f\"üìä –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "    print(f\"   Train: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"   Test: {X_test.shape}, {y_test.shape}\")\n",
    "    print(f\"   Val: {X_val.shape}, {y_val.shape}\")\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–û–ó–î–ê–ù–ò–ï –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(\n",
    "        input_dim=cfg.tokenizer.num_words, \n",
    "        output_dim=cfg.model.embedding_dim\n",
    "    ))\n",
    "\n",
    "    model.add(Bidirectional(GRU(cfg.model.gru_units)))\n",
    "\n",
    "    model.add(Dense(cfg.model.dense_units, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=cfg.training.epochs, \n",
    "        batch_size=cfg.training.batch_size, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Loss: {loss:.4f}')\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "    acc_score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "    print(f'F1-score: {f1:.4f}')\n",
    "    print(f'Accuracy: {acc_score:.4f}')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–ï–¢–†–ò–ö\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.log_metric('f1_score', f1)\n",
    "    mlflow.log_metric('accuracy', acc_score)\n",
    "    mlflow.log_metric('test_loss', loss)\n",
    "    mlflow.log_metric('test_accuracy', accuracy)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–û–•–†–ê–ù–ï–ù–ò–ï –ê–†–¢–ï–§–ê–ö–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    model.save(cfg.model.artifacts.model)\n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "\n",
    "    with open(cfg.model.artifacts.tokenizer, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    mlflow.log_artifact(cfg.model.artifacts.tokenizer, 'preprocessing')\n",
    "\n",
    "    with open(cfg.model.artifacts.encoder, 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'preprocessing')\n",
    "\n",
    "    preprocessing_info = {\n",
    "        'vocab_size': len(tokenizer.word_index),\n",
    "        'max_sequence_length': max_len,\n",
    "        'num_classes': num_classes,\n",
    "        'classes': list(encoder.classes_)\n",
    "    }\n",
    "    \n",
    "    with open(cfg.model.artifacts.preprocessing_info, 'wb') as f:\n",
    "        pickle.dump(preprocessing_info, f)\n",
    "    mlflow.log_artifact(cfg.model.artifacts.preprocessing_info, 'preprocessing')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ü–ê–†–ê–ú–ï–¢–†–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"Bidirectional_GRU\",\n",
    "        \"embedding_dim\": cfg.model.embedding_dim,\n",
    "        \"gru_units\": cfg.model.gru_units,\n",
    "        \"dense_units\": cfg.model.dense_units,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"vocab_size\": len(tokenizer.word_index),\n",
    "        \"max_sequence_length\": max_len,\n",
    "        \"batch_size\": cfg.training.batch_size,\n",
    "        \"epochs\": cfg.training.epochs\n",
    "    })\n",
    "\n",
    "    print(\"Bidirectional GRU –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505cd4d",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea0dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 8d1d38be345646f5bccb64306e83cff6\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.73it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.71it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\n",
      "üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ: {'vocab_size': 5376, 'max_sequence_length': 85, 'num_classes': 10, 'classes': ['O', '–í–ö–£–°_NEGATIVE', '–í–ö–£–°_NEUTRAL', '–í–ö–£–°_POSITIVE', '–ü–ê–ß–ö–ê_NEGATIVE', '–ü–ê–ß–ö–ê_NEUTRAL', '–ü–ê–ß–ö–ê_POSITIVE', '–¢–ï–ö–°–¢–£–†–ê_NEGATIVE', '–¢–ï–ö–°–¢–£–†–ê_NEUTRAL', '–¢–ï–ö–°–¢–£–†–ê_POSITIVE']}\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: [[43, 46, 1532, 5, 2, 1]]\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 85\n",
      "–¢–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø–∞–¥–¥–∏–Ω–≥–∞: [[  43   46 1532    5    2    1    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step\n",
      "–°—ã—Ä—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: [[0.08346293 0.04367334 0.00365102 0.09069515 0.05031741 0.00548066\n",
      "  0.6367563  0.00674535 0.06950271 0.00971512]]\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–æ–∂–Ω–æ –±—ã–ª–æ —Å–¥–µ–ª–∞—Ç—å —á–∏–ø—Å—ã –∏ –ø–æ–≤–∫—É—Å–Ω–µ–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –ü–ê–ß–ö–ê_POSITIVE\n",
      "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: 0.6368\n",
      "–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:\n",
      "  O: 0.0835\n",
      "  –í–ö–£–°_NEGATIVE: 0.0437\n",
      "  –í–ö–£–°_NEUTRAL: 0.0037\n",
      "  –í–ö–£–°_POSITIVE: 0.0907\n",
      "  –ü–ê–ß–ö–ê_NEGATIVE: 0.0503\n",
      "  –ü–ê–ß–ö–ê_NEUTRAL: 0.0055\n",
      "  –ü–ê–ß–ö–ê_POSITIVE: 0.6368\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEGATIVE: 0.0067\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEUTRAL: 0.0695\n",
      "  –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 0.0097\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_neural_network_third\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'attributes.run_name = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "model_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.model_file}\"\n",
    "tokenizer_path = f\"preprocessing/{cfg_inference.model.tokenizer}\"\n",
    "encoder_path = f\"preprocessing/{cfg_inference.model.encoder}\"\n",
    "info_path = f\"preprocessing/{cfg_inference.model.preprocessing_info}\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\")\n",
    "try:\n",
    "    model_file = client.download_artifacts(latest_run_model_id, model_path)\n",
    "    tokenizer_file = client.download_artifacts(latest_run_model_id, tokenizer_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    info_file = client.download_artifacts(latest_run_model_id, info_path)\n",
    "    print(\"–í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "model_keras = tf.keras.models.load_model(model_file)\n",
    "\n",
    "with open(tokenizer_file, 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "with open(encoder_file, 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "\n",
    "with open(info_file, 'rb') as f:\n",
    "    preprocessing_info = pickle.load(f)\n",
    "\n",
    "print(\"–í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\")\n",
    "print(f\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ: {preprocessing_info}\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "\n",
    "tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "print(f\"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {tokenized_text}\")\n",
    "\n",
    "max_len = preprocessing_info['max_sequence_length']\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_len}\")\n",
    "\n",
    "padded_text = pad_sequences(tokenized_text, maxlen=max_len, padding='post', truncating='post')\n",
    "print(f\"–¢–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø–∞–¥–¥–∏–Ω–≥–∞: {padded_text}\")\n",
    "\n",
    "pred = model_keras.predict(padded_text)\n",
    "print(f\"–°—ã—Ä—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {pred}\")\n",
    "\n",
    "predicted_class_ind = np.argmax(pred, axis=1)\n",
    "predicted_class = encoder.inverse_transform(predicted_class_ind)\n",
    "confidence = np.max(pred, axis=1)[0]\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {predicted_class[0]}')\n",
    "print(f'–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.4f}')\n",
    "print(f'–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:')\n",
    "for i, prob in enumerate(pred[0]):\n",
    "    class_name = encoder.inverse_transform([i])[0]\n",
    "    print(f'  {class_name}: {prob:.4f}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e2918",
   "metadata": {},
   "source": [
    "## –¢—Ä–µ—Ç–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74382c43",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78383437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: 8690112afeac4c7faf0867c5d30d2dd9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: datasets/first_experiment_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span             labels\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2006/2006 [00:01<00:00, 1886.15 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [00:00<00:00, 2243.91 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [00:00<00:00, 2711.61 examples/s]\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1260 08:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.251500</td>\n",
       "      <td>2.160471</td>\n",
       "      <td>0.069781</td>\n",
       "      <td>0.203187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.012900</td>\n",
       "      <td>1.942594</td>\n",
       "      <td>0.267171</td>\n",
       "      <td>0.346614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.767167</td>\n",
       "      <td>0.336707</td>\n",
       "      <td>0.398406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.694400</td>\n",
       "      <td>1.668675</td>\n",
       "      <td>0.373185</td>\n",
       "      <td>0.438247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.613000</td>\n",
       "      <td>1.593321</td>\n",
       "      <td>0.389435</td>\n",
       "      <td>0.454183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.563400</td>\n",
       "      <td>1.545394</td>\n",
       "      <td>0.426005</td>\n",
       "      <td>0.486056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.432900</td>\n",
       "      <td>1.516056</td>\n",
       "      <td>0.445592</td>\n",
       "      <td>0.501992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.399200</td>\n",
       "      <td>1.491220</td>\n",
       "      <td>0.445530</td>\n",
       "      <td>0.501992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.422100</td>\n",
       "      <td>1.481730</td>\n",
       "      <td>0.450813</td>\n",
       "      <td>0.505976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.402100</td>\n",
       "      <td>1.477343</td>\n",
       "      <td>0.453505</td>\n",
       "      <td>0.509960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {'eval_loss': 1.4958806037902832, 'eval_f1-score': 0.44198462104907194, 'eval_accuracy': 0.5059760956175299, 'eval_runtime': 0.416, 'eval_samples_per_second': 603.431, 'eval_steps_per_second': 38.466, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final metrics - F1: 0.4420, Accuracy: 0.5060, Loss: 3.4085\n",
      "‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω!\n",
      "üèÉ View run transformers_experiment_1 at: http://127.0.0.1:8080/#/experiments/0/runs/82086ed0872f47c58376be0d105fed8f\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"transformers_first\")\n",
    "\n",
    "with mlflow.start_run(run_name='transformers_experiment_1'):\n",
    "    \n",
    "    mlflow.set_tag('Transformers', cfg.model.version)\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        dataset_path = f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\"\n",
    "        art_loc = client.download_artifacts(dataset_run_id, dataset_path)\n",
    "        df = pd.read_csv(art_loc)\n",
    "        print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "\n",
    "        alternative_paths = [\n",
    "            cfg.data.dataset_file,\n",
    "            f\"artifacts/{cfg.data.dataset_path}/{cfg.data.dataset_file}\",\n",
    "            \"First_version.csv\"\n",
    "        ]\n",
    "        \n",
    "        for path in alternative_paths:\n",
    "            try:\n",
    "                art_loc = client.download_artifacts(dataset_run_id, path)\n",
    "                df = pd.read_csv(art_loc)\n",
    "                print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "\n",
    "    df = df[[cfg.data.text_column, cfg.data.label_column]]\n",
    "    df = df.rename(columns={cfg.data.label_column: \"labels\"})\n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê/–¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "    df_train, df_temp = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['labels']\n",
    "    )\n",
    "    \n",
    "    df_test, df_val = train_test_split(\n",
    "        df_temp, \n",
    "        test_size=cfg.training.val_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df_temp['labels']\n",
    "    )\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(df_train['labels'])\n",
    "    y_test = encoder.transform(df_test['labels'])\n",
    "    y_val = encoder.transform(df_val['labels'])\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "    mlflow.transformers.autolog()\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model.model_name)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model.model_name, \n",
    "        num_labels=len(encoder.classes_), \n",
    "        id2label={i: label for i, label in enumerate(encoder.classes_)},\n",
    "        label2id={label: i for i, label in enumerate(encoder.classes_)}\n",
    "    )\n",
    "    \n",
    "\n",
    "    dataset_train = Dataset.from_pandas(df_train.assign(labels=y_train))    \n",
    "    dataset_test = Dataset.from_pandas(df_test.assign(labels=y_test))    \n",
    "    dataset_val = Dataset.from_pandas(df_val.assign(labels=y_val))\n",
    "\n",
    "\n",
    "    def tokenize_dataset(row):\n",
    "        tokenizer_config = {k: v for k, v in cfg.tokenizer.items() if v is not None}\n",
    "        return tokenizer(row[cfg.data.text_column], **tokenizer_config)\n",
    "\n",
    "    dataset_tokenized_train = dataset_train.map(tokenize_dataset, batched=False)\n",
    "    dataset_tokenized_test = dataset_test.map(tokenize_dataset, batched=False)\n",
    "    dataset_tokenized_val = dataset_val.map(tokenize_dataset, batched=False)\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=cfg.training.output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        logging_dir='./logs/',\n",
    "        num_train_epochs=cfg.training.num_train_epochs,\n",
    "        learning_rate=cfg.training.learning_rate,\n",
    "        per_device_train_batch_size=cfg.training.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=cfg.training.per_device_eval_batch_size,\n",
    "        eval_strategy=cfg.training.eval_strategy,\n",
    "        save_strategy=cfg.training.save_strategy,\n",
    "        warmup_ratio=cfg.training.warmup_ratio,\n",
    "        lr_scheduler_type=cfg.training.lr_scheduler_type,\n",
    "        metric_for_best_model=cfg.training.metric_for_best_model,\n",
    "        weight_decay=cfg.training.weight_decay,\n",
    "        load_best_model_at_end=cfg.training.load_best_model_at_end,\n",
    "        save_total_limit=cfg.training.save_total_limit,\n",
    "        max_grad_norm=cfg.training.max_grad_norm,\n",
    "        logging_steps=cfg.training.logging_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return {\n",
    "            'f1-score': f1_score(labels, predictions, average='weighted'),\n",
    "            'accuracy': accuracy_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=dataset_tokenized_train,\n",
    "        eval_dataset=dataset_tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=cfg.training.early_stopping_patience)]\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    final_metrics = trainer.evaluate(dataset_tokenized_test)\n",
    "    print(f\"–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {final_metrics}\")\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–û–•–†–ê–ù–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    final_predictions = trainer.predict(dataset_tokenized_test)\n",
    "    predictions = np.argmax(final_predictions.predictions, axis=1)\n",
    "    labels = final_predictions.label_ids\n",
    "\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    mlflow.log_metric('f1_score', f1)\n",
    "    mlflow.log_metric('accuracy', accuracy) \n",
    "    \n",
    "    for key, value in final_predictions.metrics.items():\n",
    "        if key.startswith('eval_'):\n",
    "            clean_key = key.replace('eval_', '')\n",
    "            mlflow.log_metric(clean_key, value)\n",
    "\n",
    "    print(f\"Final metrics - F1: {f1:.4f}, Accuracy: {accuracy:.4f}, Loss: {test_loss:.4f}\")\n",
    "\n",
    "    model_dir = cfg.artifacts.model_dir\n",
    "    tokenizer_dir = cfg.artifacts.tokenizer_dir\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "\n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "    mlflow.log_artifacts(model_dir, \"model\")\n",
    "    mlflow.log_artifacts(tokenizer_dir, \"tokenizer\")\n",
    "\n",
    "    with open('label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    mlflow.log_artifact('label_encoder.pkl')\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'model_name': cfg.model.model_name,\n",
    "        'num_labels': len(encoder.classes_),\n",
    "        'num_train_epochs': cfg.training.num_train_epochs,\n",
    "        'learning_rate': cfg.training.learning_rate,\n",
    "        'batch_size': cfg.training.per_device_train_batch_size,\n",
    "        'early_stopping_patience': cfg.training.early_stopping_patience\n",
    "    })\n",
    "\n",
    "    print(\"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd1e63",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f288885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 82086ed0872f47c58376be0d105fed8f\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.22s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:07<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–í —Ü–µ–ª–æ–º, —á–∏–ø—Å—ã –ê—à–∞–Ω –ö—Ä–∞—Å–Ω–∞—è –ø—Ç–∏—Ü–∞ –ë–∞—Ä–±–µ–∫—é –≤–ø–æ–ª–Ω–µ —Å—ä–µ–¥–æ–±–Ω—ã–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_POSITIVE\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: 0.3161\n",
      "–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:\n",
      "  O: 0.1456\n",
      "  –í–ö–£–°_NEGATIVE: 0.1105\n",
      "  –í–ö–£–°_NEUTRAL: 0.1792\n",
      "  –í–ö–£–°_POSITIVE: 0.3161\n",
      "  –ü–ê–ß–ö–ê_NEGATIVE: 0.0258\n",
      "  –ü–ê–ß–ö–ê_NEUTRAL: 0.0288\n",
      "  –ü–ê–ß–ö–ê_POSITIVE: 0.0389\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEGATIVE: 0.0373\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEUTRAL: 0.0438\n",
      "  –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 0.0740\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_transformers_first\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\")\n",
    "try:\n",
    "\n",
    "    model_dir = client.download_artifacts(latest_run_model_id, cfg_inference.model.artifacts_path)\n",
    "    tokenizer_dir = client.download_artifacts(latest_run_model_id, cfg_inference.model.tokenizer_path)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "print(\"–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "\n",
    "\n",
    "tokenizer_config = {k: v for k, v in cfg_inference.tokenizer.items() if v is not None}\n",
    "inputs = tokenizer(text, **tokenizer_config)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "predicted_class_idx = predictions.argmax().item()\n",
    "predicted_prob = predictions.max().item()\n",
    "\n",
    "id2label = model.config.id2label\n",
    "predicted_label = id2label[predicted_class_idx]\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {predicted_label}')\n",
    "print(f'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {predicted_prob:.4f}')\n",
    "print(f'–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:')\n",
    "for i, prob in enumerate(predictions[0]):\n",
    "    label = id2label[i]\n",
    "    print(f'  {label}: {prob:.4f}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736547f",
   "metadata": {},
   "source": [
    "### –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150746b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: ba68188717234d8ea8f01745d4816ea8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: datasets/second_experiment_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                span          label\n",
       "0                          –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π  –í–ö–£–°_POSITIVE\n",
       "1  —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...              O\n",
       "2  —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...              O\n",
       "3             —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞              O\n",
       "4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ  –í–ö–£–°_NEGATIVE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2006/2006 [00:00<00:00, 3138.29 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [00:00<00:00, 4944.63 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [00:00<00:00, 6745.76 examples/s]\n",
      "C:\\Users\\Smart\\AppData\\Local\\Temp\\ipykernel_160\\4146632116.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2510' max='2510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2510/2510 47:32, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.076000</td>\n",
       "      <td>2.079241</td>\n",
       "      <td>0.366077</td>\n",
       "      <td>0.270916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.914100</td>\n",
       "      <td>1.852924</td>\n",
       "      <td>0.464990</td>\n",
       "      <td>0.394422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.721000</td>\n",
       "      <td>1.722221</td>\n",
       "      <td>0.488090</td>\n",
       "      <td>0.422311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.565800</td>\n",
       "      <td>1.661844</td>\n",
       "      <td>0.478570</td>\n",
       "      <td>0.418327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.465800</td>\n",
       "      <td>1.595550</td>\n",
       "      <td>0.531171</td>\n",
       "      <td>0.462151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.469300</td>\n",
       "      <td>1.566464</td>\n",
       "      <td>0.516893</td>\n",
       "      <td>0.454183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.430800</td>\n",
       "      <td>1.547074</td>\n",
       "      <td>0.504289</td>\n",
       "      <td>0.446215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.326700</td>\n",
       "      <td>1.541347</td>\n",
       "      <td>0.499597</td>\n",
       "      <td>0.442231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.311400</td>\n",
       "      <td>1.532064</td>\n",
       "      <td>0.498807</td>\n",
       "      <td>0.442231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.347100</td>\n",
       "      <td>1.527224</td>\n",
       "      <td>0.498972</td>\n",
       "      <td>0.446215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\n",
      "   accuracy: 0.4422\n",
      "   f1_score: 0.3732\n",
      "‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω!\n",
      "üèÉ View run transformers_experiment_2 at: http://127.0.0.1:8080/#/experiments/0/runs/734f31b8cd0c48ada580eb519ae9b457\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"transformers_second\")\n",
    "\n",
    "with mlflow.start_run(run_name='transformers_experiment_2'):\n",
    "    \n",
    "    mlflow.set_tag('Transformers', cfg.model.version)\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "    try:\n",
    "        dataset_path = f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\"\n",
    "        art_loc = client.download_artifacts(dataset_run_id, dataset_path)\n",
    "        df = pd.read_csv(art_loc)\n",
    "        print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "\n",
    "        alternative_paths = [\n",
    "            cfg.data.dataset_file,\n",
    "            f\"artifacts/{cfg.data.dataset_path}/{cfg.data.dataset_file}\",\n",
    "            \"second_experiment_dataset.csv\"\n",
    "        ]\n",
    "        \n",
    "        for path in alternative_paths:\n",
    "            try:\n",
    "                art_loc = client.download_artifacts(dataset_run_id, path)\n",
    "                df = pd.read_csv(art_loc)\n",
    "                print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df.head())\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    df['label_encoded'] = encoder.fit_transform(df[cfg.data.label_column])\n",
    "    df = df.rename(columns={'label_encoded': 'labels'})\n",
    "    df = df[[cfg.data.text_column, 'labels']]\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model.model_name)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model.model_name,\n",
    "        num_labels=len(encoder.classes_),\n",
    "        id2label={i: label for i, label in enumerate(encoder.classes_)},\n",
    "        label2id={label: i for i, label in enumerate(encoder.classes_)}\n",
    "    )\n",
    "    \n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    df_train, df_temp = train_test_split(\n",
    "        df, \n",
    "        train_size=1-cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['labels']\n",
    "    )\n",
    "    \n",
    "    df_test, df_val = train_test_split(\n",
    "        df_temp, \n",
    "        test_size=cfg.training.val_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df_temp['labels']\n",
    "    )\n",
    "\n",
    "    dataset_train = Dataset.from_pandas(df_train)\n",
    "    dataset_test = Dataset.from_pandas(df_test)\n",
    "    dataset_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    def tokenize_dataset(batch):\n",
    "        tokenizer_config = {k: v for k, v in cfg.tokenizer.items() if v is not None}\n",
    "        return tokenizer(\n",
    "            batch[cfg.data.text_column],\n",
    "            **tokenizer_config\n",
    "        )\n",
    "\n",
    "    tokenized_train = dataset_train.map(tokenize_dataset, batched=True)\n",
    "    tokenized_test = dataset_test.map(tokenize_dataset, batched=True)\n",
    "    tokenized_val = dataset_val.map(tokenize_dataset, batched=True)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         DATA COLLATOR\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        max_length=cfg.tokenizer.max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         TRAINING ARGUMENTS\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=cfg.training.output_dir,\n",
    "        num_train_epochs=cfg.training.num_train_epochs,\n",
    "        learning_rate=cfg.training.learning_rate,\n",
    "        per_device_train_batch_size=cfg.training.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=cfg.training.per_device_eval_batch_size,\n",
    "        eval_strategy=cfg.training.eval_strategy,\n",
    "        save_strategy=cfg.training.save_strategy,\n",
    "        logging_dir=cfg.training.logging_dir,\n",
    "        logging_steps=cfg.training.logging_steps,\n",
    "        load_best_model_at_end=cfg.training.load_best_model_at_end,\n",
    "        metric_for_best_model=cfg.training.metric_for_best_model\n",
    "    )\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ú–ï–¢–†–ò–ö–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "        return {\n",
    "            'f1_score': f1_score(y_pred, labels, average='weighted'),\n",
    "            'accuracy': accuracy_score(y_pred, labels)\n",
    "        }\n",
    "    \n",
    "    # =====================================================================================================================================\n",
    "    #                                         TRAINER –ò –û–ë–£–ß–ï–ù–ò–ï\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–¶–ï–ù–ö–ê –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–ï–¢–†–ò–ö\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    test_predictions = trainer.predict(tokenized_test)\n",
    "    test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "    test_labels = test_predictions.label_ids\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, test_preds)\n",
    "    f1_score_value = f1_score(test_labels, test_preds, average='weighted')\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"f1_score\", f1_score_value)\n",
    "\n",
    "    print(f\"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\")\n",
    "    print(f\"   accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   f1_score: {f1_score_value:.4f}\")\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–û–•–†–ê–ù–ï–ù–ò–ï –ê–†–¢–ï–§–ê–ö–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    model_dir = cfg.artifacts.model_dir\n",
    "    tokenizer_dir = cfg.artifacts.tokenizer_dir\n",
    "    \n",
    "    import shutil\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "    if os.path.exists(tokenizer_dir):\n",
    "        shutil.rmtree(tokenizer_dir)\n",
    "\n",
    "    trainer.save_model(model_dir)\n",
    "    tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "    with open('label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "\n",
    "    mlflow.log_artifacts(model_dir, \"model\")\n",
    "    mlflow.log_artifacts(tokenizer_dir, \"tokenizer\")\n",
    "    mlflow.log_artifact('label_encoder.pkl')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ü–ê–†–ê–ú–ï–¢–†–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'model_name': cfg.model.model_name,\n",
    "        'num_labels': len(encoder.classes_),\n",
    "        'num_train_epochs': cfg.training.num_train_epochs,\n",
    "        'learning_rate': cfg.training.learning_rate,\n",
    "        'batch_size': cfg.training.per_device_train_batch_size,\n",
    "        'test_size': cfg.training.test_size,\n",
    "        'val_size': cfg.training.val_size\n",
    "    })\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ß–ò–°–¢–ö–ê –í–†–ï–ú–ï–ù–ù–´–• –§–ê–ô–õ–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    shutil.rmtree(model_dir)\n",
    "    shutil.rmtree(tokenizer_dir)\n",
    "    os.remove('label_encoder.pkl')\n",
    "\n",
    "    print(\"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959aa2cd",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 734f31b8cd0c48ada580eb519ae9b457\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:06<00:00,  1.00it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.81it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "‚úÖ –ú–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –ü–ê–ß–ö–ê_NEUTRAL\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: 0.2678\n",
      "–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:\n",
      "  O: 0.0901\n",
      "  –í–ö–£–°_NEGATIVE: 0.0184\n",
      "  –í–ö–£–°_NEUTRAL: 0.0321\n",
      "  –í–ö–£–°_POSITIVE: 0.0212\n",
      "  –ü–ê–ß–ö–ê_NEGATIVE: 0.0978\n",
      "  –ü–ê–ß–ö–ê_NEUTRAL: 0.2678\n",
      "  –ü–ê–ß–ö–ê_POSITIVE: 0.1886\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEGATIVE: 0.0811\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEUTRAL: 0.1246\n",
      "  –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 0.0784\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_transformers_second\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'attributes.run_name = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä...\")\n",
    "try:\n",
    "    model_dir = client.download_artifacts(latest_run_model_id, 'model')\n",
    "    tokenizer_dir = client.download_artifacts(latest_run_model_id, 'tokenizer')\n",
    "    encoder_path = client.download_artifacts(latest_run_model_id, 'label_encoder.pkl')\n",
    "    print(\"–ú–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "with open(encoder_path, 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "\n",
    "print(\"–ú–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "\n",
    "tokenizer_config = {k: v for k, v in cfg_inference.tokenizer.items() if v is not None}\n",
    "inputs = tokenizer(text, **tokenizer_config)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "predicted_class_idx = predictions.argmax().item()\n",
    "predicted_prob = predictions.max().item()\n",
    "\n",
    "predicted_label = encoder.inverse_transform([predicted_class_idx])[0]\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {predicted_label}')\n",
    "print(f'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {predicted_prob:.4f}')\n",
    "print(f'–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:')\n",
    "for i, prob in enumerate(predictions[0]):\n",
    "    label = encoder.inverse_transform([i])[0]\n",
    "    print(f'  {label}: {prob:.4f}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7429c5",
   "metadata": {},
   "source": [
    "### –¢—Ä–µ—Ç–∏–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd76d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: 327f7f34ba914fce83ca6c0e787e8f15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ run: C:\\Users\\Smart\\AppData\\Local\\Temp\\tmpkmxff0ir\\third_experiment_dataset.csv\n",
      "üìä –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö: Train=3216, Val=402, Test=402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [00:00<00:00, 8500.23 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 402/402 [00:00<00:00, 7787.79 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 402/402 [00:00<00:00, 8308.13 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:\n",
      "Train columns: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Sample train: {'labels': 3, 'input_ids': [101, 110, 55795, 37312, 107, 54557, 71854, 14236, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3216' max='4020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3216/4020 2:29:03 < 37:17, 0.36 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.299100</td>\n",
       "      <td>1.166594</td>\n",
       "      <td>0.591704</td>\n",
       "      <td>0.629353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>0.634449</td>\n",
       "      <td>0.794025</td>\n",
       "      <td>0.796020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.540969</td>\n",
       "      <td>0.832404</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.585891</td>\n",
       "      <td>0.849085</td>\n",
       "      <td>0.853234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.643048</td>\n",
       "      <td>0.857232</td>\n",
       "      <td>0.858209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.622935</td>\n",
       "      <td>0.869776</td>\n",
       "      <td>0.870647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.679291</td>\n",
       "      <td>0.867847</td>\n",
       "      <td>0.868159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.656416</td>\n",
       "      <td>0.868577</td>\n",
       "      <td>0.870647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2863: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ:\n",
      "   F1-score: 0.8191\n",
      "   Accuracy: 0.8259\n",
      "‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω!\n",
      "üèÉ View run transformers_experiment_3 at: http://127.0.0.1:8080/#/experiments/0/runs/948c02959d5a46b798a981a964d40f83\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"transformers_third\")\n",
    "\n",
    "with mlflow.start_run(run_name=cfg.mlflow.run_name):\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "    mlflow.log_param(\"model_name\", cfg.model.name)\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    # ---------------------------\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_name], \n",
    "        filter_string=f\"tags.mlflow.runName = 'Third dataset'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º 'Third dataset'\")\n",
    "\n",
    "    dataset_run_id = dataset_runs[0].info.run_id\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "    dataset_path_in_run = \"datasets/third_experiment_dataset.csv\" \n",
    "    artifact_local_path = client.download_artifacts(dataset_run_id, dataset_path_in_run)\n",
    "\n",
    "    df = pd.read_csv(artifact_local_path)\n",
    "    print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ run: {artifact_local_path}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\n",
    "    # ---------------------------\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    df[\"labels\"] = encoder.fit_transform(df[\"label\"])\n",
    "    df[\"labels\"] = df[\"labels\"].astype(int)\n",
    "\n",
    "    mlflow.log_param(\"num_labels\", len(encoder.classes_))\n",
    "\n",
    "    df_train, df_temp = train_test_split(\n",
    "        df,\n",
    "        test_size=1 - cfg.data.train_size,\n",
    "        random_state=cfg.data.random_state,\n",
    "        shuffle=True,\n",
    "        stratify=df[\"labels\"]\n",
    "    )\n",
    "\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_temp,\n",
    "        test_size=cfg.data.val_size,\n",
    "        random_state=cfg.data.random_state,\n",
    "        shuffle=True,\n",
    "        stratify=df_temp[\"labels\"]\n",
    "    )\n",
    "\n",
    "    print(f\"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö: Train={len(df_train)}, Val={len(df_val)}, Test={len(df_test)}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–û–í (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)\n",
    "    # ---------------------------\n",
    "\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    dataset_train = Dataset.from_dict({\n",
    "        \"text\": df_train[\"span\"].tolist(),\n",
    "        \"labels\": df_train[\"labels\"].tolist()\n",
    "    })\n",
    "    \n",
    "    dataset_val = Dataset.from_dict({\n",
    "        \"text\": df_val[\"span\"].tolist(), \n",
    "        \"labels\": df_val[\"labels\"].tolist()\n",
    "    })\n",
    "    \n",
    "    dataset_test = Dataset.from_dict({\n",
    "        \"text\": df_test[\"span\"].tolist(),\n",
    "        \"labels\": df_test[\"labels\"].tolist()\n",
    "    })\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø\n",
    "    # ---------------------------\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=cfg.model.max_length\n",
    "        )\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_train = dataset_train.map(tokenize_function, batched=True)\n",
    "    tokenized_val = dataset_val.map(tokenize_function, batched=True)\n",
    "    tokenized_test = dataset_test.map(tokenize_function, batched=True)\n",
    "\n",
    "    tokenized_train = tokenized_train.remove_columns(['text'])\n",
    "    tokenized_val = tokenized_val.remove_columns(['text'])\n",
    "    tokenized_test = tokenized_test.remove_columns(['text'])\n",
    "\n",
    "    print(\"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:\")\n",
    "    print(f\"Train columns: {tokenized_train.column_names}\")\n",
    "    print(f\"Sample train: {tokenized_train[0]}\")\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        max_length=cfg.model.max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò\n",
    "    # ---------------------------\n",
    "\n",
    "    num_labels = len(encoder.classes_)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model.name,\n",
    "        num_labels=num_labels,\n",
    "        id2label={i: label for i, label in enumerate(encoder.classes_)},\n",
    "        label2id={label: i for i, label in enumerate(encoder.classes_)},\n",
    "        hidden_dropout_prob=cfg.model.dropout.hidden,\n",
    "        attention_probs_dropout_prob=cfg.model.dropout.attention,\n",
    "        classifier_dropout=cfg.model.dropout.classifier\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    #   TRAINING ARGUMENTS\n",
    "    # ---------------------------\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=cfg.training.output_dir,\n",
    "        num_train_epochs=cfg.training.num_train_epochs,\n",
    "        learning_rate=cfg.training.learning_rate,\n",
    "        per_device_train_batch_size=cfg.training.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=cfg.training.per_device_eval_batch_size,\n",
    "        warmup_ratio=cfg.training.warmup_ratio,\n",
    "        lr_scheduler_type=cfg.training.lr_scheduler_type,\n",
    "        eval_strategy=cfg.training.eval_strategy,\n",
    "        save_strategy=cfg.training.save_strategy,\n",
    "        weight_decay=cfg.training.weight_decay,\n",
    "        logging_steps=cfg.training.logging_steps,\n",
    "        save_total_limit=cfg.training.save_total_limit,\n",
    "        max_grad_norm=cfg.training.max_grad_norm,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        dataloader_pin_memory=False,\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=True\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –ú–ï–¢–†–ò–ö–ò\n",
    "    # ---------------------------\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "\n",
    "        return {\n",
    "            \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "            \"accuracy\": accuracy_score(labels, preds)\n",
    "        }\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –û–ë–£–ß–ï–ù–ò–ï\n",
    "    # ---------------------------\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=cfg.training.early_stopping_patience)]\n",
    "    )\n",
    "\n",
    "    print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –û–¶–ï–ù–ö–ê –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï\n",
    "    # ---------------------------\n",
    "    \n",
    "    print(\"–û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...\")\n",
    "    test_predictions = trainer.predict(tokenized_test)\n",
    "    test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "    test_labels = test_predictions.label_ids\n",
    "    \n",
    "    test_f1 = f1_score(test_labels, test_preds, average=\"weighted\")\n",
    "    test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "    \n",
    "    mlflow.log_metric(\"f1_score\", test_f1)\n",
    "    mlflow.log_metric(\"accuracy\", test_accuracy)\n",
    "    \n",
    "    print(f\"–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ:\")\n",
    "    print(f\"F1-score: {test_f1:.4f}\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –°–û–•–†–ê–ù–ï–ù–ò–ï\n",
    "    # ---------------------------\n",
    "\n",
    "    model_dir = \"best_transformer_model\"\n",
    "    tokenizer_dir = \"best_transformer_tokenizer\"\n",
    "\n",
    "    trainer.save_model(model_dir)\n",
    "    tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "    mlflow.log_artifacts(model_dir, artifact_path=\"model\")\n",
    "    mlflow.log_artifacts(tokenizer_dir, artifact_path=\"tokenizer\")\n",
    "\n",
    "    with open('label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    mlflow.log_artifact('label_encoder.pkl', artifact_path=\"preprocessing\")\n",
    "\n",
    "    import shutil\n",
    "    shutil.rmtree(model_dir)\n",
    "    shutil.rmtree(tokenizer_dir)\n",
    "    os.remove('label_encoder.pkl')\n",
    "\n",
    "    print(\"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2015c",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∏–Ω—Ñ–µ—Ä–µ–Ω—Å)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9c997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—â–µ–º run —Å –º–æ–¥–µ–ª—å—é...\n",
      "‚úÖ –ù–∞–π–¥–µ–Ω run: 948c02959d5a46b798a981a964d40f83\n",
      "–°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [02:09<00:00, 18.56s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:14<00:00,  3.70s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 34.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: C:\\Users\\Smart\\AppData\\Local\\Temp\\tmp1ut_wjcm\\model\n",
      "üìÅ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–∫–∞—á–∞–Ω: C:\\Users\\Smart\\AppData\\Local\\Temp\\tmpv5m85rm0\\tokenizer\n",
      "üìÅ –≠–Ω–∫–æ–¥–µ—Ä —Å–∫–∞—á–∞–Ω: C:\\Users\\Smart\\AppData\\Local\\Temp\\tmpstx4jzoy\\label_encoder.pkl\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'C:\\Users\\Smart\\AppData\\Local\\Temp\\tmpv5m85rm0\\tokenizer' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.  This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: ¬´–ú–æ–∂–Ω–æ –±—ã–ª–æ —Å–¥–µ–ª–∞—Ç—å —á–∏–ø—Å—ã –∏ –ø–æ–≤–∫—É—Å–Ω–µ–µ¬ª\n",
      "\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: ¬´–ú–æ–∂–Ω–æ –±—ã–ª–æ —Å–¥–µ–ª–∞—Ç—å —á–∏–ø—Å—ã –∏ –ø–æ–≤–∫—É—Å–Ω–µ–µ¬ª\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_NEUTRAL\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: 0.7908\n",
      "\n",
      "–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:\n",
      "  O: 0.0010\n",
      "  –í–ö–£–°_NEGATIVE: 0.0021\n",
      "  –í–ö–£–°_NEUTRAL: 0.7908\n",
      "  –í–ö–£–°_POSITIVE: 0.1538\n",
      "  –ü–ê–ß–ö–ê_NEGATIVE: 0.0008\n",
      "  –ü–ê–ß–ö–ê_NEUTRAL: 0.0012\n",
      "  –ü–ê–ß–ö–ê_POSITIVE: 0.0004\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEGATIVE: 0.0018\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEUTRAL: 0.0021\n",
      "  –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 0.0459\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"inference_transformers_third\")\n",
    "\n",
    "client = MlflowClient(tracking_uri=cfg.mlflow.tracking_uri)\n",
    "\n",
    "# ================================\n",
    "# 2. –ò—â–µ–º –Ω—É–∂–Ω—ã–π run –≤ MLflow\n",
    "# ================================\n",
    "print(\"–ò—â–µ–º run —Å –º–æ–¥–µ–ª—å—é...\")\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[cfg.mlflow.experiment_id],\n",
    "    filter_string=f'attributes.run_name = \"{cfg.model.run_name}\"',\n",
    "    order_by=[\"attributes.end_time desc\"]\n",
    ")\n",
    "\n",
    "if not runs:\n",
    "    raise ValueError(f\"Run '{cfg.model.run_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "\n",
    "run_id = runs[0].info.run_id\n",
    "print(f\"–ù–∞–π–¥–µ–Ω run: {run_id}\")\n",
    "\n",
    "# ================================\n",
    "# 3. –°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã\n",
    "# ================================\n",
    "print(\"–°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä...\")\n",
    "\n",
    "try:\n",
    "    model_dir = client.download_artifacts(run_id, cfg.model.artifacts_path)\n",
    "    tokenizer_dir = client.download_artifacts(run_id, cfg.model.tokenizer_path)\n",
    "    encoder_path = client.download_artifacts(run_id, \"preprocessing/label_encoder.pkl\")\n",
    "    \n",
    "    print(f\"–ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {model_dir}\")\n",
    "    print(f\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–∫–∞—á–∞–Ω: {tokenizer_dir}\")\n",
    "    print(f\"–≠–Ω–∫–æ–¥–µ—Ä —Å–∫–∞—á–∞–Ω: {encoder_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {e}\")\n",
    "    raise\n",
    "\n",
    "# ================================\n",
    "# 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å\n",
    "# ================================\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã...\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "with open(encoder_path, 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "\n",
    "print(\"–í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "\n",
    "# ================================\n",
    "# 5. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "# ================================\n",
    "text = cfg.test_text\n",
    "print(f\"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: ¬´{text}¬ª\")\n",
    "\n",
    "tokenizer_cfg = {k: v for k, v in cfg.tokenizer.items() if v is not None}\n",
    "\n",
    "inputs = tokenizer(text, **tokenizer_cfg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "pred_idx = probs.argmax().item()\n",
    "pred_prob = probs[0][pred_idx].item()\n",
    "\n",
    "pred_label = encoder.inverse_transform([pred_idx])[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: ¬´{text}¬ª\")\n",
    "print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {pred_label}\")\n",
    "print(f\"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {pred_prob:.4f}\")\n",
    "print(\"\\n–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:\")\n",
    "for i, p in enumerate(probs[0]):\n",
    "    label = encoder.inverse_transform([i])[0]\n",
    "    print(f\"  {label}: {p:.4f}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479c692",
   "metadata": {},
   "source": [
    "# –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (–¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [01:59<00:00, 17.08s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.83it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:13<00:00,  3.33s/it] \n",
      "The tokenizer you are loading from 'C:\\Users\\Smart\\AppData\\Local\\Temp\\tmpgi_nhox8\\tokenizer' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.  This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '–í–ö–£–°_NEGATIVE', 'score': 0.9860121607780457}]\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "runs = client.search_runs(experiment_ids = ['0'],\n",
    "                          order_by = ['metrics.f1_score desc', 'metrics.accuracy desc'])\n",
    "\n",
    "best_run = runs[0]\n",
    "\n",
    "\n",
    "best_run_id = best_run.info.run_id\n",
    "\n",
    "model_loc = client.download_artifacts(best_run_id, path='model')\n",
    "tokenizer_loc = client.download_artifacts(best_run_id, path='tokenizer')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_loc)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_loc)\n",
    "\n",
    "\n",
    "model_final = pipeline('text-classification', model = model, tokenizer = tokenizer)\n",
    "\n",
    "print(model_final(['–í–∫—É—Å –ø—Ä–æ—Å—Ç–æ –±–æ–º–±–∏—á–µ—Å–∫–∏–π']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d92e5c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./best_model/tokenizer\\\\tokenizer_config.json',\n",
       " './best_model/tokenizer\\\\special_tokens_map.json',\n",
       " './best_model/tokenizer\\\\vocab.txt',\n",
       " './best_model/tokenizer\\\\added_tokens.json',\n",
       " './best_model/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir_model = './best_model/model'\n",
    "output_dir_tokenizer = './best_model/tokenizer'\n",
    "\n",
    "model.save_pretrained(output_dir_model)\n",
    "tokenizer.save_pretrained(output_dir_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "586f339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'best_model\\tokenizer' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.  This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '–í–ö–£–°_NEGATIVE', 'score': 0.9977165460586548}]\n"
     ]
    }
   ],
   "source": [
    "model_test = AutoModelForSequenceClassification.from_pretrained('best_model\\\\model')\n",
    "tokenizer_test = AutoTokenizer.from_pretrained('best_model\\\\tokenizer')\n",
    "\n",
    "\n",
    "pipe = pipeline('text-classification', model = model_test, tokenizer = tokenizer_test)\n",
    "\n",
    "\n",
    "print(pipe(['–ß–∏–ø—Å—ã –Ω–µ –æ—á–µ–Ω—å']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
