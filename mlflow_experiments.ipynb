{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05f1f49",
   "metadata": {},
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Smart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import *\n",
    "\n",
    "import joblib\n",
    "import cloudpickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from functools import lru_cache\n",
    "\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import re\n",
    "import emoji\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bda6b4",
   "metadata": {},
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5623ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "def load_config(config_name):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\"\"\"\n",
    "    config_path = f\"configs/{config_name}.yml\"\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥\n",
    "    cfg = OmegaConf.load(config_path)\n",
    "    \n",
    "    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "    if 'MLFLOW_TRACKING_URI' in os.environ:\n",
    "        cfg.mlflow.tracking_uri = os.environ['MLFLOW_TRACKING_URI']\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558efd8c",
   "metadata": {},
   "source": [
    "# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e78b22",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e8ba75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!   mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a3e922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI: http://127.0.0.1:8080\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"base\")  # –∏–ª–∏ –¥—Ä—É–≥–æ–µ –∏–º—è –≤–∞—à–µ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "\n",
    "# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å\n",
    "mlflow.set_tracking_uri(cfg.mlflow.tracking_uri)\n",
    "\n",
    "print(f\"Tracking URI: {cfg.mlflow.tracking_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27692760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å—Ö–æ–¥–Ω—ã–π: –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ö–∞–∫–æ–µ –∂–µ –Ω–µ–ø—Ä–∏—è—Ç–Ω–æ–µ –º–µ—Å—Ç–æ, –Ω–µ—Ç?\n",
      "\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π: –≤–µ—Å—å –ø—Ä–∏–≤–µ—Ç –∫–∞–∫–æ–π –Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–π –º–µ—Å—Ç–æ –Ω–µ—Ç\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é\n",
    "cfg = load_config(\"preprocess_first\")\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\n",
    "analyzer = MorphAnalyzer(lang='ru')\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "stop_words_cleaned = [\n",
    "    w for w in stop_words\n",
    "    if w not in cfg.preprocess.keep_words  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ –¥–æ–ª–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å\n",
    "]\n",
    "\n",
    "@lru_cache(maxsize=cfg.preprocess.lru_cache_size)\n",
    "def lemmatization(text):\n",
    "    return analyzer.parse(text)[0].normal_form\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(cfg.preprocess.regex.remove_newlines, \" \", text)\n",
    "\n",
    "    # 2. –§–∏–∫—Å–∏—Ä—É–µ–º –∫—Ä–∏–≤—ã–µ —Å–ª–æ–≤–∞ n—á–∏–ø—Å—ã ‚Üí —á–∏–ø—Å—ã\n",
    "    text = re.sub(cfg.preprocess.regex.fix_mistyped_n, r\"\\1\", text)\n",
    "\n",
    "    # 3. –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã\n",
    "    text = re.sub(cfg.preprocess.regex.remove_symbols, \"\", text)\n",
    "\n",
    "    # 4. –ß–∏—Å—Ç–∏–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(cfg.preprocess.regex.collapse_spaces, \" \", text).strip()\n",
    "\n",
    "    # 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    result = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in stop_words_cleaned:\n",
    "            result.append(lemmatization(token))\n",
    "\n",
    "    return \" \".join(result)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã\n",
    "test_text = \"–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ö–∞–∫–æ–µ –∂–µ –Ω–µ–ø—Ä–∏—è—Ç–Ω–æ–µ –º–µ—Å—Ç–æ, –Ω–µ—Ç?\\n\"\n",
    "processed = preprocess_text(test_text)\n",
    "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π: {test_text}\")\n",
    "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π: {processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb52f206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run First dataset at: http://127.0.0.1:8080/#/experiments/0/runs/d63ca4ae2b054f49b3ce30a18216827a\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"First dataset\"):\n",
    "\n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    annotation_dfs = [\n",
    "        pd.read_json(path) for path in cfg.preprocess.input_files\n",
    "    ]\n",
    "\n",
    "    df_annotations = pd.concat(annotation_dfs)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"span\", \"label\"])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for entry in mark:\n",
    "            span = entry['text']\n",
    "            label = entry['labels'][0]\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥\n",
    "    df['span'] = df['span'].apply(preprocess_text)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    df.to_csv(cfg.preprocess.output.dataset_csv, index=False)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.dataset_csv, \"datasets\")\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å-—Ñ—É–Ω–∫—Ü–∏—é\n",
    "    with open(cfg.preprocess.output.preprocess_pickle, \"wb\") as f:\n",
    "        cloudpickle.dump(preprocess_text, f)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.preprocess_pickle, \"functions\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99a6cf",
   "metadata": {},
   "source": [
    "## –í—Ç–æ—Ä–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98c7fb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...                  O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...                  O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504       —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                     —Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                  –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Second dataset at: http://127.0.0.1:8080/#/experiments/0/runs/38adc29950984b408e94b3a3b5c0cb6c\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "cfg = load_config('preprocess_second')\n",
    "\n",
    "def clean_text_only(text, cfg=None):\n",
    "    \"\"\"\n",
    "    –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "    \n",
    "    Args:\n",
    "        text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç\n",
    "        cfg: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "    lowercase = getattr(cfg, 'clean_only', {}).get('lowercase', True) if cfg else True\n",
    "    replace_emoji = getattr(cfg, 'clean_only', {}).get('replace_emoji', True) if cfg else True\n",
    "    remove_punctuation = getattr(cfg, 'clean_only', {}).get('remove_punctuation', True) if cfg else True\n",
    "    remove_special_chars = getattr(cfg, 'clean_only', {}).get('remove_special_chars', True) if cfg else True\n",
    "    collapse_spaces = getattr(cfg, 'clean_only', {}).get('collapse_spaces', True) if cfg else True\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # –ó–∞–º–µ–Ω—è–µ–º —ç–º–æ–¥–∑–∏\n",
    "    if replace_emoji:\n",
    "        text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã\n",
    "    if remove_special_chars:\n",
    "        text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "    if collapse_spaces:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# –í–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–±–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)\n",
    "def clean_text_only_legacy(text):\n",
    "    \"\"\"–õ–µ–≥–∞—Å–∏ –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\"\"\"\n",
    "    return clean_text_only(text)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name='Second dataset'):\n",
    "    \n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    annotation_dfs = [\n",
    "        pd.read_json(path) for path in cfg.preprocess.input_files\n",
    "    ]\n",
    "\n",
    "    df_annotations = pd.concat(annotation_dfs)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"span\", \"label\"])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for entry in mark:\n",
    "            span = entry['text']\n",
    "            label = entry['labels'][0]\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø–µ—Ä–µ–¥–∞–µ–º cfg –≤ —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞\n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º lambda\n",
    "    df['span'] = df['span'].apply(lambda x: clean_text_only(x, cfg))\n",
    "    \n",
    "    # –ò–ª–∏ –í–∞—Ä–∏–∞–Ω—Ç 2: –°–æ–∑–¥–∞–µ–º —á–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é\n",
    "    # clean_text_with_config = partial(clean_text_only, cfg=cfg)\n",
    "    # df['span'] = df['span'].apply(clean_text_with_config)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    df.to_csv(cfg.preprocess.output.dataset_csv, index=False)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.dataset_csv, \"datasets\")\n",
    "\n",
    "    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥–æ–º\n",
    "    # –°–æ–∑–¥–∞–µ–º —á–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "    clean_text_with_config = partial(clean_text_only, cfg=cfg)\n",
    "    with open(cfg.preprocess.output.preprocess_pickle, \"wb\") as f:\n",
    "        cloudpickle.dump(clean_text_with_config, f)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.preprocess_pickle, \"functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0766fa",
   "metadata": {},
   "source": [
    "## –¢—Ä–µ—Ç–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∏–Ω–æ—Ä–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39b861f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minority classes: ['–ü–ê–ß–ö–ê_POSITIVE', '–¢–ï–ö–°–¢–£–†–ê_NEUTRAL', '–ü–ê–ß–ö–ê_NEUTRAL', '–¢–ï–ö–°–¢–£–†–ê_NEGATIVE', '–ü–ê–ß–ö–ê_NEGATIVE']\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π: third_experiment_dataset.csv\n",
      "üèÉ View run Third dataset at: http://127.0.0.1:8080/#/experiments/0/runs/3f371b6b84564621b3f1f656172c4f72\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Third dataset\"):\n",
    "\n",
    "    cfg = load_config(\"preprocess_third\")\n",
    "\n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "\n",
    "\n",
    "    annotation_dfs = [pd.read_json(path) for path in cfg.preprocess.input_files]\n",
    "    df_annotations = pd.concat(annotation_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"span\", \"label\"])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for entry in mark:\n",
    "            span = entry['text']\n",
    "            label = entry['labels'][0]\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "\n",
    "    class_counts = df['label'].value_counts()\n",
    "    minority_classes = class_counts[class_counts < class_counts.mean()].index.tolist()\n",
    "\n",
    "    print(f\"Minority classes: {minority_classes}\")\n",
    "\n",
    "\n",
    "    import random\n",
    "\n",
    "    synonyms = cfg.preprocess.augmentation.synonyms\n",
    "    n_variants = cfg.preprocess.augmentation.n_variants\n",
    "\n",
    "    def simple_augmentation(text):\n",
    "        augmented = []\n",
    "        for _ in range(n_variants):\n",
    "            words = text.split()\n",
    "            new_words = []\n",
    "            for w in words:\n",
    "                wl = w.lower()\n",
    "                if wl in synonyms and random.random() > 0.7:\n",
    "                    new_words.append(random.choice(synonyms[wl]))\n",
    "                else:\n",
    "                    new_words.append(w)\n",
    "            augmented.append(\" \".join(new_words))\n",
    "        return augmented\n",
    "\n",
    "\n",
    "    augmented_data = []\n",
    "    for label in minority_classes:\n",
    "        samples = df[df['label'] == label]\n",
    "        for _, row in samples.iterrows():\n",
    "            for aug_text in simple_augmentation(row[\"span\"]):\n",
    "                augmented_data.append({\"span\": aug_text, \"label\": label})\n",
    "\n",
    "    df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "\n",
    "    df_extended = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "\n",
    "    output_path = cfg.preprocess.output.dataset_csv\n",
    "    df_extended.to_csv(output_path, index=False)\n",
    "\n",
    "    mlflow.log_artifact(output_path, \"datasets\")\n",
    "\n",
    "    print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790af86c",
   "metadata": {},
   "source": [
    "# –≠–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–æ–¥–µ–ª—è–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b384f",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793612b",
   "metadata": {},
   "source": [
    "–ë—É–¥–µ–º –æ–±—É—á–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å: \"–ù–∞–∏–≤–Ω—ã–π –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\" –∏–∑ `Sklearn`, –±—É–¥–µ–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ 3 –≤–µ—Ä—Å–∏—è—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø–æ–π–º–µ–º, –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ —Å–µ–±—è –ø–æ–∫–∞–∂–µ—Ç –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ–º –∏–ª–∏ –∏–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead861b4",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: datasets/first_experiment_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.3869\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.4422\n",
      "–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run first_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/6251fed9027b47df841d5093c07414b8\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"naive_bayes_first\")\n",
    "\n",
    "with mlflow.start_run(run_name='first_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    full_dataset_path = f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\"\n",
    "    \n",
    "    try:\n",
    "        dataframe_path = client.download_artifacts(first_dataset_latest_run_id, full_dataset_path)\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {full_dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {full_dataset_path}: {e}\")\n",
    "\n",
    "        try:\n",
    "            dataframe_path = client.download_artifacts(first_dataset_latest_run_id, cfg.data.dataset_file)\n",
    "            df = pd.read_csv(dataframe_path)\n",
    "            print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {cfg.data.dataset_file}\")\n",
    "        except:\n",
    "            raise ValueError(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç: {full_dataset_path}\")\n",
    "    \n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=cfg.vectorizer.lowercase,\n",
    "        analyzer=cfg.vectorizer.analyzer,\n",
    "        max_features=cfg.vectorizer.max_features,\n",
    "        ngram_range=tuple(cfg.vectorizer.ngram_range),\n",
    "        min_df=cfg.vectorizer.min_df,\n",
    "        max_df=cfg.vectorizer.max_df\n",
    "    )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    mlflow.sklearn.autolog(disable=True)\n",
    "    \n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy:.4f}')\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã\n",
    "    joblib.dump(model, cfg.model.artifacts.model)\n",
    "    joblib.dump(vectorizer, cfg.model.artifacts.vectorizer)\n",
    "    joblib.dump(encoder, cfg.model.artifacts.encoder)\n",
    "    \n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.vectorizer, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "    \n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "    mlflow.log_metrics({\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    mlflow.log_params({\n",
    "        'vectorizer_max_features': cfg.vectorizer.max_features,\n",
    "        'vectorizer_ngram_range': str(cfg.vectorizer.ngram_range),\n",
    "        'test_size': cfg.training.test_size\n",
    "    })\n",
    "\n",
    "    print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ddbdb",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93388f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 6251fed9027b47df841d5093c07414b8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.26it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 29.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"—è –ø—Ä–∏–≤–ª–µ—á—å —Ç–∞–∫–æ–π —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "cfg_inference = load_config(\"inference_bayes_first\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "\n",
    "vectorizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.vectorizer}\"\n",
    "bayes_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.bayes}\" \n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "try:\n",
    "    vectorizer_file = client.download_artifacts(latest_run_model_id, vectorizer_path)\n",
    "    bayes_file = client.download_artifacts(latest_run_model_id, bayes_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "\n",
    "    try:\n",
    "        vectorizer_file = client.download_artifacts(latest_run_model_id, cfg_inference.model.vectorizer)\n",
    "        bayes_file = client.download_artifacts(latest_run_model_id, cfg_inference.model.bayes)\n",
    "        encoder_file = client.download_artifacts(latest_run_model_id, cfg_inference.model.encoder)\n",
    "        print(\"‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (fallback)\")\n",
    "    except:\n",
    "        raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –§–£–ù–ö–¶–ò–ò –ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì–ê\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_dataset = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.preprocess.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "latest_run_dataset_id = latest_run_dataset[0].info.run_id\n",
    "\n",
    "try:\n",
    "    art_loc = client.download_artifacts(latest_run_dataset_id, cfg_inference.preprocess.artifact_path)\n",
    "    print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "vectorizer = joblib.load(vectorizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "preprocessed_text = preprocess_func(text)\n",
    "done_text = vectorizer.transform([preprocessed_text])\n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {encoder.inverse_transform(label)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3439a",
   "metadata": {},
   "source": [
    "### –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca78c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...                  O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...                  O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504       —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                     —Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                  –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.3675\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.4223\n",
      "–í—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run second_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/f363538c87124ee0a6ced8e5d6eb5376\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = load_config(\"naive_bayes_second\")\n",
    "\n",
    "with mlflow.start_run(run_name='second_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "\n",
    "    try:\n",
    "        dataframe_path = client.download_artifacts(first_dataset_latest_run_id, \"datasets/second_experiment_dataset.csv\")\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        print(\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\")\n",
    "    except:\n",
    "        try:\n",
    "            files = client.list_artifacts(first_dataset_latest_run_id, cfg.data.dataset_path)\n",
    "            dataframe = files[0].path\n",
    "            dataframe_path = client.download_artifacts(first_dataset_latest_run_id, dataframe)\n",
    "            df = pd.read_csv(dataframe_path)\n",
    "            print(\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —á–µ—Ä–µ–∑ list_artifacts\")\n",
    "        except:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=cfg.vectorizer.lowercase,\n",
    "        analyzer=cfg.vectorizer.analyzer,\n",
    "        max_features=cfg.vectorizer.max_features,\n",
    "        ngram_range=tuple(cfg.vectorizer.ngram_range),\n",
    "        min_df=cfg.vectorizer.min_df,\n",
    "        max_df=cfg.vectorizer.max_df\n",
    "    )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy:.4f}')\n",
    "\n",
    "    joblib.dump(model, cfg.model.artifacts.model)\n",
    "    joblib.dump(vectorizer, cfg.model.artifacts.vectorizer)\n",
    "    joblib.dump(encoder, cfg.model.artifacts.encoder)\n",
    "    \n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.vectorizer, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'vectorizer_max_features': cfg.vectorizer.max_features,\n",
    "        'vectorizer_ngram_range': str(cfg.vectorizer.ngram_range),\n",
    "        'test_size': cfg.training.test_size\n",
    "    })\n",
    "\n",
    "    print(\"–í—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfbf25",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f598e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: f363538c87124ee0a6ced8e5d6eb5376\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.02it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 30.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 32.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞...\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–º–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ loudly_crying_face\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_bayes_second\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"', \n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "\n",
    "vectorizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.vectorizer}\"\n",
    "bayes_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.bayes}\"\n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\")\n",
    "try:\n",
    "    vectorizer_file = client.download_artifacts(latest_run_model_id, vectorizer_path)\n",
    "    bayes_file = client.download_artifacts(latest_run_model_id, bayes_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –§–£–ù–ö–¶–ò–ò –ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì–ê\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_dataset = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.preprocess.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_dataset:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.preprocess.run_name}\")\n",
    "\n",
    "latest_run_dataset_id = latest_run_dataset[0].info.run_id\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞...\")\n",
    "try:\n",
    "    art_loc = client.download_artifacts(latest_run_dataset_id, cfg_inference.preprocess.artifact_path)\n",
    "    print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "vectorizer = joblib.load(vectorizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞...\")\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "preprocessed_text = preprocess_func(text)\n",
    "done_text = vectorizer.transform([preprocessed_text])\n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {encoder.inverse_transform(label)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231b62e",
   "metadata": {},
   "source": [
    "### –¢—Ä–µ—Ç–∏–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d475c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: 3f371b6b84564621b3f1f656172c4f72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>—Å —á–∏–ø—Å–∞–º–∏ –≤—Å–µ–≥–¥–∞ —Å—Ç–æ–∏—Ç –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ –¥–≤–µ —Ç—Ä–µ—Ç–∏ ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>–ü–ª–∞—Å—Ç–∏–∫–æ–≤–∞—è –∫—Ä—ã—à–∫–∞ –ø–æ–ø–∞–ª–∞—Å—å –æ—á–µ–Ω—å —Ç—É–≥–∞—è, –µ–ª–µ-–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>–ü–ª–∞—Å—Ç–∏–∫–æ–≤–∞—è –∫—Ä—ã—à–∫–∞ –ø–æ–ø–∞–ª–∞—Å—å –æ—á–µ–Ω—å —Ç—É–≥–∞—è, –µ–ª–µ-–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>–ù–∞–ø–æ–ª–Ω–µ–∏–Ω–µ –ø–∞—á–∫–∏ —Ç–∏–ø–∏—á–Ω–æ–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>–ù–∞–ø–æ–ª–Ω–µ–∏–Ω–µ –ø–∞—á–∫–∏ —Ç–∏–ø–∏—á–Ω–æ–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4020 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span           label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   –í–ö–£–°_POSITIVE\n",
       "1     –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...               O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...               O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞               O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...             ...\n",
       "4015  —Å —á–∏–ø—Å–∞–º–∏ –≤—Å–µ–≥–¥–∞ —Å—Ç–æ–∏—Ç –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ –¥–≤–µ —Ç—Ä–µ—Ç–∏ ...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "4016  –ü–ª–∞—Å—Ç–∏–∫–æ–≤–∞—è –∫—Ä—ã—à–∫–∞ –ø–æ–ø–∞–ª–∞—Å—å –æ—á–µ–Ω—å —Ç—É–≥–∞—è, –µ–ª–µ-–µ...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "4017  –ü–ª–∞—Å—Ç–∏–∫–æ–≤–∞—è –∫—Ä—ã—à–∫–∞ –ø–æ–ø–∞–ª–∞—Å—å –æ—á–µ–Ω—å —Ç—É–≥–∞—è, –µ–ª–µ-–µ...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "4018  –ù–∞–ø–æ–ª–Ω–µ–∏–Ω–µ –ø–∞—á–∫–∏ —Ç–∏–ø–∏—á–Ω–æ–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "4019  –ù–∞–ø–æ–ª–Ω–µ–∏–Ω–µ –ø–∞—á–∫–∏ —Ç–∏–ø–∏—á–Ω–æ–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥...  –ü–ê–ß–ö–ê_NEGATIVE\n",
       "\n",
       "[4020 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.5503\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.5945\n",
      "–¢—Ä–µ—Ç—å—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run third_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/18e0cad0f05f43b585de0ba2b6bf7464\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"naive_bayes_third\")\n",
    "\n",
    "with mlflow.start_run(run_name='third_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        dataframe_path = client.download_artifacts(dataset_run_id, f\"datasets/{cfg.data.dataset_file}\")\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        print(\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "        alternative_paths = [\n",
    "            cfg.data.dataset_file,\n",
    "            f\"artifacts/datasets/{cfg.data.dataset_file}\",\n",
    "            \"third_experiment_dataset.csv\"\n",
    "        ]\n",
    "        \n",
    "        for path in alternative_paths:\n",
    "            try:\n",
    "                dataframe_path = client.download_artifacts(dataset_run_id, path)\n",
    "                df = pd.read_csv(dataframe_path)\n",
    "                print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=cfg.vectorizer.lowercase,\n",
    "        analyzer=cfg.vectorizer.analyzer,\n",
    "        max_features=cfg.vectorizer.max_features,\n",
    "        ngram_range=tuple(cfg.vectorizer.ngram_range),\n",
    "        min_df=cfg.vectorizer.min_df,\n",
    "        max_df=cfg.vectorizer.max_df\n",
    "    )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy:.4f}')\n",
    "\n",
    "    joblib.dump(model, cfg.model.artifacts.model)\n",
    "    joblib.dump(vectorizer, cfg.model.artifacts.vectorizer)\n",
    "    joblib.dump(encoder, cfg.model.artifacts.encoder)\n",
    "    \n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.vectorizer, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'vectorizer_max_features': cfg.vectorizer.max_features,\n",
    "        'vectorizer_ngram_range': str(cfg.vectorizer.ngram_range),\n",
    "        'test_size': cfg.training.test_size\n",
    "    })\n",
    "\n",
    "    print(\"–¢—Ä–µ—Ç—å—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20a7ee",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 18e0cad0f05f43b585de0ba2b6bf7464\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.32it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.49it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 28.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞...\n",
      "‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–º–µ–Ω—è –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –ü–ê–ß–ö–ê_POSITIVE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg_inference = load_config(\"inference_bayes_third\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "\n",
    "vectorizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.vectorizer}\"\n",
    "bayes_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.bayes}\"\n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\")\n",
    "try:\n",
    "    vectorizer_file = client.download_artifacts(latest_run_model_id, vectorizer_path)\n",
    "    bayes_file = client.download_artifacts(latest_run_model_id, bayes_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "vectorizer = joblib.load(vectorizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "done_text = vectorizer.transform([text])\n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {encoder.inverse_transform(label)[0]}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205f87a",
   "metadata": {},
   "source": [
    "### –ò—Ç–æ–≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a20932c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>–í–µ—Ä—Å–∏—è</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>–û–ø–∏—Å–∞–Ω–∏–µ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 1</td>\n",
       "      <td>0.363707</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 2</td>\n",
       "      <td>0.344306</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>–û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 3</td>\n",
       "      <td>0.344139</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     –í–µ—Ä—Å–∏—è  F1-Score  Accuracy                                      –û–ø–∏—Å–∞–Ω–∏–µ\n",
       "0  –í–µ—Ä—Å–∏—è 1  0.363707  0.413043      –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
       "1  –í–µ—Ä—Å–∏—è 2  0.344306  0.391304  –û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text\n",
       "2  –í–µ—Ä—Å–∏—è 3  0.344139  0.391304                               –ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    '–í–µ—Ä—Å–∏—è': ['–í–µ—Ä—Å–∏—è 1', '–í–µ—Ä—Å–∏—è 2', '–í–µ—Ä—Å–∏—è 3'],\n",
    "    'F1-Score': [0.363707, 0.344306, 0.344139],\n",
    "    'Accuracy': [0.413043, 0.391304, 0.391304],\n",
    "    '–û–ø–∏—Å–∞–Ω–∏–µ': ['–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏', '–û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text', '–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏']\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84af634",
   "metadata": {},
   "source": [
    "## –í—Ç–æ—Ä–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2fe0c",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: d63ca4ae2b054f49b3ce30a18216827a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/11/25 10:35:08 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential_2' has no defined input shape yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 129ms/step - accuracy: 0.1919 - loss: 2.1926 - val_accuracy: 0.2032 - val_loss: 2.0884\n",
      "Epoch 2/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 108ms/step - accuracy: 0.3215 - loss: 1.9173 - val_accuracy: 0.3825 - val_loss: 1.8288\n",
      "Epoch 3/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 115ms/step - accuracy: 0.4826 - loss: 1.5438 - val_accuracy: 0.4183 - val_loss: 1.6080\n",
      "Epoch 4/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 119ms/step - accuracy: 0.5947 - loss: 1.2116 - val_accuracy: 0.4382 - val_loss: 1.5604\n",
      "Epoch 5/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 123ms/step - accuracy: 0.6595 - loss: 0.9799 - val_accuracy: 0.4542 - val_loss: 1.5765\n",
      "Epoch 6/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 130ms/step - accuracy: 0.7278 - loss: 0.8085 - val_accuracy: 0.4622 - val_loss: 1.6628\n",
      "Epoch 7/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 125ms/step - accuracy: 0.7921 - loss: 0.6595 - val_accuracy: 0.4741 - val_loss: 1.7249\n",
      "Epoch 8/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 114ms/step - accuracy: 0.8151 - loss: 0.5676 - val_accuracy: 0.4542 - val_loss: 1.8639\n",
      "Epoch 9/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 102ms/step - accuracy: 0.8465 - loss: 0.4820 - val_accuracy: 0.4343 - val_loss: 2.0197\n",
      "Epoch 10/10\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.8624 - loss: 0.4289 - val_accuracy: 0.4183 - val_loss: 2.1185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/25 10:38:00 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: API request to endpoint /api/2.0/mlflow/logged-models failed with error code 404 != 200. Response body: '<!doctype html>\n",
      "<html lang=en>\n",
      "<title>404 Not Found</title>\n",
      "<h1>Not Found</h1>\n",
      "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n",
      "'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4741 - loss: 2.0804\n",
      "Test Accuracy: 0.4741\n",
      "Test Loss: 2.0804\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m128\u001b[0m)        ‚îÇ       \u001b[38;5;34m256,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ        \u001b[38;5;34m98,816\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ         \u001b[38;5;34m1,290\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,068,320</span> (4.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,068,320\u001b[0m (4.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,106</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,106\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">712,214</span> (2.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m712,214\u001b[0m (2.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step\n",
      "f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.4819\n",
      "‚úÖ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run first_experiment_neural_network at: http://127.0.0.1:8080/#/experiments/0/runs/5793fbc2bec14768b3df062c87b0ba98\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = load_config(\"neural_network_first\")\n",
    "\n",
    "with mlflow.start_run(run_name='first_experiment_neural_network'):\n",
    "    \n",
    "    mlflow.set_tag('LSTM', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "    try:\n",
    "        dataframe_path = client.download_artifacts(dataset_run_id, f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\")\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        print(\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "        try:\n",
    "            files = client.list_artifacts(dataset_run_id, cfg.data.dataset_path)\n",
    "            dataframe = files[0].path\n",
    "            dataframe_path = client.download_artifacts(dataset_run_id, dataframe)\n",
    "            df = pd.read_csv(dataframe_path)\n",
    "            print(\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —á–µ—Ä–µ–∑ list_artifacts\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ list_artifacts: {e2}\")\n",
    "            alternative_paths = [\n",
    "                cfg.data.dataset_file,\n",
    "                f\"artifacts/{cfg.data.dataset_path}/{cfg.data.dataset_file}\",\n",
    "                \"First_version.csv\"\n",
    "            ]\n",
    "            \n",
    "            for path in alternative_paths:\n",
    "                try:\n",
    "                    dataframe_path = client.download_artifacts(dataset_run_id, path)\n",
    "                    df = pd.read_csv(dataframe_path)\n",
    "                    print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df)\n",
    "    \n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=cfg.tokenizer.num_words,\n",
    "        oov_token=cfg.tokenizer.oov_token,\n",
    "        filters=cfg.tokenizer.filters,\n",
    "        lower=cfg.tokenizer.lower,\n",
    "        split=cfg.tokenizer.split,\n",
    "        char_level=cfg.tokenizer.char_level\n",
    "    )\n",
    "\n",
    "    tokenizer.fit_on_texts(df['span'])\n",
    "\n",
    "    df_train, df_temp = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    df_test, df_val = train_test_split(\n",
    "        df_temp, \n",
    "        test_size=cfg.training.val_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df_temp['label']\n",
    "    )\n",
    "\n",
    "    X_train_vec = tokenizer.texts_to_sequences(df_train['span'])\n",
    "    X_test_vec = tokenizer.texts_to_sequences(df_test['span'])\n",
    "    X_val_vec = tokenizer.texts_to_sequences(df_val['span'])\n",
    "\n",
    "    max_len_text = 0\n",
    "    for i in df['span']:\n",
    "        max_len_text = max(max_len_text, len(i.split(' ')))\n",
    "    print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {max_len_text}\")\n",
    "\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_vec, max_len_text, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_vec, max_len_text, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_vec, max_len_text, padding='post', truncating='post')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    y_train = encoder.fit_transform(df_train['label'])\n",
    "    y_test = encoder.transform(df_test['label'])\n",
    "    y_val = encoder.transform(df_val['label'])\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(\n",
    "        input_dim=cfg.tokenizer.num_words, \n",
    "        output_dim=cfg.model.embedding_dim, \n",
    "        input_length=max_len_text\n",
    "    ))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        cfg.model.lstm_units, \n",
    "        dropout=0.2, \n",
    "        recurrent_dropout=0.3\n",
    "    )))\n",
    "\n",
    "    model.add(Dropout(cfg.model.dropout_rate))\n",
    "\n",
    "    model.add(Dense(cfg.model.dense_units, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, \n",
    "        y_train, \n",
    "        batch_size=cfg.training.batch_size, \n",
    "        epochs=cfg.training.epochs, \n",
    "        validation_data=(X_val_pad, y_val)\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    pred_proba = model.predict(X_test_pad)\n",
    "    pred_class = np.argmax(pred_proba, axis=1)\n",
    "    \n",
    "    f1 = f1_score(pred_class, y_test, average='weighted')\n",
    "    print(f'f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "\n",
    "    mlflow.log_metric('f1_score', f1)\n",
    "    mlflow.log_metric('test_accuracy', test_accuracy)\n",
    "    mlflow.log_metric('test_loss', test_loss)\n",
    "\n",
    "    model.save(\"LSTM_ver_1.keras\")\n",
    "\n",
    "    with open(cfg.model.artifacts.tokenizer, 'wb') as f:\n",
    "        cloudpickle.dump(tokenizer, f)\n",
    "    \n",
    "    with open(cfg.model.artifacts.encoder, 'wb') as f:\n",
    "        cloudpickle.dump(encoder, f)\n",
    "\n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.tokenizer, 'models') \n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'num_words': cfg.tokenizer.num_words,\n",
    "        'embedding_dim': cfg.model.embedding_dim,\n",
    "        'lstm_units': cfg.model.lstm_units,\n",
    "        'dropout_rate': cfg.model.dropout_rate,\n",
    "        'batch_size': cfg.training.batch_size,\n",
    "        'epochs': cfg.training.epochs\n",
    "    })\n",
    "\n",
    "    print(\"‚úÖ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ff08f",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 5793fbc2bec14768b3df062c87b0ba98\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.91it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 32.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 73\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–í —Ü–µ–ª–æ–º, —á–∏–ø—Å—ã –ê—à–∞–Ω –ö—Ä–∞—Å–Ω–∞—è –ø—Ç–∏—Ü–∞ –ë–∞—Ä–±–µ–∫—é –≤–ø–æ–ª–Ω–µ —Å—ä–µ–¥–æ–±–Ω—ã–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: O\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:\n",
      "  O: 0.8583\n",
      "  –í–ö–£–°_NEGATIVE: 0.0006\n",
      "  –í–ö–£–°_NEUTRAL: 0.0257\n",
      "  –í–ö–£–°_POSITIVE: 0.0022\n",
      "  –ü–ê–ß–ö–ê_NEGATIVE: 0.0020\n",
      "  –ü–ê–ß–ö–ê_NEUTRAL: 0.0415\n",
      "  –ü–ê–ß–ö–ê_POSITIVE: 0.0004\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEGATIVE: 0.0356\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEUTRAL: 0.0226\n",
      "  –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 0.0112\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_neural_network\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "model_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.model_file}\"\n",
    "tokenizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.tokenizer}\"\n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\")\n",
    "try:\n",
    "    model_file = client.download_artifacts(latest_run_model_id, model_path)\n",
    "    tokenizer_file = client.download_artifacts(latest_run_model_id, tokenizer_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "model_keras = tf.keras.models.load_model(model_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "tokenizer = joblib.load(tokenizer_file)\n",
    "\n",
    "print(\"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "\n",
    "tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "max_len = model_keras.input_shape[1]\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_len}\")\n",
    "\n",
    "padded_text = pad_sequences(tokenized_text, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "pred = model_keras.predict(padded_text)\n",
    "\n",
    "predicted_class_ind = np.argmax(pred, axis=1)\n",
    "predicted_class = encoder.inverse_transform(predicted_class_ind)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {predicted_class[0]}')\n",
    "print(f'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:')\n",
    "for i, prob in enumerate(pred[0]):\n",
    "    class_name = encoder.inverse_transform([i])[0]\n",
    "    print(f'  {class_name}: {prob:.4f}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d0476",
   "metadata": {},
   "source": [
    "### –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: 38adc29950984b408e94b3a3b5c0cb6c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: datasets/second_experiment_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...                  O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...                  O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504       —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                     —Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                  –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/11/25 10:38:10 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential_3' has no defined input shape yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.1550 - loss: 2.2458 - val_accuracy: 0.1753 - val_loss: 2.2042\n",
      "Epoch 2/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.1780 - loss: 2.1852 - val_accuracy: 0.1753 - val_loss: 2.1808\n",
      "Epoch 3/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.2278 - loss: 2.0850 - val_accuracy: 0.2032 - val_loss: 2.0804\n",
      "Epoch 4/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3255 - loss: 1.8641 - val_accuracy: 0.3147 - val_loss: 1.9178\n",
      "Epoch 5/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4556 - loss: 1.5206 - val_accuracy: 0.3705 - val_loss: 1.7730\n",
      "Epoch 6/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5259 - loss: 1.3181 - val_accuracy: 0.4263 - val_loss: 1.7265\n",
      "Epoch 7/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5877 - loss: 1.1221 - val_accuracy: 0.4223 - val_loss: 1.6844\n",
      "Epoch 8/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6251 - loss: 0.9918 - val_accuracy: 0.4303 - val_loss: 1.6942\n",
      "Epoch 9/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6825 - loss: 0.8827 - val_accuracy: 0.4382 - val_loss: 1.7135\n",
      "Epoch 10/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7024 - loss: 0.8085 - val_accuracy: 0.4263 - val_loss: 1.7542\n",
      "Epoch 11/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7378 - loss: 0.7191 - val_accuracy: 0.4183 - val_loss: 1.7752\n",
      "Epoch 12/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7652 - loss: 0.6707 - val_accuracy: 0.4223 - val_loss: 1.8277\n",
      "Epoch 13/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7817 - loss: 0.6250 - val_accuracy: 0.4343 - val_loss: 1.8861\n",
      "Epoch 14/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7866 - loss: 0.5974 - val_accuracy: 0.4223 - val_loss: 1.9265\n",
      "Epoch 15/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7941 - loss: 0.5576 - val_accuracy: 0.4223 - val_loss: 1.9912\n",
      "Epoch 16/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8330 - loss: 0.4983 - val_accuracy: 0.4183 - val_loss: 2.0880\n",
      "Epoch 17/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8425 - loss: 0.4654 - val_accuracy: 0.3984 - val_loss: 2.1493\n",
      "Epoch 18/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8445 - loss: 0.4321 - val_accuracy: 0.4024 - val_loss: 2.1653\n",
      "Epoch 19/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8514 - loss: 0.4174 - val_accuracy: 0.4104 - val_loss: 2.2038\n",
      "Epoch 20/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8599 - loss: 0.4064 - val_accuracy: 0.4064 - val_loss: 2.2441\n",
      "Epoch 21/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8759 - loss: 0.3673 - val_accuracy: 0.4064 - val_loss: 2.3110\n",
      "Epoch 22/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8724 - loss: 0.3650 - val_accuracy: 0.4143 - val_loss: 2.3210\n",
      "Epoch 23/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8838 - loss: 0.3376 - val_accuracy: 0.3984 - val_loss: 2.4503\n",
      "Epoch 24/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9018 - loss: 0.3102 - val_accuracy: 0.3904 - val_loss: 2.5269\n",
      "Epoch 25/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8799 - loss: 0.3406 - val_accuracy: 0.3865 - val_loss: 2.5640\n",
      "Epoch 26/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8968 - loss: 0.3070 - val_accuracy: 0.3785 - val_loss: 2.6880\n",
      "Epoch 27/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9063 - loss: 0.2893 - val_accuracy: 0.3944 - val_loss: 2.6770\n",
      "Epoch 28/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9063 - loss: 0.2707 - val_accuracy: 0.3825 - val_loss: 2.8188\n",
      "Epoch 29/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9068 - loss: 0.2693 - val_accuracy: 0.3785 - val_loss: 2.8402\n",
      "Epoch 30/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9103 - loss: 0.2729 - val_accuracy: 0.3705 - val_loss: 2.8375\n",
      "Epoch 31/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9262 - loss: 0.2396 - val_accuracy: 0.3825 - val_loss: 2.9302\n",
      "Epoch 32/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9182 - loss: 0.2521 - val_accuracy: 0.3705 - val_loss: 2.8846\n",
      "Epoch 33/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9222 - loss: 0.2156 - val_accuracy: 0.3745 - val_loss: 3.0429\n",
      "Epoch 34/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9257 - loss: 0.2358 - val_accuracy: 0.3904 - val_loss: 3.0885\n",
      "Epoch 35/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9262 - loss: 0.2135 - val_accuracy: 0.3944 - val_loss: 3.1642\n",
      "Epoch 36/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9247 - loss: 0.2185 - val_accuracy: 0.4104 - val_loss: 3.2062\n",
      "Epoch 37/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9382 - loss: 0.2063 - val_accuracy: 0.3785 - val_loss: 3.2556\n",
      "Epoch 38/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9372 - loss: 0.2069 - val_accuracy: 0.3825 - val_loss: 3.3837\n",
      "Epoch 39/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9392 - loss: 0.1747 - val_accuracy: 0.3825 - val_loss: 3.4057\n",
      "Epoch 40/40\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9357 - loss: 0.1817 - val_accuracy: 0.3944 - val_loss: 3.4988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/25 10:38:50 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: API request to endpoint /api/2.0/mlflow/logged-models failed with error code 404 != 200. Response body: '<!doctype html>\n",
      "<html lang=en>\n",
      "<title>404 Not Found</title>\n",
      "<h1>Not Found</h1>\n",
      "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n",
      "'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">128,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_max_pooling1d_1          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ       \u001b[38;5;34m128,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)         ‚îÇ         \u001b[38;5;34m6,176\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)         ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_max_pooling1d_1          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ           \u001b[38;5;34m528\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ           \u001b[38;5;34m170\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">404,624</span> (1.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m404,624\u001b[0m (1.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,874</span> (526.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m134,874\u001b[0m (526.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,750</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m269,750\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4143 - loss: 3.4621\n",
      "Test Accuracy: 0.4143\n",
      "Test Loss: 3.4621\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.4187\n",
      "‚úÖ CNN –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run second_experiment_neural_network at: http://127.0.0.1:8080/#/experiments/0/runs/cd028d31ab9247ac8198400dfb0f1347\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = load_config(\"neural_network_second\")\n",
    "\n",
    "with mlflow.start_run(run_name='second_experiment_neural_network'):\n",
    "    \n",
    "    mlflow.set_tag('CNN', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "    try:\n",
    "        dataset_path = f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\"\n",
    "        art = client.download_artifacts(dataset_run_id, dataset_path)\n",
    "        df = pd.read_csv(art)\n",
    "        print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "        alternative_paths = [\n",
    "            cfg.data.dataset_file,\n",
    "            f\"artifacts/{cfg.data.dataset_path}/{cfg.data.dataset_file}\",\n",
    "            \"Second_version.csv\"\n",
    "        ]\n",
    "        \n",
    "        for path in alternative_paths:\n",
    "            try:\n",
    "                art = client.download_artifacts(dataset_run_id, path)\n",
    "                df = pd.read_csv(art)\n",
    "                print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    tokenizer_config = {k: v for k, v in cfg.tokenizer.items() if v is not None}\n",
    "    tokenizer = Tokenizer(**tokenizer_config)\n",
    "    \n",
    "    tokenizer.fit_on_texts(df['span'])\n",
    "\n",
    "    df_train, df_temp = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state\n",
    "    )\n",
    "    \n",
    "    df_test, df_val = train_test_split(\n",
    "        df_temp, \n",
    "        test_size=cfg.training.val_size, \n",
    "        random_state=cfg.training.random_state\n",
    "    )\n",
    "\n",
    "    X_train_vec = tokenizer.texts_to_sequences(df_train['span'])\n",
    "    X_test_vec = tokenizer.texts_to_sequences(df_test['span'])\n",
    "    X_val_vec = tokenizer.texts_to_sequences(df_val['span'])\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_vec, maxlen=cfg.training.max_sequence_length, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_vec, maxlen=cfg.training.max_sequence_length, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_vec, maxlen=cfg.training.max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    y_train = encoder.fit_transform(df_train['label'])\n",
    "    y_test = encoder.transform(df_test['label'])\n",
    "    y_val = encoder.transform(df_val['label'])\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò CNN\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Embedding —Å–ª–æ–π\n",
    "    model.add(Embedding(\n",
    "        input_dim=cfg.tokenizer.num_words, \n",
    "        output_dim=cfg.model.embedding_dim, \n",
    "        input_length=cfg.training.max_sequence_length\n",
    "    ))\n",
    "\n",
    "    # Conv1D —Å–ª–æ–π\n",
    "    model.add(Conv1D(\n",
    "        filters=cfg.model.conv_filters,\n",
    "        kernel_size=cfg.model.conv_kernel_size,\n",
    "        activation=cfg.model.conv_activation\n",
    "    ))\n",
    "\n",
    "    model.add(Dropout(cfg.model.dropout_rate_1))\n",
    "\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(Dense(cfg.model.dense_units_1, activation=cfg.model.dense_activation_1))\n",
    "\n",
    "    model.add(Dropout(cfg.model.dropout_rate_2))\n",
    "\n",
    "    model.add(Dense(cfg.model.dense_units_2, activation=cfg.model.dense_activation_2))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, \n",
    "        y_train, \n",
    "        batch_size=cfg.training.batch_size, \n",
    "        epochs=cfg.training.epochs, \n",
    "        validation_data=(X_val_pad, y_val)\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    pred = model.predict(X_test_pad)\n",
    "    pred_class = np.argmax(pred, axis=1)\n",
    "\n",
    "    f1 = f1_score(pred_class, y_test, average='weighted')\n",
    "    print(f'f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "\n",
    "    mlflow.log_metric('f1_score', f1)\n",
    "    mlflow.log_metric('test_accuracy', test_accuracy)\n",
    "    mlflow.log_metric('test_loss', test_loss)\n",
    "\n",
    "    model.save(cfg.model.artifacts.model)\n",
    "\n",
    "    joblib.dump(tokenizer, cfg.model.artifacts.tokenizer)\n",
    "    joblib.dump(encoder, cfg.model.artifacts.encoder)\n",
    "\n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.tokenizer, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'num_words': cfg.tokenizer.num_words,\n",
    "        'embedding_dim': cfg.model.embedding_dim,\n",
    "        'conv_filters': cfg.model.conv_filters,\n",
    "        'conv_kernel_size': cfg.model.conv_kernel_size,\n",
    "        'dropout_rate_1': cfg.model.dropout_rate_1,\n",
    "        'dropout_rate_2': cfg.model.dropout_rate_2,\n",
    "        'dense_units_1': cfg.model.dense_units_1,\n",
    "        'batch_size': cfg.training.batch_size,\n",
    "        'epochs': cfg.training.epochs,\n",
    "        'max_sequence_length': cfg.training.max_sequence_length\n",
    "    })\n",
    "\n",
    "    print(\"‚úÖ CNN –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d25bc",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011461e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: cd028d31ab9247ac8198400dfb0f1347\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.11it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–í —Ü–µ–ª–æ–º, —á–∏–ø—Å—ã –ê—à–∞–Ω –ö—Ä–∞—Å–Ω–∞—è –ø—Ç–∏—Ü–∞ –ë–∞—Ä–±–µ–∫—é –≤–ø–æ–ª–Ω–µ —Å—ä–µ–¥–æ–±–Ω—ã–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_NEUTRAL\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:\n",
      "  O: 0.4616\n",
      "  –í–ö–£–°_NEGATIVE: 0.0000\n",
      "  –í–ö–£–°_NEUTRAL: 0.5211\n",
      "  –í–ö–£–°_POSITIVE: 0.0021\n",
      "  –ü–ê–ß–ö–ê_NEGATIVE: 0.0015\n",
      "  –ü–ê–ß–ö–ê_NEUTRAL: 0.0137\n",
      "  –ü–ê–ß–ö–ê_POSITIVE: 0.0000\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEGATIVE: 0.0000\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEUTRAL: 0.0000\n",
      "  –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 0.0001\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_neural_network_second\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "model_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.model_file}\"\n",
    "tokenizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.tokenizer}\"\n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...\")\n",
    "try:\n",
    "    model_file = client.download_artifacts(latest_run_model_id, model_path)\n",
    "    tokenizer_file = client.download_artifacts(latest_run_model_id, tokenizer_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "model_keras = tf.keras.models.load_model(model_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "tokenizer = joblib.load(tokenizer_file)\n",
    "\n",
    "print(\"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "\n",
    "\n",
    "tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "\n",
    "max_len = model_keras.input_shape[1]\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_len}\")\n",
    "\n",
    "\n",
    "padded_text = pad_sequences(tokenized_text, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "pred = model_keras.predict(padded_text)\n",
    "\n",
    "\n",
    "predicted_class_ind = np.argmax(pred, axis=1)\n",
    "predicted_class = encoder.inverse_transform(predicted_class_ind)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {predicted_class[0]}')\n",
    "print(f'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:')\n",
    "for i, prob in enumerate(pred[0]):\n",
    "    class_name = encoder.inverse_transform([i])[0]\n",
    "    print(f'  {class_name}: {prob:.4f}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e2918",
   "metadata": {},
   "source": [
    "## –¢—Ä–µ—Ç–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74382c43",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78383437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: d63ca4ae2b054f49b3ce30a18216827a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: datasets/first_experiment_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span             labels\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2006/2006 [00:00<00:00, 2259.31 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [00:00<00:00, 1959.11 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [00:00<00:00, 2480.36 examples/s]\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='2510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 624/2510 02:20 < 07:06, 4.42 it/s, Epoch 1.24/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.804900</td>\n",
       "      <td>1.766063</td>\n",
       "      <td>0.316007</td>\n",
       "      <td>0.382470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run transformers_experiment_1 at: http://127.0.0.1:8080/#/experiments/0/runs/ee701fc4d7254031889b2633b9423bec\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 161\u001b[39m\n\u001b[32m    150\u001b[39m trainer = Trainer(\n\u001b[32m    151\u001b[39m     model=model, \n\u001b[32m    152\u001b[39m     args=training_args, \n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m     callbacks=[EarlyStoppingCallback(early_stopping_patience=cfg.training.early_stopping_patience)]\n\u001b[32m    158\u001b[39m )\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# –û–±—É—á–µ–Ω–∏–µ\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\u001b[39;00m\n\u001b[32m    164\u001b[39m final_metrics = trainer.evaluate(dataset_tokenized_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:483\u001b[39m, in \u001b[36msafe_patch.<locals>.safe_patch_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m call_original = update_wrapper_extended(call_original, original)\n\u001b[32m    481\u001b[39m event_logger.log_patch_function_start(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m session.state = \u001b[33m\"\u001b[39m\u001b[33msucceeded\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    486\u001b[39m event_logger.log_patch_function_success(args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\mlflow\\transformers\\__init__.py:2929\u001b[39m, in \u001b[36mautolog.<locals>.train\u001b[39m\u001b[34m(original, *args, **kwargs)\u001b[39m\n\u001b[32m   2927\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(original, *args, **kwargs):\n\u001b[32m   2928\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m disable_discrete_autologging(DISABLED_ANCILLARY_FLAVOR_AUTOLOGGING):\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:474\u001b[39m, in \u001b[36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[39m\u001b[34m(*og_args, **og_kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         original_result = original(*_og_args, **_og_kwargs)\n\u001b[32m    472\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:425\u001b[39m, in \u001b[36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[39m\u001b[34m(original_fn, og_args, og_kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    423\u001b[39m     event_logger.log_original_function_start(og_args, og_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     original_fn_result = \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m     event_logger.log_original_function_success(og_args, og_kwargs)\n\u001b[32m    428\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:471\u001b[39m, in \u001b[36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[39m\u001b[34m(*_og_args, **_og_kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n\u001b[32m    468\u001b[39m     disable_warnings=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    469\u001b[39m     reroute_warnings=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    470\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     original_result = \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_og_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_og_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"transformers_first\")\n",
    "\n",
    "with mlflow.start_run(run_name='transformers_experiment_1'):\n",
    "    \n",
    "    mlflow.set_tag('Transformers', cfg.model.version)\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg.data.source_run}\")\n",
    "\n",
    "    dataset_run = dataset_runs[0]\n",
    "    dataset_run_id = dataset_run.info.run_id\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        dataset_path = f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\"\n",
    "        art_loc = client.download_artifacts(dataset_run_id, dataset_path)\n",
    "        df = pd.read_csv(art_loc)\n",
    "        print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "\n",
    "        alternative_paths = [\n",
    "            cfg.data.dataset_file,\n",
    "            f\"artifacts/{cfg.data.dataset_path}/{cfg.data.dataset_file}\",\n",
    "            \"First_version.csv\"\n",
    "        ]\n",
    "        \n",
    "        for path in alternative_paths:\n",
    "            try:\n",
    "                art_loc = client.download_artifacts(dataset_run_id, path)\n",
    "                df = pd.read_csv(art_loc)\n",
    "                print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "\n",
    "    df = df[[cfg.data.text_column, cfg.data.label_column]]\n",
    "    df = df.rename(columns={cfg.data.label_column: \"labels\"})\n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê/–¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "    df_train, df_temp = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['labels']\n",
    "    )\n",
    "    \n",
    "    df_test, df_val = train_test_split(\n",
    "        df_temp, \n",
    "        test_size=cfg.training.val_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df_temp['labels']\n",
    "    )\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(df_train['labels'])\n",
    "    y_test = encoder.transform(df_test['labels'])\n",
    "    y_val = encoder.transform(df_val['labels'])\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "    mlflow.transformers.autolog()\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model.model_name)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model.model_name, \n",
    "        num_labels=len(encoder.classes_), \n",
    "        id2label={i: label for i, label in enumerate(encoder.classes_)},\n",
    "        label2id={label: i for i, label in enumerate(encoder.classes_)}\n",
    "    )\n",
    "    \n",
    "\n",
    "    dataset_train = Dataset.from_pandas(df_train.assign(labels=y_train))    \n",
    "    dataset_test = Dataset.from_pandas(df_test.assign(labels=y_test))    \n",
    "    dataset_val = Dataset.from_pandas(df_val.assign(labels=y_val))\n",
    "\n",
    "\n",
    "    def tokenize_dataset(row):\n",
    "        tokenizer_config = {k: v for k, v in cfg.tokenizer.items() if v is not None}\n",
    "        return tokenizer(row[cfg.data.text_column], **tokenizer_config)\n",
    "\n",
    "    dataset_tokenized_train = dataset_train.map(tokenize_dataset, batched=False)\n",
    "    dataset_tokenized_test = dataset_test.map(tokenize_dataset, batched=False)\n",
    "    dataset_tokenized_val = dataset_val.map(tokenize_dataset, batched=False)\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=cfg.training.output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        logging_dir='./logs/',\n",
    "        num_train_epochs=cfg.training.num_train_epochs,\n",
    "        learning_rate=cfg.training.learning_rate,\n",
    "        per_device_train_batch_size=cfg.training.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=cfg.training.per_device_eval_batch_size,\n",
    "        eval_strategy=cfg.training.eval_strategy,\n",
    "        save_strategy=cfg.training.save_strategy,\n",
    "        warmup_ratio=cfg.training.warmup_ratio,\n",
    "        lr_scheduler_type=cfg.training.lr_scheduler_type,\n",
    "        metric_for_best_model=cfg.training.metric_for_best_model,\n",
    "        weight_decay=cfg.training.weight_decay,\n",
    "        load_best_model_at_end=cfg.training.load_best_model_at_end,\n",
    "        save_total_limit=cfg.training.save_total_limit,\n",
    "        max_grad_norm=cfg.training.max_grad_norm,\n",
    "        logging_steps=cfg.training.logging_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return {\n",
    "            'f1-score': f1_score(labels, predictions, average='weighted'),\n",
    "            'accuracy': accuracy_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=dataset_tokenized_train,\n",
    "        eval_dataset=dataset_tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=cfg.training.early_stopping_patience)]\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    final_metrics = trainer.evaluate(dataset_tokenized_test)\n",
    "    print(f\"‚úÖ Final test metrics: {final_metrics}\")\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–û–•–†–ê–ù–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "       \n",
    "\n",
    "    model_dir = cfg.model.artifacts.model_dir\n",
    "    tokenizer_dir = cfg.model.artifacts.tokenizer_dir\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "\n",
    "    mlflow.log_artifacts(model_dir, \"model\")\n",
    "    mlflow.log_artifacts(tokenizer_dir, \"tokenizer\")\n",
    "\n",
    "\n",
    "    mlflow.log_params({\n",
    "        'model_name': cfg.model.model_name,\n",
    "        'num_labels': len(encoder.classes_),\n",
    "        'num_train_epochs': cfg.training.num_train_epochs,\n",
    "        'learning_rate': cfg.training.learning_rate,\n",
    "        'batch_size': cfg.training.per_device_train_batch_size,\n",
    "        'early_stopping_patience': cfg.training.early_stopping_patience\n",
    "    })\n",
    "\n",
    "    print(\"‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd1e63",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f288885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: fa6c6a5c7c5f45408cc33664a511cadb\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:13<00:00,  6.95s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:08<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\n",
      "‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\n",
      "–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–í —Ü–µ–ª–æ–º, —á–∏–ø—Å—ã –ê—à–∞–Ω –ö—Ä–∞—Å–Ω–∞—è –ø—Ç–∏—Ü–∞ –ë–∞—Ä–±–µ–∫—é –≤–ø–æ–ª–Ω–µ —Å—ä–µ–¥–æ–±–Ω—ã–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_NEUTRAL\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: 0.2972\n",
      "–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:\n",
      "  O: 0.1258\n",
      "  –í–ö–£–°_NEGATIVE: 0.1413\n",
      "  –í–ö–£–°_NEUTRAL: 0.2972\n",
      "  –í–ö–£–°_POSITIVE: 0.2595\n",
      "  –ü–ê–ß–ö–ê_NEGATIVE: 0.0235\n",
      "  –ü–ê–ß–ö–ê_NEUTRAL: 0.0211\n",
      "  –ü–ê–ß–ö–ê_POSITIVE: 0.0271\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEGATIVE: 0.0261\n",
      "  –¢–ï–ö–°–¢–£–†–ê_NEUTRAL: 0.0381\n",
      "  –¢–ï–ö–°–¢–£–†–ê_POSITIVE: 0.0404\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg_inference = load_config(\"inference_transformers_first\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "if not latest_run_model:\n",
    "    raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω run: {cfg_inference.model.run_name}\")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\")\n",
    "try:\n",
    "\n",
    "    model_dir = client.download_artifacts(latest_run_model_id, cfg_inference.model.artifacts_path)\n",
    "    tokenizer_dir = client.download_artifacts(latest_run_model_id, cfg_inference.model.tokenizer_path)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "print(\"–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...\")\n",
    "\n",
    "\n",
    "tokenizer_config = {k: v for k, v in cfg_inference.tokenizer.items() if v is not None}\n",
    "inputs = tokenizer(text, **tokenizer_config)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "predicted_class_idx = predictions.argmax().item()\n",
    "predicted_prob = predictions.max().item()\n",
    "\n",
    "id2label = model.config.id2label\n",
    "predicted_label = id2label[predicted_class_idx]\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {predicted_label}')\n",
    "print(f'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {predicted_prob:.4f}')\n",
    "print(f'–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:')\n",
    "for i, prob in enumerate(predictions[0]):\n",
    "    label = id2label[i]\n",
    "    print(f'  {label}: {prob:.4f}')\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7429c5",
   "metadata": {},
   "source": [
    "### –¢—Ä–µ—Ç–∏–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd76d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"transformer_third\")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=cfg.mlflow.run_name):\n",
    "\n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "    mlflow.log_param(\"model_name\", cfg.model.name)\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    # ---------------------------\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_name], \n",
    "        filter_string=f\"tags.mlflow.runName = 'Third dataset'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    if not dataset_runs:\n",
    "        raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º 'Third dataset'\")\n",
    "\n",
    "    dataset_run_id = dataset_runs[0].info.run_id\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω run —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_run_id}\")\n",
    "\n",
    "\n",
    "    dataset_path_in_run = \"datasets/third_experiment_dataset.csv\" \n",
    "    artifact_local_path = client.download_artifacts(dataset_run_id, dataset_path_in_run)\n",
    "\n",
    "\n",
    "    df = pd.read_csv(artifact_local_path)\n",
    "    print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ run: {artifact_local_path}\")\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    df[\"labels\"] = encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "    mlflow.log_param(\"num_labels\", len(encoder.classes_))\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –¢—Ä–µ–π–Ω/–í–∞–ª/–¢–µ—Å—Ç\n",
    "    # ---------------------------\n",
    "    df_train, df_temp = train_test_split(\n",
    "        df,\n",
    "        test_size=1 - cfg.data.train_size,\n",
    "        random_state=cfg.data.random_state,\n",
    "        shuffle=True,\n",
    "        stratify=df[\"labels\"]\n",
    "    )\n",
    "\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_temp,\n",
    "        test_size=cfg.data.val_size,\n",
    "        random_state=cfg.data.random_state,\n",
    "        shuffle=True,\n",
    "        stratify=df_temp[\"labels\"]\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    #   HuggingFace dataset\n",
    "    # ---------------------------\n",
    "    dataset_train = Dataset.from_pandas(df_train)\n",
    "    dataset_val = Dataset.from_pandas(df_val)\n",
    "    dataset_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "    # ---------------------------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"span\"],\n",
    "            truncation=True,\n",
    "            max_length=cfg.model.max_length\n",
    "        )\n",
    "\n",
    "    tokenized_train = dataset_train.map(tokenize, batched=True)\n",
    "    tokenized_val = dataset_val.map(tokenize, batched=True)\n",
    "    tokenized_test = dataset_test.map(tokenize, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        max_length=cfg.model.max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "    # ---------------------------\n",
    "    num_labels = len(encoder.classes_)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model.name,\n",
    "        num_labels=num_labels,\n",
    "        id2label={i: label for i, label in enumerate(encoder.classes_)},\n",
    "        label2id={label: i for i, label in enumerate(encoder.classes_)},\n",
    "        hidden_dropout_prob=cfg.model.dropout.hidden,\n",
    "        attention_probs_dropout_prob=cfg.model.dropout.attention,\n",
    "        classifier_dropout=cfg.model.dropout.classifier\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    #   Training Arguments\n",
    "    # ---------------------------\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=cfg.training.output_dir,\n",
    "        num_train_epochs=cfg.training.num_train_epochs,\n",
    "        learning_rate=cfg.training.learning_rate,\n",
    "        per_device_train_batch_size=cfg.training.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=cfg.training.per_device_eval_batch_size,\n",
    "        warmup_ratio=cfg.training.warmup_ratio,\n",
    "        lr_scheduler_type=cfg.training.lr_scheduler_type,\n",
    "        evaluation_strategy=cfg.training.eval_strategy,\n",
    "        save_strategy=cfg.training.save_strategy,\n",
    "        weight_decay=cfg.training.weight_decay,\n",
    "        logging_steps=cfg.training.logging_steps,\n",
    "        save_total_limit=cfg.training.save_total_limit,\n",
    "        max_grad_norm=cfg.training.max_grad_norm,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1-score\",\n",
    "        dataloader_num_workers=2\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –ú–µ—Ç—Ä–∏–∫–∏\n",
    "    # ---------------------------\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "\n",
    "        return {\n",
    "            \"f1-score\": f1_score(labels, preds, average=\"weighted\"),\n",
    "            \"accuracy\": accuracy_score(labels, preds)\n",
    "        }\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –û–±—É—á–µ–Ω–∏–µ\n",
    "    # ---------------------------\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=cfg.training.early_stopping_patience)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –û—Ü–µ–Ω–∫–∞\n",
    "    # ---------------------------\n",
    "    eval_test = trainer.evaluate(tokenized_test)\n",
    "    mlflow.log_metrics(eval_test)\n",
    "\n",
    "    # ---------------------------\n",
    "    #   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    # ---------------------------\n",
    "    trainer.save_model(\"best_transformer_model\")\n",
    "    tokenizer.save_pretrained(\"best_transformer_tokenizer\")\n",
    "\n",
    "    mlflow.log_artifact(\"best_transformer_model\", artifact_path=\"models\")\n",
    "    mlflow.log_artifact(\"best_transformer_tokenizer\", artifact_path=\"tokenizer\")\n",
    "\n",
    "    print(\"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2015c",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∏–Ω—Ñ–µ—Ä–µ–Ω—Å)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"inference_transformers_third\")\n",
    "\n",
    "client = MlflowClient(tracking_uri=cfg.mlflow.tracking_uri)\n",
    "\n",
    "# ================================\n",
    "# 2. –ò—â–µ–º –Ω—É–∂–Ω—ã–π run –≤ MLflow\n",
    "# ================================\n",
    "print(\"–ò—â–µ–º run —Å –º–æ–¥–µ–ª—å—é...\")\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[cfg.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg.model.run_name}\"',\n",
    "    order_by=[\"attributes.end_time desc\"]\n",
    ")\n",
    "\n",
    "if not runs:\n",
    "    raise ValueError(f\"‚ùå Run '{cfg.model.run_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "\n",
    "run_id = runs[0].info.run_id\n",
    "print(f\"‚úÖ –ù–∞–π–¥–µ–Ω run: {run_id}\")\n",
    "\n",
    "# ================================\n",
    "# 3. –°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã\n",
    "# ================================\n",
    "print(\"–°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...\")\n",
    "\n",
    "model_dir = client.download_artifacts(run_id, cfg.model.artifacts_path)\n",
    "tokenizer_dir = client.download_artifacts(run_id, cfg.model.tokenizer_path)\n",
    "\n",
    "print(f\"üìÅ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {model_dir}\")\n",
    "print(f\"üìÅ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–∫–∞—á–∞–Ω: {tokenizer_dir}\")\n",
    "\n",
    "# ================================\n",
    "# 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å\n",
    "# ================================\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "id2label = model.config.id2label\n",
    "\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "\n",
    "# ================================\n",
    "# 5. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "# ================================\n",
    "text = cfg.test_text\n",
    "print(\"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç:\", text)\n",
    "\n",
    "tokenizer_cfg = {k: v for k, v in cfg.tokenizer.items() if v is not None}\n",
    "\n",
    "inputs = tokenizer(text, **tokenizer_cfg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "pred_idx = probs.argmax().item()\n",
    "pred_label = id2label[pred_idx]\n",
    "pred_prob = probs[0][pred_idx].item()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: ¬´{text}¬ª\")\n",
    "print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {pred_label}\")\n",
    "print(f\"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {pred_prob:.4f}\")\n",
    "print(\"\\n–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:\")\n",
    "for i, p in enumerate(probs[0]):\n",
    "    print(f\"  {id2label[i]}: {p:.4f}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479c692",
   "metadata": {},
   "source": [
    "# –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (–¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf005a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
