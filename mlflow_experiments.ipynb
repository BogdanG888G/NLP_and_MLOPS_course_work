{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05f1f49",
   "metadata": {},
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Smart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import *\n",
    "\n",
    "import joblib\n",
    "import cloudpickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from functools import lru_cache\n",
    "\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import re\n",
    "import emoji\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bda6b4",
   "metadata": {},
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c5623ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "def load_config(config_name):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\"\"\"\n",
    "    config_path = f\"configs/{config_name}.yml\"\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥\n",
    "    cfg = OmegaConf.load(config_path)\n",
    "    \n",
    "    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "    if 'MLFLOW_TRACKING_URI' in os.environ:\n",
    "        cfg.mlflow.tracking_uri = os.environ['MLFLOW_TRACKING_URI']\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558efd8c",
   "metadata": {},
   "source": [
    "# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e78b22",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9e8ba75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!   mlflow server --host 127.0.0.1 --port 8080 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4a3e922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI: http://127.0.0.1:8080\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"base\")  # –∏–ª–∏ –¥—Ä—É–≥–æ–µ –∏–º—è –≤–∞—à–µ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "\n",
    "# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å\n",
    "mlflow.set_tracking_uri(cfg.mlflow.tracking_uri)\n",
    "\n",
    "print(f\"Tracking URI: {cfg.mlflow.tracking_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "27692760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å—Ö–æ–¥–Ω—ã–π: –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ö–∞–∫–æ–µ –∂–µ –Ω–µ–ø—Ä–∏—è—Ç–Ω–æ–µ –º–µ—Å—Ç–æ, –Ω–µ—Ç?\n",
      "\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π: –≤–µ—Å—å –ø—Ä–∏–≤–µ—Ç –∫–∞–∫–æ–π –Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–π –º–µ—Å—Ç–æ –Ω–µ—Ç\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é\n",
    "cfg = load_config(\"preprocess_first\")\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\n",
    "analyzer = MorphAnalyzer(lang='ru')\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "stop_words_cleaned = [\n",
    "    w for w in stop_words\n",
    "    if w not in cfg.preprocess.keep_words  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ –¥–æ–ª–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å\n",
    "]\n",
    "\n",
    "@lru_cache(maxsize=cfg.preprocess.lru_cache_size)\n",
    "def lemmatization(text):\n",
    "    return analyzer.parse(text)[0].normal_form\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(cfg.preprocess.regex.remove_newlines, \" \", text)\n",
    "\n",
    "    # 2. –§–∏–∫—Å–∏—Ä—É–µ–º –∫—Ä–∏–≤—ã–µ —Å–ª–æ–≤–∞ n—á–∏–ø—Å—ã ‚Üí —á–∏–ø—Å—ã\n",
    "    text = re.sub(cfg.preprocess.regex.fix_mistyped_n, r\"\\1\", text)\n",
    "\n",
    "    # 3. –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã\n",
    "    text = re.sub(cfg.preprocess.regex.remove_symbols, \"\", text)\n",
    "\n",
    "    # 4. –ß–∏—Å—Ç–∏–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(cfg.preprocess.regex.collapse_spaces, \" \", text).strip()\n",
    "\n",
    "    # 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    result = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in stop_words_cleaned:\n",
    "            result.append(lemmatization(token))\n",
    "\n",
    "    return \" \".join(result)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã\n",
    "test_text = \"–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ö–∞–∫–æ–µ –∂–µ –Ω–µ–ø—Ä–∏—è—Ç–Ω–æ–µ –º–µ—Å—Ç–æ, –Ω–µ—Ç?\\n\"\n",
    "processed = preprocess_text(test_text)\n",
    "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π: {test_text}\")\n",
    "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π: {processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb52f206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run First dataset at: http://127.0.0.1:8080/#/experiments/0/runs/1f8492459448453aa853b9f62e4b32eb\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"First dataset\"):\n",
    "\n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    annotation_dfs = [\n",
    "        pd.read_json(path) for path in cfg.preprocess.input_files\n",
    "    ]\n",
    "\n",
    "    df_annotations = pd.concat(annotation_dfs)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"span\", \"label\"])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for entry in mark:\n",
    "            span = entry['text']\n",
    "            label = entry['labels'][0]\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥\n",
    "    df['span'] = df['span'].apply(preprocess_text)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    df.to_csv(cfg.preprocess.output.dataset_csv, index=False)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.dataset_csv, \"datasets\")\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å-—Ñ—É–Ω–∫—Ü–∏—é\n",
    "    with open(cfg.preprocess.output.preprocess_pickle, \"wb\") as f:\n",
    "        cloudpickle.dump(preprocess_text, f)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.preprocess_pickle, \"functions\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99a6cf",
   "metadata": {},
   "source": [
    "## –í—Ç–æ—Ä–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "98c7fb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...                  O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...                  O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é –Ω–æ–≤–æ–≥–æ–¥–Ω–µ...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504       —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                     —Å–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–µ –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö –ø–æ–ª–æ–º–∞–Ω–Ω—ã—Ö ...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                  –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ —Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Second dataset at: http://127.0.0.1:8080/#/experiments/0/runs/75dac97571b544cd941c19b35ff37983\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def clean_text_only(text, cfg=None):\n",
    "    \"\"\"\n",
    "    –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "    \n",
    "    Args:\n",
    "        text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç\n",
    "        cfg: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "    lowercase = getattr(cfg, 'clean_only', {}).get('lowercase', True) if cfg else True\n",
    "    replace_emoji = getattr(cfg, 'clean_only', {}).get('replace_emoji', True) if cfg else True\n",
    "    remove_punctuation = getattr(cfg, 'clean_only', {}).get('remove_punctuation', True) if cfg else True\n",
    "    remove_special_chars = getattr(cfg, 'clean_only', {}).get('remove_special_chars', True) if cfg else True\n",
    "    collapse_spaces = getattr(cfg, 'clean_only', {}).get('collapse_spaces', True) if cfg else True\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # –ó–∞–º–µ–Ω—è–µ–º —ç–º–æ–¥–∑–∏\n",
    "    if replace_emoji:\n",
    "        text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã\n",
    "    if remove_special_chars:\n",
    "        text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "    if collapse_spaces:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# –í–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–±–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)\n",
    "def clean_text_only_legacy(text):\n",
    "    \"\"\"–õ–µ–≥–∞—Å–∏ –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\"\"\"\n",
    "    return clean_text_only(text)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name='Second dataset'):\n",
    "    \n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    annotation_dfs = [\n",
    "        pd.read_json(path) for path in cfg.preprocess.input_files\n",
    "    ]\n",
    "\n",
    "    df_annotations = pd.concat(annotation_dfs)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"span\", \"label\"])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for entry in mark:\n",
    "            span = entry['text']\n",
    "            label = entry['labels'][0]\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø–µ—Ä–µ–¥–∞–µ–º cfg –≤ —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞\n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º lambda\n",
    "    df['span'] = df['span'].apply(lambda x: clean_text_only(x, cfg))\n",
    "    \n",
    "    # –ò–ª–∏ –í–∞—Ä–∏–∞–Ω—Ç 2: –°–æ–∑–¥–∞–µ–º —á–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é\n",
    "    # clean_text_with_config = partial(clean_text_only, cfg=cfg)\n",
    "    # df['span'] = df['span'].apply(clean_text_with_config)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    df.to_csv(cfg.preprocess.output.dataset_csv, index=False)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.dataset_csv, \"datasets\")\n",
    "\n",
    "    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥–æ–º\n",
    "    # –°–æ–∑–¥–∞–µ–º —á–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "    clean_text_with_config = partial(clean_text_only, cfg=cfg)\n",
    "    with open(cfg.preprocess.output.preprocess_pickle, \"wb\") as f:\n",
    "        cloudpickle.dump(clean_text_with_config, f)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.preprocess_pickle, \"functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0766fa",
   "metadata": {},
   "source": [
    "## –¢—Ä–µ—Ç–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39b861f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>–Ø—Ä–∫–æ , –∫—Ä–∞—Å–∏–≤–æ. –§–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é, –Ω–æ–≤–æ–≥...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>–ß–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ, –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –Ω–æ –∑–∞–º–µ—Ç–Ω–æ.</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>–°–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–µ, –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ, –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö, –ø–æ–ª–æ–º–∞–Ω–Ω...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ-—Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                             –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...                  O\n",
       "2     —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...                  O\n",
       "3                —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  –Ø—Ä–∫–æ , –∫—Ä–∞—Å–∏–≤–æ. –§–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç —è –ª—é–±–ª—é, –Ω–æ–≤–æ–≥...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504    –ß–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–µ, –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –Ω–æ –∑–∞–º–µ—Ç–Ω–æ.   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                     –°–∞–º–∏ —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–µ, –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–µ, –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã—Ö, –ø–æ–ª–æ–º–∞–Ω–Ω...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                  –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—è –≤—Å–µ-—Ç–∞–∫–∏ –Ω–∞ –ª—é–±–∏—Ç–µ–ª—è       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Third dataset at: http://127.0.0.1:8080/#/experiments/0/runs/af9721d820364e839d0a6d2a5735ed57\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='Third dataset'):\n",
    "    \n",
    "    mlflow.set_tag(\"Dataset_version\", cfg.mlflow.dataset_version)\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    annotation_dfs = [\n",
    "        pd.read_json(path) for path in cfg.preprocess.input_files\n",
    "    ]\n",
    "\n",
    "    df_annotations = pd.concat(annotation_dfs)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"span\", \"label\"])\n",
    "\n",
    "    for mark in df_annotations['aspect_sentiment']:\n",
    "        for entry in mark:\n",
    "            span = entry['text']\n",
    "            label = entry['labels'][0]\n",
    "            df.loc[len(df)] = [span, label]\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    df.to_csv(cfg.preprocess.output.dataset_csv, index=False)\n",
    "\n",
    "    mlflow.log_artifact(cfg.preprocess.output.dataset_csv, \"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790af86c",
   "metadata": {},
   "source": [
    "# –≠–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–æ–¥–µ–ª—è–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b384f",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793612b",
   "metadata": {},
   "source": [
    "–ë—É–¥–µ–º –æ–±—É—á–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å: \"–ù–∞–∏–≤–Ω—ã–π –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\" –∏–∑ `Sklearn`, –±—É–¥–µ–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ 3 –≤–µ—Ä—Å–∏—è—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø–æ–π–º–µ–º, –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ —Å–µ–±—è –ø–æ–∫–∞–∂–µ—Ç –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ–º –∏–ª–∏ –∏–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead861b4",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56e92964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –°–¢–†–£–ö–¢–£–†–ê –ö–û–ù–§–ò–ì–ê ===\n",
      "mlflow:\n",
      "  tracking_uri: http://127.0.0.1:8080\n",
      "  experiment_id: '0'\n",
      "data:\n",
      "  source_run: First dataset\n",
      "  dataset_file: first_experiment_dataset.csv\n",
      "  dataset_path: datasets\n",
      "vectorizer:\n",
      "  lowercase: true\n",
      "  analyzer: word\n",
      "  max_features: 5000\n",
      "  ngram_range:\n",
      "  - 1\n",
      "  - 4\n",
      "  min_df: 2\n",
      "  max_df: 0.9\n",
      "training:\n",
      "  test_size: 0.2\n",
      "  random_state: 42\n",
      "model:\n",
      "  version: 1.0.0\n",
      "  artifacts:\n",
      "    model: naive_bayes_model.joblib\n",
      "    vectorizer: tfidf_vectorizer.joblib\n",
      "    encoder: label_encoder.joblib\n",
      "\n",
      "‚úì –°–µ–∫—Ü–∏—è 'mlflow' –Ω–∞–π–¥–µ–Ω–∞\n",
      "‚úì –°–µ–∫—Ü–∏—è 'data' –Ω–∞–π–¥–µ–Ω–∞\n",
      "‚úì –°–µ–∫—Ü–∏—è 'vectorizer' –Ω–∞–π–¥–µ–Ω–∞\n",
      "‚úì –°–µ–∫—Ü–∏—è 'training' –Ω–∞–π–¥–µ–Ω–∞\n",
      "‚úì –°–µ–∫—Ü–∏—è 'model' –Ω–∞–π–¥–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "cfg = load_config(\"naive_bayes_train\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "print(\"=== –°–¢–†–£–ö–¢–£–†–ê –ö–û–ù–§–ò–ì–ê ===\")\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å–µ–∫—Ü–∏–π\n",
    "required_sections = ['mlflow', 'data', 'vectorizer', 'training', 'model']\n",
    "for section in required_sections:\n",
    "    if section in cfg:\n",
    "        print(f\"‚úì –°–µ–∫—Ü–∏—è '{section}' –Ω–∞–π–¥–µ–Ω–∞\")\n",
    "    else:\n",
    "        print(f\"‚úó –°–µ–∫—Ü–∏—è '{section}' –ù–ï –Ω–∞–π–¥–µ–Ω–∞\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5e81f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 738.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: datasets/first_experiment_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>—è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>—á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>—Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...</td>\n",
       "      <td>–¢–ï–ö–°–¢–£–†–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>–≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å</td>\n",
       "      <td>–í–ö–£–°_NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2508 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   span              label\n",
       "0                                 –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π      –í–ö–£–°_POSITIVE\n",
       "1     —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...                  O\n",
       "2     —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...                  O\n",
       "3                             –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞                  O\n",
       "4             –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π      –í–ö–£–°_NEGATIVE\n",
       "...                                                 ...                ...\n",
       "2503  —è—Ä–∫–æ –∫—Ä–∞—Å–∏–≤–æ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ü–≤–µ—Ç –ª—é–±–∏—Ç—å –Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π...     –ü–ê–ß–ö–ê_POSITIVE\n",
       "2504          —á–∏–ø—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –∂–∏—Ä–Ω—ã–π –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞–º–µ—Ç–Ω–æ   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2505                      —Å–∞–º —á–∏–ø—Å—ã –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–π   –¢–ï–ö–°–¢–£–†–ê_NEUTRAL\n",
       "2506  –∫—Ä–∞—Å–∏–≤—ã–π –∑–æ–ª–æ—Ç–∏—Å—Ç—ã–π –Ω–µ—Ç –ø—Ä–∏–≥–æ—Ä–µ–ª—ã–π –ø–æ–ª–æ–º–∞—Ç—å –º–∏...  –¢–ï–ö–°–¢–£–†–ê_POSITIVE\n",
       "2507                      –≤–∫—É—Å —Ç—Ä—é—Ñ–µ–ª—å –≤—Å–µ—Ç–∞–∫–∏ –ª—é–±–∏—Ç–µ–ª—å       –í–ö–£–°_NEUTRAL\n",
       "\n",
       "[2508 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.3869\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.4422\n",
      "–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\n",
      "üèÉ View run first_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/3d737c05885647aaaeb4174bc7809e48\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='first_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', cfg.model.version)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=[cfg.mlflow.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{cfg.data.source_run}'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    full_dataset_path = f\"{cfg.data.dataset_path}/{cfg.data.dataset_file}\"\n",
    "    \n",
    "    try:\n",
    "        dataframe_path = client.download_artifacts(first_dataset_latest_run_id, full_dataset_path)\n",
    "        df = pd.read_csv(dataframe_path)\n",
    "        print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {full_dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {full_dataset_path}: {e}\")\n",
    "        # Fallback: –ø—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞\n",
    "        try:\n",
    "            dataframe_path = client.download_artifacts(first_dataset_latest_run_id, cfg.data.dataset_file)\n",
    "            df = pd.read_csv(dataframe_path)\n",
    "            print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {cfg.data.dataset_file}\")\n",
    "        except:\n",
    "            raise ValueError(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç: {full_dataset_path}\")\n",
    "    \n",
    "    display(df)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=cfg.vectorizer.lowercase,\n",
    "        analyzer=cfg.vectorizer.analyzer,\n",
    "        max_features=cfg.vectorizer.max_features,\n",
    "        ngram_range=tuple(cfg.vectorizer.ngram_range),\n",
    "        min_df=cfg.vectorizer.min_df,\n",
    "        max_df=cfg.vectorizer.max_df\n",
    "    )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, \n",
    "        test_size=cfg.training.test_size, \n",
    "        random_state=cfg.training.random_state, \n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    # –û—Ç–∫–ª—é—á–∞–µ–º autologging —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å warning\n",
    "    mlflow.sklearn.autolog(disable=True)  # ‚Üê –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π autolog\n",
    "    \n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1:.4f}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy:.4f}')\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã\n",
    "    joblib.dump(model, cfg.model.artifacts.model)\n",
    "    joblib.dump(vectorizer, cfg.model.artifacts.vectorizer)\n",
    "    joblib.dump(encoder, cfg.model.artifacts.encoder)\n",
    "    \n",
    "    mlflow.log_artifact(cfg.model.artifacts.model, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.vectorizer, 'models')\n",
    "    mlflow.log_artifact(cfg.model.artifacts.encoder, 'models')\n",
    "    \n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "    mlflow.log_metrics({\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "    mlflow.log_params({\n",
    "        'vectorizer_max_features': cfg.vectorizer.max_features,\n",
    "        'vectorizer_ngram_range': str(cfg.vectorizer.ngram_range),\n",
    "        'test_size': cfg.training.test_size\n",
    "    })\n",
    "\n",
    "    print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ddbdb",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "93388f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID –º–æ–¥–µ–ª–∏: 3d737c05885647aaaeb4174bc7809e48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 696.96it/s] \n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 631.86it/s]  \n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 248.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 804.74it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"—è –ø—Ä–∏–≤–ª–µ—á—å —Ç–∞–∫–æ–π —Å–æ—á–µ—Ç–∞–Ω–∏–µ\"\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: –í–ö–£–°_POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "cfg_inference = load_config(\"inference\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "# –ò—â–µ–º run —Å –º–æ–¥–µ–ª—å—é\n",
    "latest_run_model = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.model.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "print(f\"Run ID –º–æ–¥–µ–ª–∏: {latest_run_model_id}\")\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏ –Ω–∞–ø—Ä—è–º—É—é (–±–µ–∑ list_artifacts)\n",
    "vectorizer_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.vectorizer}\"\n",
    "bayes_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.bayes}\" \n",
    "encoder_path = f\"{cfg_inference.model.artifacts_path}/{cfg_inference.model.encoder}\"\n",
    "\n",
    "try:\n",
    "    vectorizer_file = client.download_artifacts(latest_run_model_id, vectorizer_path)\n",
    "    bayes_file = client.download_artifacts(latest_run_model_id, bayes_path)\n",
    "    encoder_file = client.download_artifacts(latest_run_model_id, encoder_path)\n",
    "    print(\"‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "    # Fallback: –ø—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å –±–µ–∑ –ø—É—Ç–∏\n",
    "    try:\n",
    "        vectorizer_file = client.download_artifacts(latest_run_model_id, cfg_inference.model.vectorizer)\n",
    "        bayes_file = client.download_artifacts(latest_run_model_id, cfg_inference.model.bayes)\n",
    "        encoder_file = client.download_artifacts(latest_run_model_id, cfg_inference.model.encoder)\n",
    "        print(\"‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (fallback)\")\n",
    "    except:\n",
    "        raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏\")\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–û–õ–£–ß–ï–ù–ò–ï –§–£–ù–ö–¶–ò–ò –ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì–ê\n",
    "# ===========================================================================================\n",
    "\n",
    "# –ò—â–µ–º run —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞\n",
    "latest_run_dataset = client.search_runs(\n",
    "    experiment_ids=[cfg_inference.mlflow.experiment_id],\n",
    "    filter_string=f'tags.mlflow.runName = \"{cfg_inference.preprocess.run_name}\"',\n",
    "    order_by=['attributes.end_time desc']\n",
    ")\n",
    "\n",
    "latest_run_dataset_id = latest_run_dataset[0].info.run_id\n",
    "\n",
    "try:\n",
    "    art_loc = client.download_artifacts(latest_run_dataset_id, cfg_inference.preprocess.artifact_path)\n",
    "    print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏: {e}\")\n",
    "    raise\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "# ===========================================================================================\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏\n",
    "vectorizer = joblib.load(vectorizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "# ===========================================================================================\n",
    "# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# ===========================================================================================\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
    "text = cfg_inference.test_text\n",
    "\n",
    "preprocessed_text = preprocess_func(text)\n",
    "done_text = vectorizer.transform([preprocessed_text])\n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {encoder.inverse_transform(label)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3439a",
   "metadata": {},
   "source": [
    "### –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca78c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>—É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2065</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2066</td>\n",
       "      <td>–∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               span  \\\n",
       "0              0                          –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   \n",
       "1              1  —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...   \n",
       "2              2  —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...   \n",
       "3              3             —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞   \n",
       "4              4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   \n",
       "...          ...                                                ...   \n",
       "2063        2063  check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...   \n",
       "2064        2064             —É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥   \n",
       "2065        2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å   \n",
       "2066        2066                               –∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é   \n",
       "2067        2067  –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...   \n",
       "\n",
       "               label  \n",
       "0      –í–ö–£–°_POSITIVE  \n",
       "1                  O  \n",
       "2                  O  \n",
       "3                  O  \n",
       "4      –í–ö–£–°_NEGATIVE  \n",
       "...              ...  \n",
       "2063   –í–ö–£–°_POSITIVE  \n",
       "2064  –ü–ê–ß–ö–ê_POSITIVE  \n",
       "2065   –í–ö–£–°_POSITIVE  \n",
       "2066   –í–ö–£–°_POSITIVE  \n",
       "2067               O  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.31295994125933585\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.37922705314009664\n",
      "üèÉ View run second_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/448ebe798ba94371abc1f9c5491c9de0\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = 'second_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', '2.0.0')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=['0'],  # Default —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç ID = 0\n",
    "        filter_string=\"tags.mlflow.runName = 'Second dataset'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    \n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "    files = client.list_artifacts(first_dataset_latest_run_id, 'datasets')\n",
    "\n",
    "    dataframe = files[0].path\n",
    "\n",
    "    dataframe_path = client.download_artifacts(first_dataset_latest_run_id, dataframe)\n",
    "\n",
    "    df = pd.read_csv(dataframe_path)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectoraizer = TfidfVectorizer(lowercase=True,\n",
    "                                  analyzer = 'word',\n",
    "                                  max_features=5000,\n",
    "                                  ngram_range=(1, 4),\n",
    "                                  min_df=2,\n",
    "                                  max_df=0.9\n",
    "                                  )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST/VAL\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectoraizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectoraizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1_score(y_test, y_pred, average='weighted')}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "    joblib.dump(model, 'naive_bayes.pkl')\n",
    "    joblib.dump(vectoraizer, 'tfidfvectoraizer.pkl')\n",
    "    joblib.dump(encoder, 'labelencoder.pkl')\n",
    "    \n",
    "    mlflow.log_artifact('tfidfvectoraizer.pkl', 'models')\n",
    "    mlflow.log_artifact('labelencoder.pkl', 'models')\n",
    "    mlflow.log_artifact('naive_bayes.pkl', 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfbf25",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f598e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448ebe798ba94371abc1f9c5491c9de0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.24s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.83it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 24.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–º–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ loudly_crying_face\"\n",
      "['–í–ö–£–°_POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(experiment_ids=['0'],\n",
    "                         filter_string='attribute.run_name = \"second_model_experiment\"',\n",
    "                         order_by=['attribute.end_time desc'])\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "\n",
    "print(latest_run_model_id)\n",
    "\n",
    "model_run = client.get_run(latest_run_model_id)\n",
    "\n",
    "artifacts = client.list_artifacts(latest_run_model_id, 'models')\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–∏–º –º–æ–¥–µ–ª—è–º –∏–∑ MLFlow\n",
    "\n",
    "vectoraizer_file = client.download_artifacts(latest_run_model_id, 'models/tfidfvectoraizer.pkl')\n",
    "bayes_file = client.download_artifacts(latest_run_model_id, 'models/naive_bayes.pkl')\n",
    "encoder_file = client.download_artifacts(latest_run_model_id, 'models/labelencoder.pkl')\n",
    "\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ MLFlow\n",
    "\n",
    "\n",
    "latest_run_dataset = client.search_runs(experiment_ids=['0'],\n",
    "                                        filter_string='attributes.run_name=\"Second dataset\"',\n",
    "                                        order_by=['attributes.end_time desc'])\n",
    "\n",
    "\n",
    "latest_run_dataset_id = latest_run_dataset[0].info.run_id\n",
    "\n",
    "art_loc = client.download_artifacts(latest_run_dataset_id, 'functions/preprocess_text_second.pkl')\n",
    "\n",
    "\n",
    "# –ò–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö –≤ –≤–∏–¥–µ –≤—ã–ø–æ–ª–Ω—è–µ–º–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞\n",
    "\n",
    "vectoraizer = joblib.load(vectoraizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "text = '–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠'\n",
    "\n",
    "preprocessed_text = preprocess_func(text)\n",
    "\n",
    "done_text = vectoraizer.transform([preprocessed_text])\n",
    "    \n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(encoder.inverse_transform(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231b62e",
   "metadata": {},
   "source": [
    "### –¢—Ä–µ—Ç–∏–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d475c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>–£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2065</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2066</td>\n",
       "      <td>–ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>–ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               span  \\\n",
       "0              0                          –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   \n",
       "1              1  –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...   \n",
       "2              2  —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä...   \n",
       "3              3             —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞   \n",
       "4              4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –ù–ê–ú–ù–û–ì–û –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   \n",
       "...          ...                                                ...   \n",
       "2063        2063  ‚úÖ –ò–¢–û–ì\\n–ß–∏–ø—Å—ã –õ–µ–π—Å Flamin Hot \"–û—Å—Ç—Ä–∞—è –∫—Ä–µ–≤–µ—Ç–∫–∞...   \n",
       "2064        2064             –£ –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥   \n",
       "2065        2065                       –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å.   \n",
       "2066        2066                              –ö –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!   \n",
       "2067        2067  –ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ üåπ üåπ üåπ\\n–ß–∏–ø—Å—ã –õ–µ–π—Å \"–û–≥–Ω–µ–Ω...   \n",
       "\n",
       "               label  \n",
       "0      –í–ö–£–°_POSITIVE  \n",
       "1                  O  \n",
       "2                  O  \n",
       "3                  O  \n",
       "4      –í–ö–£–°_NEGATIVE  \n",
       "...              ...  \n",
       "2063   –í–ö–£–°_POSITIVE  \n",
       "2064  –ü–ê–ß–ö–ê_POSITIVE  \n",
       "2065   –í–ö–£–°_POSITIVE  \n",
       "2066   –í–ö–£–°_POSITIVE  \n",
       "2067               O  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score —Ä–∞–≤–µ–Ω 0.31762069535565496\n",
      "accuracy-score —Ä–∞–≤–µ–Ω 0.38164251207729466\n",
      "üèÉ View run third_model_experiment at: http://127.0.0.1:8080/#/experiments/0/runs/b8c2c44296e747ff82fc1d01712e92b3\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = 'third_model_experiment'):\n",
    "\n",
    "    mlflow.set_tag('NaiveBayes', '3.0.0')\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    dataset_runs = client.search_runs(\n",
    "        experiment_ids=['0'],  # Default —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç ID = 0\n",
    "        filter_string=\"tags.mlflow.runName = 'Third dataset'\",\n",
    "        order_by=['attributes.end_time desc']\n",
    "    )\n",
    "\n",
    "    first_dataset_latest_run = dataset_runs[0]\n",
    "    \n",
    "    first_dataset_latest_run_id = first_dataset_latest_run.info.run_id\n",
    "\n",
    "    files = client.list_artifacts(first_dataset_latest_run_id, 'datasets')\n",
    "\n",
    "    dataframe = files[0].path\n",
    "\n",
    "    dataframe_path = client.download_artifacts(first_dataset_latest_run_id, dataframe)\n",
    "\n",
    "    df = pd.read_csv(dataframe_path)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    vectoraizer = TfidfVectorizer(lowercase=True,\n",
    "                                  analyzer = 'word',\n",
    "                                  max_features=5000,\n",
    "                                  ngram_range=(1, 4),\n",
    "                                  min_df=2,\n",
    "                                  max_df=0.9\n",
    "                                  )\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê TRAIN/TEST/VAL\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "    X_train = df_train['span']\n",
    "    X_train = vectoraizer.fit_transform(X_train)\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "\n",
    "    X_test = df_test['span']\n",
    "    X_test = vectoraizer.transform(X_test)\n",
    "\n",
    "    y_test = df_test['label']\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –û–ë–£–ß–ï–ù–ò–ï –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f'f1-score —Ä–∞–≤–µ–Ω {f1_score(y_test, y_pred, average='weighted')}')\n",
    "    print(f'accuracy-score —Ä–∞–≤–µ–Ω {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "    joblib.dump(model, 'naive_bayes.pkl')\n",
    "    joblib.dump(vectoraizer, 'tfidfvectoraizer.pkl')\n",
    "    joblib.dump(encoder, 'labelencoder.pkl')\n",
    "    \n",
    "    mlflow.log_artifact('tfidfvectoraizer.pkl', 'models')\n",
    "    mlflow.log_artifact('labelencoder.pkl', 'models')\n",
    "    mlflow.log_artifact('naive_bayes.pkl', 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20a7ee",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b8c2c44296e747ff82fc1d01712e92b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠\"\n",
      "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"–º–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ loudly_crying_face\"\n",
      "['–í–ö–£–°_POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "\n",
    "latest_run_model = client.search_runs(experiment_ids=['0'],\n",
    "                         filter_string='attribute.run_name = \"third_model_experiment\"',\n",
    "                         order_by=['attribute.end_time desc'])\n",
    "\n",
    "latest_run_model_id = latest_run_model[0].info.run_id\n",
    "\n",
    "print(latest_run_model_id)\n",
    "\n",
    "model_run = client.get_run(latest_run_model_id)\n",
    "\n",
    "artifacts = client.list_artifacts(latest_run_model_id, 'models')\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–∏–º –º–æ–¥–µ–ª—è–º –∏–∑ MLFlow\n",
    "\n",
    "vectoraizer_file = client.download_artifacts(latest_run_model_id, 'models/tfidfvectoraizer.pkl')\n",
    "bayes_file = client.download_artifacts(latest_run_model_id, 'models/naive_bayes.pkl')\n",
    "encoder_file = client.download_artifacts(latest_run_model_id, 'models/labelencoder.pkl')\n",
    "\n",
    "\n",
    "# –ò–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö –≤ –≤–∏–¥–µ –≤—ã–ø–æ–ª–Ω—è–µ–º–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞\n",
    "\n",
    "vectoraizer = joblib.load(vectoraizer_file)\n",
    "bayes = joblib.load(bayes_file)\n",
    "encoder = joblib.load(encoder_file)\n",
    "\n",
    "with open(art_loc, 'rb') as f:\n",
    "    preprocess_func = cloudpickle.load(f)\n",
    "\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "text = '–ú–µ–Ω—è –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–ª–æ —Ç–∞–∫–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ üò≠'\n",
    "\n",
    "preprocessed_text = preprocess_func(text)\n",
    "\n",
    "done_text = vectoraizer.transform([preprocessed_text])\n",
    "    \n",
    "label = bayes.predict(done_text)\n",
    "\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{text}\"')\n",
    "print(f'–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \"{preprocessed_text}\"')\n",
    "print(encoder.inverse_transform(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205f87a",
   "metadata": {},
   "source": [
    "### –ò—Ç–æ–≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a20932c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>–í–µ—Ä—Å–∏—è</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>–û–ø–∏—Å–∞–Ω–∏–µ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 1</td>\n",
       "      <td>0.363707</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 2</td>\n",
       "      <td>0.344306</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>–û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–í–µ—Ä—Å–∏—è 3</td>\n",
       "      <td>0.344139</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     –í–µ—Ä—Å–∏—è  F1-Score  Accuracy                                      –û–ø–∏—Å–∞–Ω–∏–µ\n",
       "0  –í–µ—Ä—Å–∏—è 1  0.363707  0.413043      –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
       "1  –í–µ—Ä—Å–∏—è 2  0.344306  0.391304  –û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text\n",
       "2  –í–µ—Ä—Å–∏—è 3  0.344139  0.391304                               –ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    '–í–µ—Ä—Å–∏—è': ['–í–µ—Ä—Å–∏—è 1', '–í–µ—Ä—Å–∏—è 2', '–í–µ—Ä—Å–∏—è 3'],\n",
    "    'F1-Score': [0.363707, 0.344306, 0.344139],\n",
    "    'Accuracy': [0.413043, 0.391304, 0.391304],\n",
    "    '–û–ø–∏—Å–∞–Ω–∏–µ': ['–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏', '–û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ emoji to text', '–ù–∏–∫–∞–∫–æ–π –æ—á–∏—Å—Ç–∫–∏']\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84af634",
   "metadata": {},
   "source": [
    "## –í—Ç–æ—Ä–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2fe0c",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>—è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>–∏—Ç–æ–≥—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞—Å–∞...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>—É –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2065</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2066</td>\n",
       "      <td>–∫ –ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               span  \\\n",
       "0              0                              –≤–∫—É—Å —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   \n",
       "1              1  —è –µ—Å—Ç—å —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –≥–æ–¥ 2 –Ω–∞–∑–∞–¥ —Å–æ–≤–µ—Ç–æ–≤–∞—Ç...   \n",
       "2              2  —Ö–æ—Ç–µ—Ç—å—Å—è –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º—è —Å—Ç–∞—Ç—å –æ—á...   \n",
       "3              3                          –≤–∫—É—Å –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π 2 –ø–∞—á–∫–∞   \n",
       "4              4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä—ã–π –æ–±—ã—á–Ω—ã–π   \n",
       "...          ...                                                ...   \n",
       "2063        2063  –∏—Ç–æ–≥—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot –æ—Å—Ç—Ä—ã–π –∫—Ä–µ–≤–µ—Ç–∫–∞ –≤–∞—Å–∞...   \n",
       "2064        2064                   —É –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥   \n",
       "2065        2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å   \n",
       "2066        2066                            –∫ –ø–æ–∫—É–ø–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å   \n",
       "2067        2067  –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏–ø—Å—ã –ª–µ–π—Å –æ–≥–Ω–µ–Ω–Ω—ã–π —Ç–∞–∫–æ ...   \n",
       "\n",
       "               label  \n",
       "0      –í–ö–£–°_POSITIVE  \n",
       "1                  O  \n",
       "2                  O  \n",
       "3                  O  \n",
       "4      –í–ö–£–°_NEGATIVE  \n",
       "...              ...  \n",
       "2063   –í–ö–£–°_POSITIVE  \n",
       "2064  –ü–ê–ß–ö–ê_POSITIVE  \n",
       "2065   –í–ö–£–°_POSITIVE  \n",
       "2066   –í–ö–£–°_POSITIVE  \n",
       "2067               O  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/11/14 14:15:12 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential' has no defined input shape yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.2275 - loss: 2.1984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 206ms/step - accuracy: 0.2255 - loss: 2.1665 - val_accuracy: 0.2319 - val_loss: 2.0960\n",
      "Epoch 2/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.2287 - loss: 2.0806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 174ms/step - accuracy: 0.2733 - loss: 2.0092 - val_accuracy: 0.3382 - val_loss: 1.8667\n",
      "Epoch 3/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.3864 - loss: 1.7235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 150ms/step - accuracy: 0.4081 - loss: 1.6525 - val_accuracy: 0.4444 - val_loss: 1.6026\n",
      "Epoch 4/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.5375 - loss: 1.4016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.5435 - loss: 1.3561 - val_accuracy: 0.4638 - val_loss: 1.4836\n",
      "Epoch 5/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.6509 - loss: 1.0705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 181ms/step - accuracy: 0.6312 - loss: 1.0949 - val_accuracy: 0.4734 - val_loss: 1.4338\n",
      "Epoch 6/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 153ms/step - accuracy: 0.7019 - loss: 0.8826 - val_accuracy: 0.4831 - val_loss: 1.4537\n",
      "Epoch 7/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 160ms/step - accuracy: 0.7485 - loss: 0.7422 - val_accuracy: 0.4928 - val_loss: 1.4907\n",
      "Epoch 8/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 171ms/step - accuracy: 0.7987 - loss: 0.6143 - val_accuracy: 0.5121 - val_loss: 1.5313\n",
      "Epoch 9/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step - accuracy: 0.8404 - loss: 0.5175 - val_accuracy: 0.4976 - val_loss: 1.6287\n",
      "Epoch 10/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - accuracy: 0.8688 - loss: 0.4360 - val_accuracy: 0.4831 - val_loss: 1.6938\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/14 14:18:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/14 14:19:28 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Smart\\AppData\\Local\\Temp\\tmpbyukzg5w\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.20.0', 'cloudpickle==3.1.2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.4396 - loss: 2.0402\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m128\u001b[0m)        ‚îÇ       \u001b[38;5;34m256,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ        \u001b[38;5;34m98,816\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ         \u001b[38;5;34m1,290\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,068,320</span> (4.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,068,320\u001b[0m (4.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,106</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,106\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">712,214</span> (2.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m712,214\u001b[0m (2.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step\n",
      "f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.4556642959166068\n",
      "üèÉ View run first_experiment_neural_network at: http://127.0.0.1:8080/#/experiments/0/runs/bb16c224144d4b47a34f1365031d3cf4\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='first_experiment_neural_network'):\n",
    "    \n",
    "    mlflow.set_tag('LSTM', '1.0.0')\n",
    "\n",
    "    \n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    first_dataset_run = client.search_runs(experiment_ids=['0'],\n",
    "                                           filter_string = \"attributes.run_name='First dataset'\",\n",
    "                                           order_by = ['attributes.end_time desc'])\n",
    "    \n",
    "    first_dataset_run_latest = first_dataset_run[0]\n",
    "    \n",
    "    first_dataset_run_id = first_dataset_run_latest.info.run_id\n",
    "    art = client.download_artifacts(first_dataset_run_id, 'datasets/First_version.csv')\n",
    "\n",
    "    df = pd.read_csv(art)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = 2000, \n",
    "                          oov_token = '<OOV>',\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True,\n",
    "                          split=' ',\n",
    "                          char_level=False)\n",
    "\n",
    "    tokenizer.fit_on_texts(df['span'])\n",
    "\n",
    "\n",
    "    df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "    df_test, df_val = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp['label'])\n",
    "    \n",
    "\n",
    "    X_train_vec = tokenizer.texts_to_sequences(df_train['span'])\n",
    "    X_test_vec = tokenizer.texts_to_sequences(df_test['span'])\n",
    "    X_val_vec = tokenizer.texts_to_sequences(df_val['span'])\n",
    "\n",
    "\n",
    "    max_len_text = 0\n",
    "\n",
    "    for i in df['span']:\n",
    "        max_len_text = max(max_len_text, len(i.split(' ')))\n",
    "\n",
    "    print(max_len_text)\n",
    "\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_vec, max_len_text, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_vec, max_len_text, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_vec, max_len_text, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    y_train = encoder.fit_transform(df_train['label'])\n",
    "    y_test = encoder.transform(df_test['label'])\n",
    "    y_val = encoder.transform(df_val['label'])\n",
    "\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                              –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim = 2000, output_dim = 128, input_length = max_len_text))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.3)))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(X_train_pad, y_train, batch_size = 16, epochs = 10, validation_data=(X_val_pad, y_val))\n",
    "\n",
    "    model.evaluate(X_test_pad, y_test)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    pred_proba = model.predict(X_test_pad)\n",
    "\n",
    "    pred_class = np.argmax(pred_proba, axis = 1)\n",
    "    \n",
    "    print(f'f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {f1_score(pred_class, y_test, average = 'weighted')}')\n",
    "\n",
    "    mlflow.log_metric('f1', f1_score(pred_class, y_test, average = 'weighted'))\n",
    "\n",
    "    model.save('LSTM_ver_1.keras')\n",
    "\n",
    "    with open('tokenizer.pkl', 'wb') as f:\n",
    "        cloudpickle.dump(tokenizer, f)\n",
    "\n",
    "    \n",
    "    with open('encoder.pkl', 'wb') as f:\n",
    "        cloudpickle.dump(encoder, f)\n",
    "\n",
    "\n",
    "    mlflow.log_artifact('LSTM_ver_1.keras', 'models')\n",
    "    mlflow.log_artifact('tokenizer.pkl', 'models') \n",
    "    mlflow.log_artifact('encoder.pkl', 'models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ff08f",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.73s/it]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.56it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.31it/s]\n",
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 11 variables whereas the saved optimizer has 20 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987ms/step\n",
      "['O']\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "last_model_runs = client.search_runs(experiment_ids=['0'],\n",
    "                                    filter_string=\"attributes.run_name = 'first_experiment_neural_network'\",\n",
    "                                    order_by=['attributes.end_time desc'])\n",
    "\n",
    "\n",
    "last_model_run_id = last_model_runs[0].info.run_id\n",
    "\n",
    "nn_loc = client.download_artifacts(last_model_run_id, 'models/LSTM_ver_1.keras')\n",
    "encoder_loc = client.download_artifacts(last_model_run_id, 'models/encoder.pkl')\n",
    "tokenizer_loc = client.download_artifacts(last_model_run_id, 'models/tokenizer.pkl')\n",
    "\n",
    "model_keras = keras.models.load_model(nn_loc)\n",
    "encoder = joblib.load(encoder_loc)\n",
    "tokenizer = joblib.load(tokenizer_loc)\n",
    "\n",
    "\n",
    "# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏\n",
    "\n",
    "text = '–í —Ü–µ–ª–æ–º, —á–∏–ø—Å—ã –ê—à–∞–Ω –ö—Ä–∞—Å–Ω–∞—è –ø—Ç–∏—Ü–∞ –ë–∞—Ä–±–µ–∫—é –≤–ø–æ–ª–Ω–µ —Å—ä–µ–¥–æ–±–Ω—ã–µ'\n",
    "\n",
    "tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "max_len = model_keras.input_shape[1]\n",
    "padded_text = pad_sequences(tokenized_text, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "pred = model_keras.predict(padded_text)\n",
    "\n",
    "predicted_class_ind = np.argmax(pred, axis=1)\n",
    "predicted_class = encoder.inverse_transform(predicted_class_ind)\n",
    "\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d0476",
   "metadata": {},
   "source": [
    "### –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>—è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>—Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>—Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ</td>\n",
       "      <td>–í–ö–£–°_NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>—É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥</td>\n",
       "      <td>–ü–ê–ß–ö–ê_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2065</td>\n",
       "      <td>–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2066</td>\n",
       "      <td>–∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é</td>\n",
       "      <td>–í–ö–£–°_POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               span  \\\n",
       "0              0                          –≤–∫—É—Å –±—ã–ª —Ä–µ–∞–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã–π   \n",
       "1              1  —è –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å–æ...   \n",
       "2              2  —Ö–æ—Ç–µ–ª–æ—Å—å –∫—É–ø–∏—Ç—å –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ...   \n",
       "3              3             —Å–µ–π—á–∞—Å —Ç–∞–∫–æ–π –≤–∫—É—Å –∏–º–µ–µ—Ç –∫–∞–∂–¥–∞—è 2 –ø–∞—á–∫–∞   \n",
       "4              4          –≤–∫—É—Å –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–∞–ª –Ω–∞–º–Ω–æ–≥–æ –æ—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–≥–æ   \n",
       "...          ...                                                ...   \n",
       "2063        2063  check_mark_button –∏—Ç–æ–≥ n—á–∏–ø—Å—ã –ª–µ–π—Å flamin hot ...   \n",
       "2064        2064             —É –Ω–∏—Ö –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∏ –∞–ø–ø–µ—Ç–∏—Ç–Ω—ã–π –≤–∏–¥   \n",
       "2065        2065                        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∏–∫–∞–Ω—Ç–Ω—ã–π –≤–∫—É—Å   \n",
       "2066        2066                               –∫ –ø–æ–∫—É–ø–∫–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é   \n",
       "2067        2067  –±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ rose rose rose n—á–∏–ø—Å—ã –ª–µ...   \n",
       "\n",
       "               label  \n",
       "0      –í–ö–£–°_POSITIVE  \n",
       "1                  O  \n",
       "2                  O  \n",
       "3                  O  \n",
       "4      –í–ö–£–°_NEGATIVE  \n",
       "...              ...  \n",
       "2063   –í–ö–£–°_POSITIVE  \n",
       "2064  –ü–ê–ß–ö–ê_POSITIVE  \n",
       "2065   –í–ö–£–°_POSITIVE  \n",
       "2066   –í–ö–£–°_POSITIVE  \n",
       "2067               O  \n",
       "\n",
       "[2068 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/11/14 14:19:47 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential_1' has no defined input shape yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m100/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1403 - loss: 2.2957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.1808 - loss: 2.2741 - val_accuracy: 0.2657 - val_loss: 2.2133\n",
      "Epoch 2/40\n",
      "\u001b[1m 99/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2390 - loss: 2.1599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2527 - loss: 2.1250 - val_accuracy: 0.2995 - val_loss: 2.0611\n",
      "Epoch 3/40\n",
      "\u001b[1m101/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3188 - loss: 1.9072"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.3295 - loss: 1.8683 - val_accuracy: 0.3623 - val_loss: 1.9236\n",
      "Epoch 4/40\n",
      "\u001b[1m100/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3909 - loss: 1.6462"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.3888 - loss: 1.6180 - val_accuracy: 0.3671 - val_loss: 1.8543\n",
      "Epoch 5/40\n",
      "\u001b[1m 99/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4735 - loss: 1.3830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.4680 - loss: 1.3847 - val_accuracy: 0.3913 - val_loss: 1.8164\n",
      "Epoch 6/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5520 - loss: 1.2025 - val_accuracy: 0.3865 - val_loss: 1.8280\n",
      "Epoch 7/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6070 - loss: 1.0507 - val_accuracy: 0.3382 - val_loss: 1.8678\n",
      "Epoch 8/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6469 - loss: 0.9636 - val_accuracy: 0.3430 - val_loss: 1.9461\n",
      "Epoch 9/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6892 - loss: 0.8456 - val_accuracy: 0.3092 - val_loss: 2.0117\n",
      "Epoch 10/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7279 - loss: 0.7700 - val_accuracy: 0.3188 - val_loss: 2.0898\n",
      "Epoch 11/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7745 - loss: 0.6687 - val_accuracy: 0.3285 - val_loss: 2.1958\n",
      "Epoch 12/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7944 - loss: 0.6087 - val_accuracy: 0.3382 - val_loss: 2.3151\n",
      "Epoch 13/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8083 - loss: 0.5391 - val_accuracy: 0.3043 - val_loss: 2.4450\n",
      "Epoch 14/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8368 - loss: 0.4954 - val_accuracy: 0.3430 - val_loss: 2.5048\n",
      "Epoch 15/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8543 - loss: 0.4219 - val_accuracy: 0.3188 - val_loss: 2.5906\n",
      "Epoch 16/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8730 - loss: 0.3824 - val_accuracy: 0.3575 - val_loss: 2.6669\n",
      "Epoch 17/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8791 - loss: 0.3593 - val_accuracy: 0.3527 - val_loss: 2.7960\n",
      "Epoch 18/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8845 - loss: 0.3598 - val_accuracy: 0.3527 - val_loss: 2.8071\n",
      "Epoch 19/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8936 - loss: 0.3085 - val_accuracy: 0.3478 - val_loss: 2.9213\n",
      "Epoch 20/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8996 - loss: 0.2955 - val_accuracy: 0.3430 - val_loss: 3.0270\n",
      "Epoch 21/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9117 - loss: 0.2739 - val_accuracy: 0.3237 - val_loss: 3.1631\n",
      "Epoch 22/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9123 - loss: 0.2704 - val_accuracy: 0.3478 - val_loss: 3.2169\n",
      "Epoch 23/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9178 - loss: 0.2470 - val_accuracy: 0.3478 - val_loss: 3.2923\n",
      "Epoch 24/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9244 - loss: 0.2386 - val_accuracy: 0.3478 - val_loss: 3.4059\n",
      "Epoch 25/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9287 - loss: 0.2144 - val_accuracy: 0.3720 - val_loss: 3.4226\n",
      "Epoch 26/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9202 - loss: 0.2389 - val_accuracy: 0.3671 - val_loss: 3.5264\n",
      "Epoch 27/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9268 - loss: 0.2200 - val_accuracy: 0.3478 - val_loss: 3.6170\n",
      "Epoch 28/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9347 - loss: 0.2088 - val_accuracy: 0.3671 - val_loss: 3.6800\n",
      "Epoch 29/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9299 - loss: 0.2053 - val_accuracy: 0.3575 - val_loss: 3.7965\n",
      "Epoch 30/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9432 - loss: 0.1819 - val_accuracy: 0.3430 - val_loss: 3.8857\n",
      "Epoch 31/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9498 - loss: 0.1582 - val_accuracy: 0.3575 - val_loss: 3.9097\n",
      "Epoch 32/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9335 - loss: 0.1963 - val_accuracy: 0.3478 - val_loss: 3.9976\n",
      "Epoch 33/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9522 - loss: 0.1492 - val_accuracy: 0.3527 - val_loss: 4.1059\n",
      "Epoch 34/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9395 - loss: 0.1732 - val_accuracy: 0.3768 - val_loss: 4.0556\n",
      "Epoch 35/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9510 - loss: 0.1427 - val_accuracy: 0.3623 - val_loss: 4.2409\n",
      "Epoch 36/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9414 - loss: 0.1581 - val_accuracy: 0.3768 - val_loss: 4.1708\n",
      "Epoch 37/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9444 - loss: 0.1649 - val_accuracy: 0.3768 - val_loss: 4.2276\n",
      "Epoch 38/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9528 - loss: 0.1526 - val_accuracy: 0.3527 - val_loss: 4.3643\n",
      "Epoch 39/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9498 - loss: 0.1495 - val_accuracy: 0.3623 - val_loss: 4.3726\n",
      "Epoch 40/40\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9547 - loss: 0.1420 - val_accuracy: 0.3768 - val_loss: 4.5574\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/14 14:20:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/14 14:21:09 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Smart\\AppData\\Local\\Temp\\tmpsovyssop\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.20.0', 'cloudpickle==3.1.2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">128,000</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_max_pooling1d            ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ       \u001b[38;5;34m128,000\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)         ‚îÇ         \u001b[38;5;34m6,176\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)         ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_max_pooling1d            ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ           \u001b[38;5;34m528\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ           \u001b[38;5;34m170\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">404,624</span> (1.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m404,624\u001b[0m (1.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,874</span> (526.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m134,874\u001b[0m (526.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,750</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m269,750\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3575 - loss: 4.1014 \n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000218F239F600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000218F239F600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω 0.352281074230904\n",
      "üèÉ View run second_experiment_neural_network at: http://127.0.0.1:8080/#/experiments/0/runs/96e203d967044ad4ac9f0267b29b3ccb\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='second_experiment_neural_network'):\n",
    "    \n",
    "    mlflow.set_tag('LSTM', '2.0.0')\n",
    "\n",
    "    \n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    first_dataset_run = client.search_runs(experiment_ids=['0'],\n",
    "                                           filter_string = \"attributes.run_name='Second dataset'\",\n",
    "                                           order_by = ['attributes.end_time desc'])\n",
    "    \n",
    "    first_dataset_run_latest = first_dataset_run[0]\n",
    "    \n",
    "    first_dataset_run_id = first_dataset_run_latest.info.run_id\n",
    "    art = client.download_artifacts(first_dataset_run_id, 'datasets/Second_version.csv')\n",
    "\n",
    "    df = pd.read_csv(art)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "        \n",
    "    # =====================================================================================================================================\n",
    "    #                                         –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = 2000, \n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True,\n",
    "                          split=' ',\n",
    "                          char_level=False,\n",
    "                          oov_token=None,\n",
    "                          analyzer=None)\n",
    "    \n",
    "    tokenizer.fit_on_texts(df['span'])\n",
    "\n",
    "    df_train, df_temp = train_test_split(df, test_size = 0.2, random_state = 42)\n",
    "    df_test, df_val = train_test_split(df_temp, test_size=0.5, random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "    X_train_vec = tokenizer.texts_to_sequences(df_train['span'])\n",
    "    X_test_vec = tokenizer.texts_to_sequences(df_test['span'])\n",
    "    X_val_vec = tokenizer.texts_to_sequences(df_val['span'])\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_vec, maxlen=100, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_vec, maxlen=100, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_vec, maxlen=100, padding='post', truncating='post')\n",
    "    \n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    y_train = encoder.fit_transform(df_train['label'])\n",
    "    y_test = encoder.transform(df_test['label'])\n",
    "    y_val = encoder.transform(df_val['label'])\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim = 2000, output_dim = 64, input_length = 100))\n",
    "\n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(Dense(16, activation = 'relu'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(X_train_pad, y_train, batch_size=16, epochs=40, validation_data = (X_val_pad, y_val))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.evaluate(X_test_pad, y_test)\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "\n",
    "    pred = model.predict(X_test_pad)\n",
    "\n",
    "    pred_class = np.argmax(pred, axis=1)\n",
    "\n",
    "    print(f'f1-score —É –º–æ–¥–µ–ª–∏ —Ä–∞–≤–µ–Ω {f1_score(pred_class, y_test, average = 'weighted')}')\n",
    "\n",
    "    mlflow.log_metric('f1', f1_score(pred_class, y_test, average = 'weighted'))\n",
    "\n",
    "    model.save('LSTM_ver_2.keras')\n",
    "\n",
    "    joblib.dump(tokenizer, 'tokenizer.pkl')\n",
    "    joblib.dump(encoder, 'encoder.pkl')\n",
    "\n",
    "    mlflow.log_artifact('tokenizer.pkl', 'models')\n",
    "    mlflow.log_artifact('encoder.pkl', 'models')\n",
    "    mlflow.log_artifact('LSTM_ver_2.keras', 'models')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e2918",
   "metadata": {},
   "source": [
    "## –¢—Ä–µ—Ç–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74382c43",
   "metadata": {},
   "source": [
    "### –ü–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78383437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1654/1654 [00:00<00:00, 1884.99 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207/207 [00:00<00:00, 2099.89 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207/207 [00:00<00:00, 2401.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: ['span', 'labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Val features: ['span', 'labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='414' max='414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [414/414 01:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.960500</td>\n",
       "      <td>1.936422</td>\n",
       "      <td>0.230187</td>\n",
       "      <td>0.338164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smart\\PycharmProjects\\NLP_and_MLOPS_course_work\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run transformers_experiment_1 at: http://127.0.0.1:8080/#/experiments/0/runs/15a40ee889404e4180950782979c694b\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='transformers_experiment_1'):\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –°–ß–ò–¢–´–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "    \n",
    "    first_dataset_runs = client.search_runs(experiment_ids=['0'],\n",
    "                             filter_string=\"attributes.run_name = 'First dataset'\",\n",
    "                             order_by=['attributes.end_time desc'])\n",
    "    \n",
    "    need_run = first_dataset_runs[0]\n",
    "    need_run_id = need_run.info.run_id\n",
    "    art_loc = client.download_artifacts(need_run_id, 'datasets/First_version.csv')\n",
    "    df = pd.read_csv(art_loc)\n",
    "    df = df[['span', 'label']]\n",
    "    df = df.rename(columns={\"label\": \"labels\"})\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê/–¢–ê–†–ì–ï–¢–û–í\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "    \n",
    "    df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42, stratify=df['labels'])\n",
    "    df_test, df_val = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp['labels'])\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(df_train['labels'])\n",
    "    y_test = encoder.transform(df_test['labels'])\n",
    "    y_val = encoder.transform(df_val['labels'])\n",
    "\n",
    "\n",
    "    # =====================================================================================================================================\n",
    "    #                                         –ü–û–î–ì–û–¢–û–í–ö–ê –ú–û–î–ï–õ–ò\n",
    "    # =====================================================================================================================================\n",
    "    \n",
    "\n",
    "    mlflow.transformers.autolog()\n",
    "\n",
    "    model_name = 'cointegrated/rubert-tiny2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=len(encoder.classes_), \n",
    "        id2label={i: label for i, label in enumerate(encoder.classes_)},\n",
    "        label2id={label: i for i, label in enumerate(encoder.classes_)}\n",
    "    )\n",
    "    \n",
    "    dataset_train = Dataset.from_pandas(df_train.assign(labels=y_train))    \n",
    "    dataset_test = Dataset.from_pandas(df_test.assign(labels=y_test))    \n",
    "    dataset_val = Dataset.from_pandas(df_val.assign(labels=y_val))\n",
    "\n",
    "    def tokenize_dataset(row):\n",
    "        return tokenizer(\n",
    "            row['span'],\n",
    "            truncation=True, \n",
    "            padding=False,\n",
    "            max_length=512,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "    dataset_tokenized_train = dataset_train.map(tokenize_dataset, batched=False)\n",
    "    dataset_tokenized_test = dataset_test.map(tokenize_dataset, batched=False)\n",
    "    dataset_tokenized_val = dataset_val.map(tokenize_dataset, batched=False)\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./models/',\n",
    "        overwrite_output_dir=True,\n",
    "        logging_dir='./logs/',\n",
    "        num_train_epochs=50,\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=4,   \n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type='cosine',\n",
    "        metric_for_best_model='f1-score',\n",
    "        weight_decay=0.2,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        max_grad_norm=1.0,\n",
    "        logging_steps=100\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return {\n",
    "            'f1-score': f1_score(labels, predictions, average='weighted'),\n",
    "            'accuracy': accuracy_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=dataset_tokenized_train,\n",
    "        eval_dataset=dataset_tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    final_metrics = trainer.evaluate(dataset_tokenized_test)\n",
    "    \n",
    "    model_dir = \"./Transformers_ver_1\"\n",
    "    tokenizer_dir = \"./Tokenizer_transformers_ver_1\"\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "    mlflow.log_artifacts(model_dir, \"model\")\n",
    "    mlflow.log_artifacts(tokenizer_dir, \"tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
