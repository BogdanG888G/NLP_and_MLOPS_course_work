{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714fbb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 22:32:04,583 - INFO - –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–æ–≤...\n",
      "2025-10-18 22:32:04,586 - INFO - –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã 1: https://irecommend.ru/catalog/reviews/939-13393\n",
      "2025-10-18 22:32:07,450 - WARNING - –°—Ç–∞—Ç—É—Å –∫–æ–¥ 521 –¥–ª—è https://irecommend.ru/catalog/reviews/939-13393\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 341\u001b[39m\n\u001b[32m    338\u001b[39m         logging.error(\u001b[33m\"\u001b[39m\u001b[33m–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –æ—Ç–∑—ã–≤—ã\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 326\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    323\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33m–ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–æ–≤...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# –°–æ–±–∏—Ä–∞–µ–º –æ—Ç–∑—ã–≤—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å 3 —Å—Ç—Ä–∞–Ω–∏—Ü)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m reviews = \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscrape_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTART_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reviews:\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV\u001b[39;00m\n\u001b[32m    330\u001b[39m     filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreviews_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 252\u001b[39m, in \u001b[36mReviewParser.scrape_reviews\u001b[39m\u001b[34m(self, start_url, pages)\u001b[39m\n\u001b[32m    248\u001b[39m     url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m html:\n\u001b[32m    254\u001b[39m     logging.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mReviewParser.get_page\u001b[39m\u001b[34m(self, url, retries, delay)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m         response = \u001b[38;5;28mself\u001b[39m.session.get(url, timeout=\u001b[32m15\u001b[39m)\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     46\u001b[39m             \u001b[38;5;66;03m# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å –æ—Ç–∑—ã–≤–∞–º–∏\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import logging\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('review_parser.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class ReviewParser:\n",
    "    def __init__(self, base_url=\"https://irecommend.ru\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.setup_session()\n",
    "        \n",
    "    def setup_session(self):\n",
    "        \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏\"\"\"\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Referer': 'https://irecommend.ru/',\n",
    "        })\n",
    "\n",
    "    def get_page(self, url, retries=3, delay=2):\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                time.sleep(random.uniform(delay, delay * 2))\n",
    "                \n",
    "                response = self.session.get(url, timeout=15)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å –æ—Ç–∑—ã–≤–∞–º–∏\n",
    "                    if 'smTeaser' in response.text or 'reviewBlock' in response.text:\n",
    "                        return response.text\n",
    "                    else:\n",
    "                        logging.warning(f\"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {url} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–∑—ã–≤–æ–≤\")\n",
    "                        return None\n",
    "                else:\n",
    "                    logging.warning(f\"–°—Ç–∞—Ç—É—Å –∫–æ–¥ {response.status_code} –¥–ª—è {url}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}\")\n",
    "            \n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay * (attempt + 1))\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_reviews_from_list(self, html):\n",
    "        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        reviews_data = []\n",
    "        \n",
    "        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –±–ª–æ–∫–∏ —Å –æ—Ç–∑—ã–≤–∞–º–∏\n",
    "        review_blocks = soup.find_all('div', class_='smTeaser')\n",
    "        \n",
    "        for block in review_blocks:\n",
    "            review_data = self.parse_review_preview(block)\n",
    "            if review_data:\n",
    "                reviews_data.append(review_data)\n",
    "        \n",
    "        logging.info(f\"–ù–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(reviews_data)}\")\n",
    "        return reviews_data\n",
    "\n",
    "    def parse_review_preview(self, block):\n",
    "        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ–≤—å—é –æ—Ç–∑—ã–≤–∞ –∏–∑ —Å–ø–∏—Å–∫–∞\"\"\"\n",
    "        try:\n",
    "            # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "            product_elem = block.find('div', class_='productName')\n",
    "            product_name = product_elem.get_text(strip=True) if product_elem else \"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç\"\n",
    "            \n",
    "            author_elem = block.find('div', class_='authorName')\n",
    "            author_name = author_elem.get_text(strip=True) if author_elem else \"–ê–Ω–æ–Ω–∏–º\"\n",
    "            \n",
    "            # –†–µ–π—Ç–∏–Ω–≥\n",
    "            rating = self.extract_rating(block)\n",
    "            \n",
    "            # –î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è\n",
    "            date_elem = block.find('span', class_='date-created')\n",
    "            time_elem = block.find('span', class_='time-created')\n",
    "            date_created = date_elem.get_text(strip=True) if date_elem else \"\"\n",
    "            time_created = time_elem.get_text(strip=True) if time_elem else \"\"\n",
    "            \n",
    "            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Ç–µ–∫—Å—Ç –ø—Ä–µ–≤—å—é\n",
    "            title_elem = block.find('div', class_='reviewTitle')\n",
    "            title = title_elem.get_text(strip=True) if title_elem else \"\"\n",
    "            \n",
    "            teaser_elem = block.find('span', class_='reviewTeaserText')\n",
    "            teaser_text = teaser_elem.get_text(strip=True) if teaser_elem else \"\"\n",
    "            \n",
    "            # –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–ª–Ω—ã–π –æ—Ç–∑—ã–≤\n",
    "            link_elem = block.find('a', class_='reviewTextSnippet')\n",
    "            review_url = urljoin(self.base_url, link_elem['href']) if link_elem and link_elem.get('href') else \"\"\n",
    "            \n",
    "            return {\n",
    "                'product_name': product_name,\n",
    "                'author': author_name,\n",
    "                'rating': rating,\n",
    "                'date_created': date_created,\n",
    "                'time_created': time_created,\n",
    "                'title': title,\n",
    "                'teaser_text': teaser_text,\n",
    "                'review_url': review_url,\n",
    "                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –ø—Ä–µ–≤—å—é –æ—Ç–∑—ã–≤–∞: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_full_review(self, url):\n",
    "        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –æ—Ç–∑—ã–≤–∞\"\"\"\n",
    "        html = self.get_page(url)\n",
    "        if not html:\n",
    "            return None\n",
    "            \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        review_block = soup.find('div', class_='reviewBlock')\n",
    "        \n",
    "        if not review_block:\n",
    "            logging.warning(f\"–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ –æ—Ç–∑—ã–≤–∞ –¥–ª—è {url}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞\n",
    "            review_body = review_block.find('div', itemprop='reviewBody')\n",
    "            full_text = self.clean_text(review_body) if review_body else \"\"\n",
    "            \n",
    "            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "            experience = self.extract_experience(review_block)\n",
    "            pluses = self.extract_pluses(review_block)\n",
    "            minuses = self.extract_minuses(review_block)\n",
    "            verdict = self.extract_verdict(review_block)\n",
    "            \n",
    "            # –†–µ–π—Ç–∏–Ω–≥ –∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–∞\n",
    "            rating_meta = review_block.find('meta', itemprop='ratingValue')\n",
    "            rating = rating_meta['content'] if rating_meta else \"\"\n",
    "            \n",
    "            return {\n",
    "                'full_text': full_text,\n",
    "                'experience': experience,\n",
    "                'pluses': ' | '.join(pluses),\n",
    "                'minuses': ' | '.join(minuses),\n",
    "                'verdict': verdict,\n",
    "                'detailed_rating': rating\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_rating(self, block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∞ –∏–∑ –∑–≤–µ–∑–¥\"\"\"\n",
    "        try:\n",
    "            rating_elem = block.find('div', class_='starsRating')\n",
    "            if rating_elem:\n",
    "                # –ò—â–µ–º –∫–ª–∞—Å—Å —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º\n",
    "                for cls in rating_elem.get('class', []):\n",
    "                    if 'fivestarWidgetStatic-' in cls:\n",
    "                        return cls.split('-')[-1]\n",
    "                \n",
    "                # –°—á–∏—Ç–∞–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–≤–µ–∑–¥—ã\n",
    "                stars = rating_elem.find_all('div', class_='star')\n",
    "                filled = sum(1 for star in stars if star.find('div', class_='on'))\n",
    "                return str(filled)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ä–µ–π—Ç–∏–Ω–≥–∞: {e}\")\n",
    "        \n",
    "        return \"0\"\n",
    "\n",
    "    def extract_experience(self, review_block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\"\"\"\n",
    "        try:\n",
    "            experience_elem = review_block.find('div', class_='item-data')\n",
    "            return experience_elem.get_text(strip=True) if experience_elem else \"\"\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def extract_pluses(self, review_block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤\"\"\"\n",
    "        try:\n",
    "            plus_block = review_block.find('div', class_='plus')\n",
    "            if plus_block:\n",
    "                plus_items = plus_block.find_all('li')\n",
    "                return [item.get_text(strip=True) for item in plus_items]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    def extract_minuses(self, review_block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤\"\"\"\n",
    "        try:\n",
    "            minus_block = review_block.find('div', class_='minus')\n",
    "            if minus_block:\n",
    "                minus_items = minus_block.find_all('li')\n",
    "                return [item.get_text(strip=True) for item in minus_items]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    def extract_verdict(self, review_block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ—Ä–¥–∏–∫—Ç–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç/–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç)\"\"\"\n",
    "        try:\n",
    "            verdict_elem = review_block.find('span', class_='verdict')\n",
    "            return verdict_elem.get_text(strip=True) if verdict_elem else \"\"\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def clean_text(self, element):\n",
    "        \"\"\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç HTML —Ç–µ–≥–æ–≤\"\"\"\n",
    "        if not element:\n",
    "            return \"\"\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫\n",
    "        for br in element.find_all(\"br\"):\n",
    "            br.replace_with(\"\\n\")\n",
    "        \n",
    "        text = element.get_text(separator='\\n')\n",
    "        \n",
    "        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "        lines = [line.strip() for line in text.split('\\n')]\n",
    "        lines = [line for line in lines if line]\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def scrape_reviews(self, start_url, pages=5):\n",
    "        \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–±–æ—Ä–∞ –æ—Ç–∑—ã–≤–æ–≤\"\"\"\n",
    "        all_reviews = []\n",
    "        \n",
    "        for page in range(pages):\n",
    "            if page == 0:\n",
    "                url = start_url\n",
    "            else:\n",
    "                url = f\"{start_url}?page={page}\"\n",
    "            \n",
    "            logging.info(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page + 1}: {url}\")\n",
    "            \n",
    "            html = self.get_page(url)\n",
    "            if not html:\n",
    "                logging.warning(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page + 1}\")\n",
    "                continue\n",
    "            \n",
    "            # –ü–∞—Ä—Å–∏–º –æ—Ç–∑—ã–≤—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞\n",
    "            previews = self.parse_reviews_from_list(html)\n",
    "            \n",
    "            for i, preview in enumerate(previews, 1):\n",
    "                logging.info(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–∞ {i}/{len(previews)}: {preview['title'][:30]}...\")\n",
    "                \n",
    "                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞\n",
    "                full_data = self.parse_full_review(preview['review_url'])\n",
    "                \n",
    "                if full_data:\n",
    "                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "                    complete_review = {**preview, **full_data}\n",
    "                    all_reviews.append(complete_review)\n",
    "                else:\n",
    "                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–æ—Ç—è –±—ã –ø—Ä–µ–≤—å—é\n",
    "                    preview['full_text'] = preview.get('teaser_text', '')\n",
    "                    all_reviews.append(preview)\n",
    "                \n",
    "                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            logging.info(f\"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞. –í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {len(all_reviews)}\")\n",
    "            \n",
    "            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏\n",
    "            if page < pages - 1:\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "        \n",
    "        return all_reviews\n",
    "\n",
    "    def save_to_csv(self, reviews, filename):\n",
    "        \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –≤ CSV —Ñ–∞–π–ª\"\"\"\n",
    "        if not reviews:\n",
    "            logging.error(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–ª—è\n",
    "            fieldnames = [\n",
    "                'product_name', 'author', 'rating', 'date_created', 'time_created',\n",
    "                'title', 'teaser_text', 'full_text', 'experience', 'pluses', \n",
    "                'minuses', 'verdict', 'detailed_rating', 'review_url', 'scraped_at'\n",
    "            ]\n",
    "            \n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                \n",
    "                for review in reviews:\n",
    "                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è\n",
    "                    row = {field: review.get(field, '') for field in fieldnames}\n",
    "                    writer.writerow(row)\n",
    "            \n",
    "            logging.info(f\"–£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –≤ {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "def main():\n",
    "    # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ—Ç–∑—ã–≤–∞–º–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π)\n",
    "    START_URL = \"https://irecommend.ru/catalog/reviews/939-13393\"\n",
    "    \n",
    "    parser = ReviewParser()\n",
    "    \n",
    "    logging.info(\"–ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–æ–≤...\")\n",
    "    \n",
    "    # –°–æ–±–∏—Ä–∞–µ–º –æ—Ç–∑—ã–≤—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å 3 —Å—Ç—Ä–∞–Ω–∏—Ü)\n",
    "    reviews = parser.scrape_reviews(START_URL, pages=1)\n",
    "    \n",
    "    if reviews:\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV\n",
    "        filename = f\"reviews_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "        parser.save_to_csv(reviews, filename)\n",
    "        \n",
    "        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "        logging.info(f\"–°–æ–±—Ä–∞–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {len(reviews)}\")\n",
    "        logging.info(f\"–ü–µ—Ä–≤—ã–π –æ—Ç–∑—ã–≤: {reviews[0]['title']}\")\n",
    "        logging.info(f\"–¢–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {reviews[0]['full_text'][:100]}...\")\n",
    "    else:\n",
    "        logging.error(\"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –æ—Ç–∑—ã–≤—ã\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02faf9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcec226e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>experience</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>verdict</th>\n",
       "      <th>detailed_rating</th>\n",
       "      <th>review_url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Twister –ö–æ–ª–±–∞—Å–∫–∏ –≥—Ä–∏–ª—å —Å –≥–æ...</td>\n",
       "      <td>–°–∞–Ω–¥—É –ú–∞–¥–∞–Ω</td>\n",
       "      <td>3</td>\n",
       "      <td>18.10.2025</td>\n",
       "      <td>18:04</td>\n",
       "      <td>–ü–µ—Ä–µ–±–æ—Ä —Å –æ—Å—Ç—Ä–æ—Ç–æ–π</td>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>–≥–æ–¥ –∏–ª–∏ –±–æ–ª–µ–µ</td>\n",
       "      <td>–°—Ç–æ–∏–º–æ—Å—Ç—å</td>\n",
       "      <td>–°–ª–∏—à–∫–æ–º –æ—Å—Ç—Ä—ã–µ | –•–∏–º–æ–∑–Ω–æ–µ –ø–æ—Å–ª–µ–≤–∫—É—Å–∏–µ</td>\n",
       "      <td>–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://irecommend.ru/content/perebor-s-ostrotoi</td>\n",
       "      <td>2025-10-18 22:15:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays \"–û–ª–∏–≤—å–µ —Å –ø–µ—Ä–µ–ø–µ–ª–∫–æ–π\"</td>\n",
       "      <td>Olga Bogdanova</td>\n",
       "      <td>4</td>\n",
       "      <td>17.10.2025</td>\n",
       "      <td>18:14</td>\n",
       "      <td>–í –ø–∞—á–∫–∞—Ö Lay's –∑–∞–ø–∞—Ö–ª–æ –ù–æ–≤—ã–º –≥–æ–¥–æ–ºüéÑ‚õÑ, –ø—Ä–æ–±—É—é –Ω...</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö –ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è –∑...</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö\\nüëã\\n–ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª...</td>\n",
       "      <td>149 —Ä—É–±.</td>\n",
       "      <td>–í –º–µ—Ä—É —Å–æ–ª–µ–Ω—ã–µ | –ï—Å—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –≤–æ –≤–∫—É—Å–µ —Å –æ–ª–∏–≤...</td>\n",
       "      <td>–ú–∞–ª–æ –º—è—Å–Ω–æ–π –∞—Ä–æ–º–∞—Ç–∏–∫–∏ | –ù–µ–æ–±—ã—á–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://irecommend.ru/content/v-pachkakh-lays-...</td>\n",
       "      <td>2025-10-18 22:15:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name          author  rating  \\\n",
       "0  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Twister –ö–æ–ª–±–∞—Å–∫–∏ –≥—Ä–∏–ª—å —Å –≥–æ...     –°–∞–Ω–¥—É –ú–∞–¥–∞–Ω       3   \n",
       "1      –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays \"–û–ª–∏–≤—å–µ —Å –ø–µ—Ä–µ–ø–µ–ª–∫–æ–π\"  Olga Bogdanova       4   \n",
       "\n",
       "  date_created time_created  \\\n",
       "0   18.10.2025        18:04   \n",
       "1   17.10.2025        18:14   \n",
       "\n",
       "                                               title  \\\n",
       "0                                 –ü–µ—Ä–µ–±–æ—Ä —Å –æ—Å—Ç—Ä–æ—Ç–æ–π   \n",
       "1  –í –ø–∞—á–∫–∞—Ö Lay's –∑–∞–ø–∞—Ö–ª–æ –ù–æ–≤—ã–º –≥–æ–¥–æ–ºüéÑ‚õÑ, –ø—Ä–æ–±—É—é –Ω...   \n",
       "\n",
       "                                         teaser_text  \\\n",
       "0  –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...   \n",
       "1  –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö –ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è –∑...   \n",
       "\n",
       "                                           full_text     experience  \\\n",
       "0  –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...  –≥–æ–¥ –∏–ª–∏ –±–æ–ª–µ–µ   \n",
       "1  –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö\\nüëã\\n–ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª...       149 —Ä—É–±.   \n",
       "\n",
       "                                              pluses  \\\n",
       "0                                          –°—Ç–æ–∏–º–æ—Å—Ç—å   \n",
       "1  –í –º–µ—Ä—É —Å–æ–ª–µ–Ω—ã–µ | –ï—Å—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –≤–æ –≤–∫—É—Å–µ —Å –æ–ª–∏–≤...   \n",
       "\n",
       "                                  minuses         verdict  detailed_rating  \\\n",
       "0   –°–ª–∏—à–∫–æ–º –æ—Å—Ç—Ä—ã–µ | –•–∏–º–æ–∑–Ω–æ–µ –ø–æ—Å–ª–µ–≤–∫—É—Å–∏–µ  –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç              3.0   \n",
       "1  –ú–∞–ª–æ –º—è—Å–Ω–æ–π –∞—Ä–æ–º–∞—Ç–∏–∫–∏ | –ù–µ–æ–±—ã—á–Ω—ã–π –≤–∫—É—Å     —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç              4.0   \n",
       "\n",
       "                                          review_url           scraped_at  \n",
       "0   https://irecommend.ru/content/perebor-s-ostrotoi  2025-10-18 22:15:41  \n",
       "1  https://irecommend.ru/content/v-pachkakh-lays-...  2025-10-18 22:15:41  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reviews_20251018_2220.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a310fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Obtaining dependency information for psycopg2 from https://files.pythonhosted.org/packages/88/5a/18c8cb13fc6908dc41a483d2c14d927a7a3f29883748747e8cb625da6587/psycopg2-2.9.11-cp313-cp313-win_amd64.whl.metadata\n",
      "  Downloading psycopg2-2.9.11-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Downloading psycopg2-2.9.11-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB 217.8 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.0/2.7 MB 306.8 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.1/2.7 MB 660.7 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.3/2.7 MB 1.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.5/2.7 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.9/2.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.5/2.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.5/2.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.7/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.7/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.0/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.2/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.5/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea68e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58e5d21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Smart\\AppData\\Local\\Temp\\ipykernel_19368\\764397683.py:7: UserWarning: Parsing dates in %d.%m.%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['combined_created'] = pd.to_datetime(df['date_created'] + ' ' + df['time_created'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>experience</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>verdict</th>\n",
       "      <th>detailed_rating</th>\n",
       "      <th>review_url</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>combined_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Twister –ö–æ–ª–±–∞—Å–∫–∏ –≥—Ä–∏–ª—å —Å –≥–æ...</td>\n",
       "      <td>–°–∞–Ω–¥—É –ú–∞–¥–∞–Ω</td>\n",
       "      <td>3</td>\n",
       "      <td>18.10.2025</td>\n",
       "      <td>18:04</td>\n",
       "      <td>–ü–µ—Ä–µ–±–æ—Ä —Å –æ—Å—Ç—Ä–æ—Ç–æ–π</td>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>–Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...</td>\n",
       "      <td>–≥–æ–¥ –∏–ª–∏ –±–æ–ª–µ–µ</td>\n",
       "      <td>–°—Ç–æ–∏–º–æ—Å—Ç—å</td>\n",
       "      <td>–°–ª–∏—à–∫–æ–º –æ—Å—Ç—Ä—ã–µ | –•–∏–º–æ–∑–Ω–æ–µ –ø–æ—Å–ª–µ–≤–∫—É—Å–∏–µ</td>\n",
       "      <td>–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://irecommend.ru/content/perebor-s-ostrotoi</td>\n",
       "      <td>2025-10-18 22:15:41</td>\n",
       "      <td>2025-10-18 18:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays \"–û–ª–∏–≤—å–µ —Å –ø–µ—Ä–µ–ø–µ–ª–∫–æ–π\"</td>\n",
       "      <td>Olga Bogdanova</td>\n",
       "      <td>4</td>\n",
       "      <td>17.10.2025</td>\n",
       "      <td>18:14</td>\n",
       "      <td>–í –ø–∞—á–∫–∞—Ö Lay's –∑–∞–ø–∞—Ö–ª–æ –ù–æ–≤—ã–º –≥–æ–¥–æ–ºüéÑ‚õÑ, –ø—Ä–æ–±—É—é –Ω...</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö –ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è –∑...</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö\\nüëã\\n–ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª...</td>\n",
       "      <td>149 —Ä—É–±.</td>\n",
       "      <td>–í –º–µ—Ä—É —Å–æ–ª–µ–Ω—ã–µ | –ï—Å—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –≤–æ –≤–∫—É—Å–µ —Å –æ–ª–∏–≤...</td>\n",
       "      <td>–ú–∞–ª–æ –º—è—Å–Ω–æ–π –∞—Ä–æ–º–∞—Ç–∏–∫–∏ | –ù–µ–æ–±—ã—á–Ω—ã–π –≤–∫—É—Å</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://irecommend.ru/content/v-pachkakh-lays-...</td>\n",
       "      <td>2025-10-18 22:15:41</td>\n",
       "      <td>2025-10-17 18:14:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name          author  rating  \\\n",
       "0  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Twister –ö–æ–ª–±–∞—Å–∫–∏ –≥—Ä–∏–ª—å —Å –≥–æ...     –°–∞–Ω–¥—É –ú–∞–¥–∞–Ω       3   \n",
       "1      –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays \"–û–ª–∏–≤—å–µ —Å –ø–µ—Ä–µ–ø–µ–ª–∫–æ–π\"  Olga Bogdanova       4   \n",
       "\n",
       "  date_created time_created  \\\n",
       "0   18.10.2025        18:04   \n",
       "1   17.10.2025        18:14   \n",
       "\n",
       "                                               title  \\\n",
       "0                                 –ü–µ—Ä–µ–±–æ—Ä —Å –æ—Å—Ç—Ä–æ—Ç–æ–π   \n",
       "1  –í –ø–∞—á–∫–∞—Ö Lay's –∑–∞–ø–∞—Ö–ª–æ –ù–æ–≤—ã–º –≥–æ–¥–æ–ºüéÑ‚õÑ, –ø—Ä–æ–±—É—é –Ω...   \n",
       "\n",
       "                                         teaser_text  \\\n",
       "0  –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...   \n",
       "1  –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö –ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è –∑...   \n",
       "\n",
       "                                           full_text     experience  \\\n",
       "0  –Ø –µ–ª —ç—Ç–∏ —á–∏–ø—Å—ã –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, –µ—â–µ –≥–æ–¥–∞ 2 –Ω–∞–∑–∞–¥ —Å...  –≥–æ–¥ –∏–ª–∏ –±–æ–ª–µ–µ   \n",
       "1  –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤—Å–µ—Ö\\nüëã\\n–ù–∞ —É–ª–∏—Ü–∞—Ö –µ—â—ë –Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª...       149 —Ä—É–±.   \n",
       "\n",
       "                                              pluses  \\\n",
       "0                                          –°—Ç–æ–∏–º–æ—Å—Ç—å   \n",
       "1  –í –º–µ—Ä—É —Å–æ–ª–µ–Ω—ã–µ | –ï—Å—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –≤–æ –≤–∫—É—Å–µ —Å –æ–ª–∏–≤...   \n",
       "\n",
       "                                  minuses         verdict  detailed_rating  \\\n",
       "0   –°–ª–∏—à–∫–æ–º –æ—Å—Ç—Ä—ã–µ | –•–∏–º–æ–∑–Ω–æ–µ –ø–æ—Å–ª–µ–≤–∫—É—Å–∏–µ  –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç              3.0   \n",
       "1  –ú–∞–ª–æ –º—è—Å–Ω–æ–π –∞—Ä–æ–º–∞—Ç–∏–∫–∏ | –ù–µ–æ–±—ã—á–Ω—ã–π –≤–∫—É—Å     —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç              4.0   \n",
       "\n",
       "                                          review_url           scraped_at  \\\n",
       "0   https://irecommend.ru/content/perebor-s-ostrotoi  2025-10-18 22:15:41   \n",
       "1  https://irecommend.ru/content/v-pachkakh-lays-...  2025-10-18 22:15:41   \n",
       "\n",
       "     combined_created  \n",
       "0 2025-10-18 18:04:00  \n",
       "1 2025-10-17 18:14:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine('postgresql+psycopg2://airflow:airflow@localhost:5433/airflow')\n",
    "\n",
    "df['combined_created'] = pd.to_datetime(df['date_created'] + ' ' + df['time_created'])\n",
    "display(df.head(2))\n",
    "\n",
    "# Now use the engine with to_sql\n",
    "df.to_sql(\n",
    "    name='reviews',\n",
    "    schema='parser', \n",
    "    con=engine,\n",
    "    if_exists='append',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "112c7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql(sql = \"select max(combined_created) from parser.reviews\", con = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1f25559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-18 18:04:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  max\n",
       "0 2025-10-18 18:04:00"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d19cdae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-10-18 18:04:00')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df1['max'].iloc[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faeb6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ef46c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 23:38:06,763 - INFO - üéØ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ iRecommend —á–µ—Ä–µ–∑ –∫—ç—à\n",
      "2025-10-19 23:38:06,784 - INFO - üöÄ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: https://irecommend.ru/catalog/reviews/939-13393?page=93\n",
      "2025-10-19 23:38:21,402 - ERROR - ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: https://irecommend.ru/catalog/reviews/939-13393?page=93\n",
      "2025-10-19 23:38:21,410 - ERROR - ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import logging\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('irecommend_parser.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class IRecommendCacheParser:\n",
    "    def __init__(self, base_url=\"https://irecommend.ru\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.setup_session()\n",
    "        \n",
    "    def setup_session(self):\n",
    "        \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏\"\"\"\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        })\n",
    "\n",
    "    def get_through_cached_services(self, url):\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã\"\"\"\n",
    "        cached_services = [\n",
    "            self._try_google_cache,\n",
    "            self._try_archive_org,\n",
    "        ]\n",
    "        \n",
    "        for service in cached_services:\n",
    "            html = service(url)\n",
    "            if html:\n",
    "                return html\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _try_google_cache(self, url):\n",
    "        \"\"\"–ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ Google Cache\"\"\"\n",
    "        try:\n",
    "            cache_url = f\"https://webcache.googleusercontent.com/search?q=cache:{url}\"\n",
    "            response = self.session.get(cache_url, timeout=15)\n",
    "            if response.status_code == 200 and self._validate_content(response.text):\n",
    "                logging.info(\"‚úÖ –£—Å–ø–µ—Ö —á–µ—Ä–µ–∑ Google Cache\")\n",
    "                return response.text\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Google Cache –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}\")\n",
    "        return None\n",
    "\n",
    "    def _try_archive_org(self, url):\n",
    "        \"\"\"–ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ Archive.org\"\"\"\n",
    "        try:\n",
    "            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –¥–∞—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "            dates = [\"20241019\", \"20241018\", \"20241015\", \"20241010\", \"20241001\"]\n",
    "            for date in dates:\n",
    "                archive_url = f\"https://web.archive.org/web/{date}/{url}\"\n",
    "                response = self.session.get(archive_url, timeout=15)\n",
    "                if response.status_code == 200 and self._validate_content(response.text):\n",
    "                    logging.info(f\"‚úÖ –£—Å–ø–µ—Ö —á–µ—Ä–µ–∑ Archive.org ({date})\")\n",
    "                    return response.text\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Archive.org –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}\")\n",
    "        return None\n",
    "\n",
    "    def _validate_content(self, html):\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–∑—ã–≤—ã\"\"\"\n",
    "        required_elements = ['smTeaser', 'productName', 'reviewTitle']\n",
    "        return any(element in html for element in required_elements)\n",
    "\n",
    "    def parse_reviews_list(self, html):\n",
    "        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –æ—Ç–∑—ã–≤–æ–≤\"\"\"\n",
    "        if not html:\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        reviews = []\n",
    "        \n",
    "        # –ò—â–µ–º –±–ª–æ–∫–∏ —Å –æ—Ç–∑—ã–≤–∞–º–∏\n",
    "        review_blocks = soup.find_all('div', class_='smTeaser')\n",
    "        \n",
    "        for block in review_blocks:\n",
    "            review_data = self.parse_review_block(block)\n",
    "            if review_data:\n",
    "                reviews.append(review_data)\n",
    "        \n",
    "        logging.info(f\"üìù –ù–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {len(reviews)}\")\n",
    "        return reviews\n",
    "\n",
    "    def parse_review_block(self, block):\n",
    "        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –±–ª–æ–∫–∞ –æ—Ç–∑—ã–≤–∞\"\"\"\n",
    "        try:\n",
    "            # –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞\n",
    "            product_elem = block.find('div', class_='productName')\n",
    "            product_name = product_elem.get_text(strip=True) if product_elem else \"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç\"\n",
    "            \n",
    "            # –ê–≤—Ç–æ—Ä\n",
    "            author_elem = block.find('div', class_='authorName')\n",
    "            author_name = author_elem.get_text(strip=True) if author_elem else \"–ê–Ω–æ–Ω–∏–º\"\n",
    "            \n",
    "            # –†–µ–π—Ç–∏–Ω–≥\n",
    "            rating = self.extract_rating_from_block(block)\n",
    "            \n",
    "            # –î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è\n",
    "            date_elem = block.find('span', class_='date-created')\n",
    "            time_elem = block.find('span', class_='time-created')\n",
    "            date_created = date_elem.get_text(strip=True) if date_elem else \"\"\n",
    "            time_created = time_elem.get_text(strip=True) if time_elem else \"\"\n",
    "            \n",
    "            # –ó–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "            title_elem = block.find('div', class_='reviewTitle')\n",
    "            title = title_elem.get_text(strip=True) if title_elem else \"\"\n",
    "            \n",
    "            # –¢–µ–∫—Å—Ç –ø—Ä–µ–≤—å—é\n",
    "            teaser_elem = block.find('span', class_='reviewTeaserText')\n",
    "            teaser_text = teaser_elem.get_text(strip=True) if teaser_elem else \"\"\n",
    "            \n",
    "            # –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–ª–Ω—ã–π –æ—Ç–∑—ã–≤\n",
    "            link_elem = block.find('a', class_='reviewTextSnippet')\n",
    "            review_url = urljoin(self.base_url, link_elem['href']) if link_elem and link_elem.get('href') else \"\"\n",
    "            \n",
    "            return {\n",
    "                'product_name': product_name,\n",
    "                'author': author_name,\n",
    "                'rating': rating,\n",
    "                'date_created': date_created,\n",
    "                'time_created': time_created,\n",
    "                'title': title,\n",
    "                'teaser_text': teaser_text,\n",
    "                'review_url': review_url,\n",
    "                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –±–ª–æ–∫–∞: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_rating_from_block(self, block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∞\"\"\"\n",
    "        try:\n",
    "            rating_elem = block.find('div', class_='starsRating')\n",
    "            if rating_elem:\n",
    "                # –ò—â–µ–º –∫–ª–∞—Å—Å —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ fivestarWidgetStatic-X\n",
    "                for cls in rating_elem.get('class', []):\n",
    "                    if 'fivestarWidgetStatic-' in cls:\n",
    "                        return cls.split('-')[-1]\n",
    "                \n",
    "                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥: —Å—á–∏—Ç–∞–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–≤–µ–∑–¥—ã\n",
    "                stars = rating_elem.find_all('div', class_='star')\n",
    "                filled_stars = sum(1 for star in stars if star.find('div', class_='on'))\n",
    "                return str(filled_stars)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–π—Ç–∏–Ω–≥–∞: {e}\")\n",
    "        return \"0\"\n",
    "\n",
    "    def parse_full_review(self, url):\n",
    "        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞\"\"\"\n",
    "        if not url:\n",
    "            return {}\n",
    "            \n",
    "        logging.info(f\"üîç –ü–∞—Ä—Å–∏–º –ø–æ–ª–Ω—ã–π –æ—Ç–∑—ã–≤: {url}\")\n",
    "        html = self.get_through_cached_services(url)\n",
    "        \n",
    "        if not html:\n",
    "            logging.warning(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–∑—ã–≤: {url}\")\n",
    "            return {}\n",
    "            \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        review_data = {}\n",
    "        \n",
    "        try:\n",
    "            review_block = soup.find('div', class_='reviewBlock')\n",
    "            if not review_block:\n",
    "                return {}\n",
    "            \n",
    "            # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞\n",
    "            review_body = review_block.find('div', itemprop='reviewBody')\n",
    "            if review_body:\n",
    "                review_data['full_text'] = self.clean_text(review_body)\n",
    "            else:\n",
    "                review_data['full_text'] = \"\"\n",
    "            \n",
    "            # –û–ø—ã—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (—Å—Ç–æ–∏–º–æ—Å—Ç—å –∏ —Ç.–¥.)\n",
    "            experience_data = self.extract_experience_info(review_block)\n",
    "            review_data['experience'] = experience_data\n",
    "            \n",
    "            # –î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞\n",
    "            pluses = self.extract_pluses(review_block)\n",
    "            review_data['pluses'] = ' | '.join(pluses) if pluses else \"\"\n",
    "            \n",
    "            # –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏\n",
    "            minuses = self.extract_minuses(review_block)\n",
    "            review_data['minuses'] = ' | '.join(minuses) if minuses else \"\"\n",
    "            \n",
    "            # –í–µ—Ä–¥–∏–∫—Ç\n",
    "            verdict = self.extract_verdict(review_block)\n",
    "            review_data['verdict'] = verdict\n",
    "            \n",
    "            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "            additional_info = self.extract_additional_info(review_block)\n",
    "            review_data.update(additional_info)\n",
    "            \n",
    "            logging.info(f\"‚úÖ –ü–æ–ª–Ω—ã–π –æ—Ç–∑—ã–≤: {len(review_data.get('full_text', ''))} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {e}\")\n",
    "        \n",
    "        return review_data\n",
    "\n",
    "    def extract_experience_info(self, review_block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–ø—ã—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\"\"\"\n",
    "        try:\n",
    "            extra_info = review_block.find('div', class_='extraInfo')\n",
    "            if extra_info:\n",
    "                item_data = extra_info.find('div', class_='item-data')\n",
    "                if item_data:\n",
    "                    return item_data.get_text(strip=True)\n",
    "        except:\n",
    "            pass\n",
    "        return \"\"\n",
    "\n",
    "    def extract_pluses(self, review_block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤\"\"\"\n",
    "        try:\n",
    "            plus_block = review_block.find('div', class_='plus')\n",
    "            if plus_block:\n",
    "                plus_items = plus_block.find_all('li')\n",
    "                return [item.get_text(strip=True) for item in plus_items]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    def extract_minuses(self, review_block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤\"\"\"\n",
    "        try:\n",
    "            minus_block = review_block.find('div', class_='minus')\n",
    "            if minus_block:\n",
    "                minus_items = minus_block.find_all('li')\n",
    "                return [item.get_text(strip=True) for item in minus_items]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    def extract_verdict(self, review_block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ—Ä–¥–∏–∫—Ç–∞\"\"\"\n",
    "        try:\n",
    "            conclusion = review_block.find('div', class_='conclusion')\n",
    "            if conclusion:\n",
    "                verdict_elem = conclusion.find('span', class_='verdict')\n",
    "                if verdict_elem:\n",
    "                    return verdict_elem.get_text(strip=True)\n",
    "        except:\n",
    "            pass\n",
    "        return \"\"\n",
    "\n",
    "    def extract_additional_info(self, review_block):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\"\"\"\n",
    "        info = {}\n",
    "        try:\n",
    "            # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞\n",
    "            date_elem = review_block.find('span', class_='dtreviewed')\n",
    "            if date_elem:\n",
    "                info['full_date'] = date_elem.get_text(strip=True)\n",
    "            \n",
    "            # –†–µ–π—Ç–∏–Ω–≥ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)\n",
    "            rating_meta = review_block.find('meta', itemprop='ratingValue')\n",
    "            if rating_meta:\n",
    "                info['rating_verified'] = rating_meta.get('content', '')\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return info\n",
    "\n",
    "    def clean_text(self, element):\n",
    "        \"\"\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "        if not element:\n",
    "            return \"\"\n",
    "        \n",
    "        for br in element.find_all(\"br\"):\n",
    "            br.replace_with(\"\\n\")\n",
    "        \n",
    "        text = element.get_text(separator='\\n')\n",
    "        lines = [line.strip() for line in text.split('\\n')]\n",
    "        lines = [line for line in lines if line]\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def scrape_page(self, page_url, max_reviews=10, get_full_reviews=True):\n",
    "        \"\"\"–°–±–æ—Ä –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã\"\"\"\n",
    "        logging.info(f\"üöÄ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_url}\")\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ –∫—ç—à\n",
    "        html = self.get_through_cached_services(page_url)\n",
    "        \n",
    "        if not html:\n",
    "            logging.error(f\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_url}\")\n",
    "            return []\n",
    "        \n",
    "        # –ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–æ–∫ –æ—Ç–∑—ã–≤–æ–≤\n",
    "        preview_reviews = self.parse_reviews_list(html)\n",
    "        \n",
    "        if not preview_reviews:\n",
    "            logging.warning(f\"‚ùå –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_url} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤\")\n",
    "            return []\n",
    "        \n",
    "        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ\n",
    "        preview_reviews = preview_reviews[:max_reviews]\n",
    "        \n",
    "        complete_reviews = []\n",
    "        successful_full = 0\n",
    "        \n",
    "        for i, preview in enumerate(preview_reviews, 1):\n",
    "            logging.info(f\"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–∞ {i}/{len(preview_reviews)}: {preview['title'][:50]}...\")\n",
    "            \n",
    "            if get_full_reviews and preview['review_url']:\n",
    "                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞\n",
    "                full_data = self.parse_full_review(preview['review_url'])\n",
    "                \n",
    "                if full_data:\n",
    "                    complete_review = {**preview, **full_data}\n",
    "                    complete_reviews.append(complete_review)\n",
    "                    successful_full += 1\n",
    "                else:\n",
    "                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–≤—å—é\n",
    "                    preview['full_text'] = preview.get('teaser_text', '')\n",
    "                    complete_reviews.append(preview)\n",
    "            else:\n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–µ–≤—å—é –¥–∞–Ω–Ω—ã–µ\n",
    "                preview['full_text'] = preview.get('teaser_text', '')\n",
    "                complete_reviews.append(preview)\n",
    "            \n",
    "            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—Ç–∑—ã–≤–æ–≤\n",
    "            if i < len(preview_reviews):\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "        \n",
    "        logging.info(f\"üéâ –°–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! –ü–æ–ª–Ω—ã–µ –æ—Ç–∑—ã–≤—ã: {successful_full}/{len(complete_reviews)}\")\n",
    "        return complete_reviews\n",
    "\n",
    "    def scrape_multiple_pages(self, base_url, pages, max_reviews_per_page=5):\n",
    "        \"\"\"–°–±–æ—Ä –æ—Ç–∑—ã–≤–æ–≤ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\"\"\"\n",
    "        all_reviews = []\n",
    "        \n",
    "        for page in pages:\n",
    "            if page == 1:\n",
    "                page_url = base_url\n",
    "            else:\n",
    "                page_url = f\"{base_url}?page={page}\"\n",
    "            \n",
    "            logging.info(f\"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}\")\n",
    "            \n",
    "            page_reviews = self.scrape_page(page_url, max_reviews_per_page, get_full_reviews=True)\n",
    "            all_reviews.extend(page_reviews)\n",
    "            \n",
    "            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏\n",
    "            if page < pages[-1]:\n",
    "                time.sleep(random.uniform(5, 10))\n",
    "        \n",
    "        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –æ—Ç–∑—ã–≤—ã –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö)\n",
    "        unique_reviews = self.remove_duplicates(all_reviews)\n",
    "        return unique_reviews\n",
    "\n",
    "    def remove_duplicates(self, reviews):\n",
    "        \"\"\"–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤\"\"\"\n",
    "        seen = set()\n",
    "        unique = []\n",
    "        \n",
    "        for review in reviews:\n",
    "            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –∞–≤—Ç–æ—Ä–∞\n",
    "            key = (review['title'], review['author'])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique.append(review)\n",
    "        \n",
    "        return unique\n",
    "\n",
    "    def save_to_csv(self, reviews, filename):\n",
    "        \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV\"\"\"\n",
    "        if not reviews:\n",
    "            logging.error(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            fieldnames = [\n",
    "                'product_name', 'author', 'rating', 'date_created', 'time_created',\n",
    "                'title', 'teaser_text', 'full_text', 'experience', 'pluses', \n",
    "                'minuses', 'verdict', 'full_date', 'rating_verified', 'review_url', \n",
    "                'scraped_at'\n",
    "            ]\n",
    "            \n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                \n",
    "                for review in reviews:\n",
    "                    row = {field: review.get(field, '') for field in fieldnames}\n",
    "                    writer.writerow(row)\n",
    "            \n",
    "            logging.info(f\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –≤ {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}\")\n",
    "            return False\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "def scrape_single_page(page_number=1):\n",
    "    \"\"\"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã\"\"\"\n",
    "    parser = IRecommendCacheParser()\n",
    "    \n",
    "    if page_number == 1:\n",
    "        url = \"https://irecommend.ru/catalog/reviews/939-13393\"\n",
    "    else:\n",
    "        url = f\"https://irecommend.ru/catalog/reviews/939-13393?page={page_number}\"\n",
    "    \n",
    "    reviews = parser.scrape_page(url, max_reviews=10, get_full_reviews=True)\n",
    "    return reviews\n",
    "\n",
    "def scrape_multiple_pages(page_numbers):\n",
    "    \"\"\"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\"\"\"\n",
    "    parser = IRecommendCacheParser()\n",
    "    \n",
    "    reviews = parser.scrape_multiple_pages(\n",
    "        base_url=\"https://irecommend.ru/catalog/reviews/939-13393\",\n",
    "        pages=page_numbers,\n",
    "        max_reviews_per_page=5\n",
    "    )\n",
    "    return reviews\n",
    "\n",
    "def main():\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤\"\"\"\n",
    "    logging.info(\"üéØ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ iRecommend —á–µ—Ä–µ–∑ –∫—ç—à\")\n",
    "    \n",
    "    # –°—Ü–µ–Ω–∞—Ä–∏–π 1: –û–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞\n",
    "    # reviews = scrape_single_page(page_number=1)\n",
    "    \n",
    "    # –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü\n",
    "    #reviews = scrape_multiple_pages(page_numbers=[1, 2, 3])\n",
    "    \n",
    "    # –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 99)\n",
    "    reviews = scrape_single_page(page_number=93)\n",
    "    \n",
    "    if reviews:\n",
    "        filename = f\"irecommend_reviews_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "        parser = IRecommendCacheParser()\n",
    "        success = parser.save_to_csv(reviews, filename)\n",
    "        \n",
    "        if success:\n",
    "            logging.info(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {len(reviews)}\")\n",
    "            \n",
    "            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "            full_text_count = sum(1 for r in reviews if r.get('full_text') and len(r['full_text']) > 500)\n",
    "            pluses_count = sum(1 for r in reviews if r.get('pluses'))\n",
    "            minuses_count = sum(1 for r in reviews if r.get('minuses'))\n",
    "            \n",
    "            logging.info(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "            logging.info(f\"   - –ü–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã: {full_text_count}\")\n",
    "            logging.info(f\"   - –° –ø–ª—é—Å–∞–º–∏: {pluses_count}\") \n",
    "            logging.info(f\"   - –° –º–∏–Ω—É—Å–∞–º–∏: {minuses_count}\")\n",
    "            \n",
    "            if reviews:\n",
    "                sample = reviews[0]\n",
    "                logging.info(f\"üìÑ –ü—Ä–∏–º–µ—Ä –æ—Ç–∑—ã–≤–∞: '{sample['title']}'\")\n",
    "        else:\n",
    "            logging.error(\"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\")\n",
    "    else:\n",
    "        logging.error(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad76b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudscraper in c:\\users\\smart\\pycharmprojects\\nlp_and_mlops_course_work\\.venv\\lib\\site-packages (1.2.71)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement fp.free-proxy (from versions: none)\n",
      "ERROR: No matching distribution found for fp.free-proxy\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install cloudscraper fp.free-proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e30d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fdd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>experience</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>verdict</th>\n",
       "      <th>full_date</th>\n",
       "      <th>rating_verified</th>\n",
       "      <th>review_url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays \"–û–ª–∏–≤—å–µ —Å –ø–µ—Ä–µ–ø–µ–ª–∫–æ–π\"</td>\n",
       "      <td>–≥–µ–ª–ª–∞ —Ä–∞–Ω—å—à–µ –ø–µ–ª–∞</td>\n",
       "      <td>5</td>\n",
       "      <td>19.10.2025</td>\n",
       "      <td>21:47</td>\n",
       "      <td>–û—Å–µ–Ω–Ω–∏–π –ù–æ–≤—ã–π –≥–æ–¥ —á–∞—Å—Ç—å –≤—Ç–æ—Ä–∞—è. –£–¥–∞–ª–æ—Å—å –ª–∏ Lay...</td>\n",
       "      <td>–í–°–ï–ú –ü–†–ò–í–ï–¢–´) ¬† –ù–µ —Å–∫–∞–∂—É, —á—Ç–æ —á–∞—Å—Ç–æ –µ–º —á–∏–ø—Å—ã, ...</td>\n",
       "      <td>–í–°–ï–ú –ü–†–ò–í–ï–¢–´)\\n–ù–µ —Å–∫–∞–∂—É, —á—Ç–æ —á–∞—Å—Ç–æ –µ–º —á–∏–ø—Å—ã, –µ...</td>\n",
       "      <td>149 —Ä—É–±–ª–µ–π</td>\n",
       "      <td>–í –º–µ—Ä—É —Å–æ–ª–µ–Ω—ã–µ | –ï—Å—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –≤–æ –≤–∫—É—Å–µ —Å –æ–ª–∏–≤...</td>\n",
       "      <td>–í–∫—É—Å —É–ª–µ—Ç—É—á–∏–ª—Å—è –Ω–∞ –∑–∞–≤—Ç—Ä–∞, —Å–ª–æ–≤–Ω–æ –ø—Ä–æ—à–ª—ã–π –≥–æ–¥ ...</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç</td>\n",
       "      <td>19 –û–∫—Ç—è–±—Ä—å, 2025 - 21:47</td>\n",
       "      <td>5</td>\n",
       "      <td>https://irecommend.ru/content/osennii-novyi-go...</td>\n",
       "      <td>2025-10-19 23:02:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    product_name             author  rating  \\\n",
       "0  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays \"–û–ª–∏–≤—å–µ —Å –ø–µ—Ä–µ–ø–µ–ª–∫–æ–π\"  –≥–µ–ª–ª–∞ —Ä–∞–Ω—å—à–µ –ø–µ–ª–∞       5   \n",
       "\n",
       "  date_created time_created  \\\n",
       "0   19.10.2025        21:47   \n",
       "\n",
       "                                               title  \\\n",
       "0  –û—Å–µ–Ω–Ω–∏–π –ù–æ–≤—ã–π –≥–æ–¥ —á–∞—Å—Ç—å –≤—Ç–æ—Ä–∞—è. –£–¥–∞–ª–æ—Å—å –ª–∏ Lay...   \n",
       "\n",
       "                                         teaser_text  \\\n",
       "0  –í–°–ï–ú –ü–†–ò–í–ï–¢–´) ¬† –ù–µ —Å–∫–∞–∂—É, —á—Ç–æ —á–∞—Å—Ç–æ –µ–º —á–∏–ø—Å—ã, ...   \n",
       "\n",
       "                                           full_text  experience  \\\n",
       "0  –í–°–ï–ú –ü–†–ò–í–ï–¢–´)\\n–ù–µ —Å–∫–∞–∂—É, —á—Ç–æ —á–∞—Å—Ç–æ –µ–º —á–∏–ø—Å—ã, –µ...  149 —Ä—É–±–ª–µ–π   \n",
       "\n",
       "                                              pluses  \\\n",
       "0  –í –º–µ—Ä—É —Å–æ–ª–µ–Ω—ã–µ | –ï—Å—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –≤–æ –≤–∫—É—Å–µ —Å –æ–ª–∏–≤...   \n",
       "\n",
       "                                             minuses      verdict  \\\n",
       "0  –í–∫—É—Å —É–ª–µ—Ç—É—á–∏–ª—Å—è –Ω–∞ –∑–∞–≤—Ç—Ä–∞, —Å–ª–æ–≤–Ω–æ –ø—Ä–æ—à–ª—ã–π –≥–æ–¥ ...  —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç   \n",
       "\n",
       "                  full_date  rating_verified  \\\n",
       "0  19 –û–∫—Ç—è–±—Ä—å, 2025 - 21:47                5   \n",
       "\n",
       "                                          review_url           scraped_at  \n",
       "0  https://irecommend.ru/content/osennii-novyi-go...  2025-10-19 23:02:35  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('irecommend_reviews_20251019_2303.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd2ea261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>experience</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>verdict</th>\n",
       "      <th>full_date</th>\n",
       "      <th>rating_verified</th>\n",
       "      <th>review_url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lava Lava –ö—Ä–∞–±–æ-–Ω–∏–Ω–¥–∑—è</td>\n",
       "      <td>XeniumX</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–¢–∞–∫ –ª–∏ —Ö–æ—Ä–æ—à–∏ —á–∏–ø—Å—ã –æ—Ç –±—É–º–∞–∂–Ω–æ–≥–æ –±–ª–æ–≥–µ—Ä–∞, –∫–∞–∫ ...</td>\n",
       "      <td>–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –Ø –¥—É–º–∞—é, –º–Ω–æ–≥–∏–µ —Ä–æ–¥–∏—Ç–µ–ª–∏, —É –∫–æ–≥–æ ...</td>\n",
       "      <td>–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –Ø –¥—É–º–∞—é, –º–Ω–æ–≥–∏–µ —Ä–æ–¥–∏—Ç–µ–ª–∏, —É –∫–æ–≥–æ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lorenz Crunchips X-Cut —Å–º–µ—Ç...</td>\n",
       "      <td>katerina_5512</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–û—á–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–∏–ø—Å—ã –ø–æ –Ω–µ–≤—ã—Å–æ–∫–æ–π —Ü–µ–Ω–µ!</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ —Ä–∏—Ñ–ª–µ–Ω—ã–µ ¬´Crunchips X-Cut¬ª ...</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ —Ä–∏—Ñ–ª–µ–Ω—ã–µ ¬´Crunchips X-Cut¬ª ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays –ú–æ—Ü–∞—Ä–µ–ª–ª–∞ —Å –ø–µ—Å—Ç–æ</td>\n",
       "      <td>katerina_5512</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–í–∫—É—Å—ã —á–∏–ø—Å–æ–≤ —Å –∫–∞–∂–¥—ã–º —Ä–∞–∑–æ–º —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –º–µ–Ω–µ–µ –±...</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Å–æ –≤–∫—É—Å–æ–º...</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Å–æ –≤–∫—É—Å–æ–º...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–ß–∏–ø—Å—ã TWISTER –°—ã—Ä</td>\n",
       "      <td>–ï–∫–∞—Ç–µ—Ä–∏–Ω–∞1703niz</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–•—Ä—É—Å—Ç—è—â–∏–µ –∏ –ª–µ–≥–∫–∏–µ —Å—ã—Ä–Ω—ã–µ —á–∏–ø—Å—ãüßÄ</td>\n",
       "      <td>–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ò–Ω–æ–≥–¥–∞ –æ—á–µ–Ω—å —Ö–æ—á–µ—Ç—Å—è —á–µ–º-–Ω–∏–±—É–¥—å –ø...</td>\n",
       "      <td>–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ò–Ω–æ–≥–¥–∞ –æ—á–µ–Ω—å —Ö–æ—á–µ—Ç—Å—è —á–µ–º-–Ω–∏–±—É–¥—å –ø...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays –†–∏—Ñ–ª–µ–Ω—ã–µ –û—Å—Ç—Ä—ã–µ –∫—Ä—ã–ª—ã—à–∫–∏</td>\n",
       "      <td>katerina_5512</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>–ù–∞—Å–∫–æ–ª—å–∫–æ —É–¥–∞—á–Ω–∞ –Ω–æ–≤–∏–Ω–∫–∞?</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Ä–∏—Ñ–ª–µ–Ω—ã–µ ...</td>\n",
       "      <td>–ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Ä–∏—Ñ–ª–µ–Ω—ã–µ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name            author  \\\n",
       "0          –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lava Lava –ö—Ä–∞–±–æ-–Ω–∏–Ω–¥–∑—è           XeniumX   \n",
       "1  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lorenz Crunchips X-Cut —Å–º–µ—Ç...     katerina_5512   \n",
       "2          –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays –ú–æ—Ü–∞—Ä–µ–ª–ª–∞ —Å –ø–µ—Å—Ç–æ     katerina_5512   \n",
       "3                                  –ß–∏–ø—Å—ã TWISTER –°—ã—Ä  –ï–∫–∞—Ç–µ—Ä–∏–Ω–∞1703niz   \n",
       "4   –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ Lays –†–∏—Ñ–ª–µ–Ω—ã–µ –û—Å—Ç—Ä—ã–µ –∫—Ä—ã–ª—ã—à–∫–∏     katerina_5512   \n",
       "\n",
       "   rating  date_created  time_created  \\\n",
       "0       3           NaN           NaN   \n",
       "1       5           NaN           NaN   \n",
       "2       5           NaN           NaN   \n",
       "3       5           NaN           NaN   \n",
       "4       5           NaN           NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0  –¢–∞–∫ –ª–∏ —Ö–æ—Ä–æ—à–∏ —á–∏–ø—Å—ã –æ—Ç –±—É–º–∞–∂–Ω–æ–≥–æ –±–ª–æ–≥–µ—Ä–∞, –∫–∞–∫ ...   \n",
       "1        –û—á–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–∏–ø—Å—ã –ø–æ –Ω–µ–≤—ã—Å–æ–∫–æ–π —Ü–µ–Ω–µ!   \n",
       "2  –í–∫—É—Å—ã —á–∏–ø—Å–æ–≤ —Å –∫–∞–∂–¥—ã–º —Ä–∞–∑–æ–º —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –º–µ–Ω–µ–µ –±...   \n",
       "3                   –•—Ä—É—Å—Ç—è—â–∏–µ –∏ –ª–µ–≥–∫–∏–µ —Å—ã—Ä–Ω—ã–µ —á–∏–ø—Å—ãüßÄ   \n",
       "4                          –ù–∞—Å–∫–æ–ª—å–∫–æ —É–¥–∞—á–Ω–∞ –Ω–æ–≤–∏–Ω–∫–∞?   \n",
       "\n",
       "                                         teaser_text  \\\n",
       "0  –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –Ø –¥—É–º–∞—é, –º–Ω–æ–≥–∏–µ —Ä–æ–¥–∏—Ç–µ–ª–∏, —É –∫–æ–≥–æ ...   \n",
       "1  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ —Ä–∏—Ñ–ª–µ–Ω—ã–µ ¬´Crunchips X-Cut¬ª ...   \n",
       "2  –ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Å–æ –≤–∫—É—Å–æ–º...   \n",
       "3  –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ò–Ω–æ–≥–¥–∞ –æ—á–µ–Ω—å —Ö–æ—á–µ—Ç—Å—è —á–µ–º-–Ω–∏–±—É–¥—å –ø...   \n",
       "4  –ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Ä–∏—Ñ–ª–µ–Ω—ã–µ ...   \n",
       "\n",
       "                                           full_text  experience  pluses  \\\n",
       "0  –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –Ø –¥—É–º–∞—é, –º–Ω–æ–≥–∏–µ —Ä–æ–¥–∏—Ç–µ–ª–∏, —É –∫–æ–≥–æ ...         NaN     NaN   \n",
       "1  –ß–∏–ø—Å—ã –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å–Ω—ã–µ —Ä–∏—Ñ–ª–µ–Ω—ã–µ ¬´Crunchips X-Cut¬ª ...         NaN     NaN   \n",
       "2  –ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Å–æ –≤–∫—É—Å–æ–º...         NaN     NaN   \n",
       "3  –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ò–Ω–æ–≥–¥–∞ –æ—á–µ–Ω—å —Ö–æ—á–µ—Ç—Å—è —á–µ–º-–Ω–∏–±—É–¥—å –ø...         NaN     NaN   \n",
       "4  –ß–∏–ø—Å—ã –∏–∑ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è Lays —Ä–∏—Ñ–ª–µ–Ω—ã–µ ...         NaN     NaN   \n",
       "\n",
       "   minuses  verdict  full_date  rating_verified  review_url  \\\n",
       "0      NaN      NaN        NaN              NaN         NaN   \n",
       "1      NaN      NaN        NaN              NaN         NaN   \n",
       "2      NaN      NaN        NaN              NaN         NaN   \n",
       "3      NaN      NaN        NaN              NaN         NaN   \n",
       "4      NaN      NaN        NaN              NaN         NaN   \n",
       "\n",
       "            scraped_at  \n",
       "0  2025-10-19 23:34:35  \n",
       "1  2025-10-19 23:34:35  \n",
       "2  2025-10-19 23:34:35  \n",
       "3  2025-10-19 23:34:35  \n",
       "4  2025-10-19 23:34:35  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ò–Ω–æ–≥–¥–∞ –æ—á–µ–Ω—å —Ö–æ—á–µ—Ç—Å—è —á–µ–º-–Ω–∏–±—É–¥—å –ø–æ—Ö—Ä—É—Å—Ç–µ—Ç—å, —á–∏–ø—Å–∞–º–∏ –∫–∞–∫–∏–º–∏-–Ω–∏–±—É–¥—å –≤–∫—É—Å–Ω—ã–º–∏ –ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ —ç—Ç–æ —á–∏–ø—Å—ã - TWISTER —Å–æ –≤–∫—É—Å–æ–º —Å—ã—Ä–∞ –æ—Ç –ú–æ—Å–∫–æ–≤—Å–∫–æ–≥–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è.–ü—Ä–æ–¥–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ —á–∏–ø—Å—ã –≤–æ –≤—Å–µ—Ö —Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç–∞—Ö –∏ –æ–±—ã—á–Ω—ã—Ö –º–∞–≥–∞–∑–∏–Ω–∞—Ö, —Ü–µ–Ω–∞ –∑–∞ —É–ø–∞–∫–æ–≤–∫—É 70 –≥—Ä–∞–º–º –≤ —Ä–∞–π–æ–Ω–µ 60 —Ä—É–±–ª–µ–π.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('irecommend_reviews_20251019_2335.csv')\n",
    "display(df)\n",
    "df['full_text'][3]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
