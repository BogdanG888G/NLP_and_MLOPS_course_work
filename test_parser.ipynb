{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714fbb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 22:32:04,583 - INFO - Начало парсинга отзывов...\n",
      "2025-10-18 22:32:04,586 - INFO - Обработка страницы 1: https://irecommend.ru/catalog/reviews/939-13393\n",
      "2025-10-18 22:32:07,450 - WARNING - Статус код 521 для https://irecommend.ru/catalog/reviews/939-13393\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 341\u001b[39m\n\u001b[32m    338\u001b[39m         logging.error(\u001b[33m\"\u001b[39m\u001b[33mНе удалось собрать отзывы\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 326\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    323\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mНачало парсинга отзывов...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# Собираем отзывы (например, с 3 страниц)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m reviews = \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscrape_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTART_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reviews:\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# Сохраняем в CSV\u001b[39;00m\n\u001b[32m    330\u001b[39m     filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreviews_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 252\u001b[39m, in \u001b[36mReviewParser.scrape_reviews\u001b[39m\u001b[34m(self, start_url, pages)\u001b[39m\n\u001b[32m    248\u001b[39m     url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mОбработка страницы \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m html:\n\u001b[32m    254\u001b[39m     logging.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mНе удалось загрузить страницу \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mReviewParser.get_page\u001b[39m\u001b[34m(self, url, retries, delay)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m         response = \u001b[38;5;28mself\u001b[39m.session.get(url, timeout=\u001b[32m15\u001b[39m)\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     46\u001b[39m             \u001b[38;5;66;03m# Проверяем, что получили контент с отзывами\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import logging\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('review_parser.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class ReviewParser:\n",
    "    def __init__(self, base_url=\"https://irecommend.ru\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.setup_session()\n",
    "        \n",
    "    def setup_session(self):\n",
    "        \"\"\"Настройка сессии с реалистичными заголовками\"\"\"\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Referer': 'https://irecommend.ru/',\n",
    "        })\n",
    "\n",
    "    def get_page(self, url, retries=3, delay=2):\n",
    "        \"\"\"Получение страницы с обработкой ошибок\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                time.sleep(random.uniform(delay, delay * 2))\n",
    "                \n",
    "                response = self.session.get(url, timeout=15)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Проверяем, что получили контент с отзывами\n",
    "                    if 'smTeaser' in response.text or 'reviewBlock' in response.text:\n",
    "                        return response.text\n",
    "                    else:\n",
    "                        logging.warning(f\"Страница {url} не содержит отзывов\")\n",
    "                        return None\n",
    "                else:\n",
    "                    logging.warning(f\"Статус код {response.status_code} для {url}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Ошибка при запросе {url} (попытка {attempt+1}): {e}\")\n",
    "            \n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay * (attempt + 1))\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_reviews_from_list(self, html):\n",
    "        \"\"\"Парсинг всех отзывов со страницы списка\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        reviews_data = []\n",
    "        \n",
    "        # Находим все блоки с отзывами\n",
    "        review_blocks = soup.find_all('div', class_='smTeaser')\n",
    "        \n",
    "        for block in review_blocks:\n",
    "            review_data = self.parse_review_preview(block)\n",
    "            if review_data:\n",
    "                reviews_data.append(review_data)\n",
    "        \n",
    "        logging.info(f\"Найдено отзывов на странице: {len(reviews_data)}\")\n",
    "        return reviews_data\n",
    "\n",
    "    def parse_review_preview(self, block):\n",
    "        \"\"\"Парсинг превью отзыва из списка\"\"\"\n",
    "        try:\n",
    "            # Основная информация\n",
    "            product_elem = block.find('div', class_='productName')\n",
    "            product_name = product_elem.get_text(strip=True) if product_elem else \"Неизвестный продукт\"\n",
    "            \n",
    "            author_elem = block.find('div', class_='authorName')\n",
    "            author_name = author_elem.get_text(strip=True) if author_elem else \"Аноним\"\n",
    "            \n",
    "            # Рейтинг\n",
    "            rating = self.extract_rating(block)\n",
    "            \n",
    "            # Дата и время\n",
    "            date_elem = block.find('span', class_='date-created')\n",
    "            time_elem = block.find('span', class_='time-created')\n",
    "            date_created = date_elem.get_text(strip=True) if date_elem else \"\"\n",
    "            time_created = time_elem.get_text(strip=True) if time_elem else \"\"\n",
    "            \n",
    "            # Заголовок и текст превью\n",
    "            title_elem = block.find('div', class_='reviewTitle')\n",
    "            title = title_elem.get_text(strip=True) if title_elem else \"\"\n",
    "            \n",
    "            teaser_elem = block.find('span', class_='reviewTeaserText')\n",
    "            teaser_text = teaser_elem.get_text(strip=True) if teaser_elem else \"\"\n",
    "            \n",
    "            # Ссылка на полный отзыв\n",
    "            link_elem = block.find('a', class_='reviewTextSnippet')\n",
    "            review_url = urljoin(self.base_url, link_elem['href']) if link_elem and link_elem.get('href') else \"\"\n",
    "            \n",
    "            return {\n",
    "                'product_name': product_name,\n",
    "                'author': author_name,\n",
    "                'rating': rating,\n",
    "                'date_created': date_created,\n",
    "                'time_created': time_created,\n",
    "                'title': title,\n",
    "                'teaser_text': teaser_text,\n",
    "                'review_url': review_url,\n",
    "                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка при парсинге превью отзыва: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_full_review(self, url):\n",
    "        \"\"\"Парсинг полной версии отзыва\"\"\"\n",
    "        html = self.get_page(url)\n",
    "        if not html:\n",
    "            return None\n",
    "            \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        review_block = soup.find('div', class_='reviewBlock')\n",
    "        \n",
    "        if not review_block:\n",
    "            logging.warning(f\"Не найден блок отзыва для {url}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Полный текст отзыва\n",
    "            review_body = review_block.find('div', itemprop='reviewBody')\n",
    "            full_text = self.clean_text(review_body) if review_body else \"\"\n",
    "            \n",
    "            # Дополнительная информация\n",
    "            experience = self.extract_experience(review_block)\n",
    "            pluses = self.extract_pluses(review_block)\n",
    "            minuses = self.extract_minuses(review_block)\n",
    "            verdict = self.extract_verdict(review_block)\n",
    "            \n",
    "            # Рейтинг из мета-тега\n",
    "            rating_meta = review_block.find('meta', itemprop='ratingValue')\n",
    "            rating = rating_meta['content'] if rating_meta else \"\"\n",
    "            \n",
    "            return {\n",
    "                'full_text': full_text,\n",
    "                'experience': experience,\n",
    "                'pluses': ' | '.join(pluses),\n",
    "                'minuses': ' | '.join(minuses),\n",
    "                'verdict': verdict,\n",
    "                'detailed_rating': rating\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка при парсинге полного отзыва {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_rating(self, block):\n",
    "        \"\"\"Извлечение рейтинга из звезд\"\"\"\n",
    "        try:\n",
    "            rating_elem = block.find('div', class_='starsRating')\n",
    "            if rating_elem:\n",
    "                # Ищем класс с рейтингом\n",
    "                for cls in rating_elem.get('class', []):\n",
    "                    if 'fivestarWidgetStatic-' in cls:\n",
    "                        return cls.split('-')[-1]\n",
    "                \n",
    "                # Считаем заполненные звезды\n",
    "                stars = rating_elem.find_all('div', class_='star')\n",
    "                filled = sum(1 for star in stars if star.find('div', class_='on'))\n",
    "                return str(filled)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Ошибка при извлечении рейтинга: {e}\")\n",
    "        \n",
    "        return \"0\"\n",
    "\n",
    "    def extract_experience(self, review_block):\n",
    "        \"\"\"Извлечение опыта использования\"\"\"\n",
    "        try:\n",
    "            experience_elem = review_block.find('div', class_='item-data')\n",
    "            return experience_elem.get_text(strip=True) if experience_elem else \"\"\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def extract_pluses(self, review_block):\n",
    "        \"\"\"Извлечение достоинств\"\"\"\n",
    "        try:\n",
    "            plus_block = review_block.find('div', class_='plus')\n",
    "            if plus_block:\n",
    "                plus_items = plus_block.find_all('li')\n",
    "                return [item.get_text(strip=True) for item in plus_items]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    def extract_minuses(self, review_block):\n",
    "        \"\"\"Извлечение недостатков\"\"\"\n",
    "        try:\n",
    "            minus_block = review_block.find('div', class_='minus')\n",
    "            if minus_block:\n",
    "                minus_items = minus_block.find_all('li')\n",
    "                return [item.get_text(strip=True) for item in minus_items]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    def extract_verdict(self, review_block):\n",
    "        \"\"\"Извлечение вердикта (рекомендует/не рекомендует)\"\"\"\n",
    "        try:\n",
    "            verdict_elem = review_block.find('span', class_='verdict')\n",
    "            return verdict_elem.get_text(strip=True) if verdict_elem else \"\"\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def clean_text(self, element):\n",
    "        \"\"\"Очистка текста от HTML тегов\"\"\"\n",
    "        if not element:\n",
    "            return \"\"\n",
    "        \n",
    "        # Сохраняем переносы строк\n",
    "        for br in element.find_all(\"br\"):\n",
    "            br.replace_with(\"\\n\")\n",
    "        \n",
    "        text = element.get_text(separator='\\n')\n",
    "        \n",
    "        # Очистка от лишних пробелов\n",
    "        lines = [line.strip() for line in text.split('\\n')]\n",
    "        lines = [line for line in lines if line]\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def scrape_reviews(self, start_url, pages=5):\n",
    "        \"\"\"Основной метод для сбора отзывов\"\"\"\n",
    "        all_reviews = []\n",
    "        \n",
    "        for page in range(pages):\n",
    "            if page == 0:\n",
    "                url = start_url\n",
    "            else:\n",
    "                url = f\"{start_url}?page={page}\"\n",
    "            \n",
    "            logging.info(f\"Обработка страницы {page + 1}: {url}\")\n",
    "            \n",
    "            html = self.get_page(url)\n",
    "            if not html:\n",
    "                logging.warning(f\"Не удалось загрузить страницу {page + 1}\")\n",
    "                continue\n",
    "            \n",
    "            # Парсим отзывы со страницы списка\n",
    "            previews = self.parse_reviews_from_list(html)\n",
    "            \n",
    "            for i, preview in enumerate(previews, 1):\n",
    "                logging.info(f\"Обработка отзыва {i}/{len(previews)}: {preview['title'][:30]}...\")\n",
    "                \n",
    "                # Получаем полный текст отзыва\n",
    "                full_data = self.parse_full_review(preview['review_url'])\n",
    "                \n",
    "                if full_data:\n",
    "                    # Объединяем данные\n",
    "                    complete_review = {**preview, **full_data}\n",
    "                    all_reviews.append(complete_review)\n",
    "                else:\n",
    "                    # Сохраняем хотя бы превью\n",
    "                    preview['full_text'] = preview.get('teaser_text', '')\n",
    "                    all_reviews.append(preview)\n",
    "                \n",
    "                # Задержка между запросами\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            logging.info(f\"Страница {page + 1} обработана. Всего отзывов: {len(all_reviews)}\")\n",
    "            \n",
    "            # Задержка между страницами\n",
    "            if page < pages - 1:\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "        \n",
    "        return all_reviews\n",
    "\n",
    "    def save_to_csv(self, reviews, filename):\n",
    "        \"\"\"Сохранение отзывов в CSV файл\"\"\"\n",
    "        if not reviews:\n",
    "            logging.error(\"Нет данных для сохранения\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            # Определяем все возможные поля\n",
    "            fieldnames = [\n",
    "                'product_name', 'author', 'rating', 'date_created', 'time_created',\n",
    "                'title', 'teaser_text', 'full_text', 'experience', 'pluses', \n",
    "                'minuses', 'verdict', 'detailed_rating', 'review_url', 'scraped_at'\n",
    "            ]\n",
    "            \n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                \n",
    "                for review in reviews:\n",
    "                    # Записываем только существующие поля\n",
    "                    row = {field: review.get(field, '') for field in fieldnames}\n",
    "                    writer.writerow(row)\n",
    "            \n",
    "            logging.info(f\"Успешно сохранено {len(reviews)} отзывов в {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка при сохранении в CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "# Пример использования\n",
    "def main():\n",
    "    # URL страницы с отзывами (замените на актуальный)\n",
    "    START_URL = \"https://irecommend.ru/catalog/reviews/939-13393\"\n",
    "    \n",
    "    parser = ReviewParser()\n",
    "    \n",
    "    logging.info(\"Начало парсинга отзывов...\")\n",
    "    \n",
    "    # Собираем отзывы (например, с 3 страниц)\n",
    "    reviews = parser.scrape_reviews(START_URL, pages=1)\n",
    "    \n",
    "    if reviews:\n",
    "        # Сохраняем в CSV\n",
    "        filename = f\"reviews_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "        parser.save_to_csv(reviews, filename)\n",
    "        \n",
    "        # Выводим статистику\n",
    "        logging.info(f\"Собрано отзывов: {len(reviews)}\")\n",
    "        logging.info(f\"Первый отзыв: {reviews[0]['title']}\")\n",
    "        logging.info(f\"Текст первого отзыва: {reviews[0]['full_text'][:100]}...\")\n",
    "    else:\n",
    "        logging.error(\"Не удалось собрать отзывы\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02faf9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcec226e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>experience</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>verdict</th>\n",
       "      <th>detailed_rating</th>\n",
       "      <th>review_url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Чипсы картофельные Twister Колбаски гриль с го...</td>\n",
       "      <td>Санду Мадан</td>\n",
       "      <td>3</td>\n",
       "      <td>18.10.2025</td>\n",
       "      <td>18:04</td>\n",
       "      <td>Перебор с остротой</td>\n",
       "      <td>Я ел эти чипсы очень долго, еще года 2 назад с...</td>\n",
       "      <td>Я ел эти чипсы очень долго, еще года 2 назад с...</td>\n",
       "      <td>год или более</td>\n",
       "      <td>Стоимость</td>\n",
       "      <td>Слишком острые | Химозное послевкусие</td>\n",
       "      <td>не рекомендует</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://irecommend.ru/content/perebor-s-ostrotoi</td>\n",
       "      <td>2025-10-18 22:15:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Чипсы картофельные Lays \"Оливье с перепелкой\"</td>\n",
       "      <td>Olga Bogdanova</td>\n",
       "      <td>4</td>\n",
       "      <td>17.10.2025</td>\n",
       "      <td>18:14</td>\n",
       "      <td>В пачках Lay's запахло Новым годом🎄⛄, пробую н...</td>\n",
       "      <td>Приветствую всех На улицах ещё не закончился з...</td>\n",
       "      <td>Приветствую всех\\n👋\\nНа улицах ещё не закончил...</td>\n",
       "      <td>149 руб.</td>\n",
       "      <td>В меру соленые | Есть схожесть во вкусе с олив...</td>\n",
       "      <td>Мало мясной ароматики | Необычный вкус</td>\n",
       "      <td>рекомендует</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://irecommend.ru/content/v-pachkakh-lays-...</td>\n",
       "      <td>2025-10-18 22:15:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name          author  rating  \\\n",
       "0  Чипсы картофельные Twister Колбаски гриль с го...     Санду Мадан       3   \n",
       "1      Чипсы картофельные Lays \"Оливье с перепелкой\"  Olga Bogdanova       4   \n",
       "\n",
       "  date_created time_created  \\\n",
       "0   18.10.2025        18:04   \n",
       "1   17.10.2025        18:14   \n",
       "\n",
       "                                               title  \\\n",
       "0                                 Перебор с остротой   \n",
       "1  В пачках Lay's запахло Новым годом🎄⛄, пробую н...   \n",
       "\n",
       "                                         teaser_text  \\\n",
       "0  Я ел эти чипсы очень долго, еще года 2 назад с...   \n",
       "1  Приветствую всех На улицах ещё не закончился з...   \n",
       "\n",
       "                                           full_text     experience  \\\n",
       "0  Я ел эти чипсы очень долго, еще года 2 назад с...  год или более   \n",
       "1  Приветствую всех\\n👋\\nНа улицах ещё не закончил...       149 руб.   \n",
       "\n",
       "                                              pluses  \\\n",
       "0                                          Стоимость   \n",
       "1  В меру соленые | Есть схожесть во вкусе с олив...   \n",
       "\n",
       "                                  minuses         verdict  detailed_rating  \\\n",
       "0   Слишком острые | Химозное послевкусие  не рекомендует              3.0   \n",
       "1  Мало мясной ароматики | Необычный вкус     рекомендует              4.0   \n",
       "\n",
       "                                          review_url           scraped_at  \n",
       "0   https://irecommend.ru/content/perebor-s-ostrotoi  2025-10-18 22:15:41  \n",
       "1  https://irecommend.ru/content/v-pachkakh-lays-...  2025-10-18 22:15:41  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reviews_20251018_2220.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a310fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Obtaining dependency information for psycopg2 from https://files.pythonhosted.org/packages/88/5a/18c8cb13fc6908dc41a483d2c14d927a7a3f29883748747e8cb625da6587/psycopg2-2.9.11-cp313-cp313-win_amd64.whl.metadata\n",
      "  Downloading psycopg2-2.9.11-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Downloading psycopg2-2.9.11-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB 217.8 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.0/2.7 MB 306.8 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.1/2.7 MB 660.7 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.3/2.7 MB 1.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.5/2.7 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.9/2.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.5/2.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.5/2.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.7/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.7/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.0/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.2/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.5/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea68e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58e5d21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Smart\\AppData\\Local\\Temp\\ipykernel_19368\\764397683.py:7: UserWarning: Parsing dates in %d.%m.%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['combined_created'] = pd.to_datetime(df['date_created'] + ' ' + df['time_created'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>experience</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>verdict</th>\n",
       "      <th>detailed_rating</th>\n",
       "      <th>review_url</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>combined_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Чипсы картофельные Twister Колбаски гриль с го...</td>\n",
       "      <td>Санду Мадан</td>\n",
       "      <td>3</td>\n",
       "      <td>18.10.2025</td>\n",
       "      <td>18:04</td>\n",
       "      <td>Перебор с остротой</td>\n",
       "      <td>Я ел эти чипсы очень долго, еще года 2 назад с...</td>\n",
       "      <td>Я ел эти чипсы очень долго, еще года 2 назад с...</td>\n",
       "      <td>год или более</td>\n",
       "      <td>Стоимость</td>\n",
       "      <td>Слишком острые | Химозное послевкусие</td>\n",
       "      <td>не рекомендует</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://irecommend.ru/content/perebor-s-ostrotoi</td>\n",
       "      <td>2025-10-18 22:15:41</td>\n",
       "      <td>2025-10-18 18:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Чипсы картофельные Lays \"Оливье с перепелкой\"</td>\n",
       "      <td>Olga Bogdanova</td>\n",
       "      <td>4</td>\n",
       "      <td>17.10.2025</td>\n",
       "      <td>18:14</td>\n",
       "      <td>В пачках Lay's запахло Новым годом🎄⛄, пробую н...</td>\n",
       "      <td>Приветствую всех На улицах ещё не закончился з...</td>\n",
       "      <td>Приветствую всех\\n👋\\nНа улицах ещё не закончил...</td>\n",
       "      <td>149 руб.</td>\n",
       "      <td>В меру соленые | Есть схожесть во вкусе с олив...</td>\n",
       "      <td>Мало мясной ароматики | Необычный вкус</td>\n",
       "      <td>рекомендует</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://irecommend.ru/content/v-pachkakh-lays-...</td>\n",
       "      <td>2025-10-18 22:15:41</td>\n",
       "      <td>2025-10-17 18:14:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name          author  rating  \\\n",
       "0  Чипсы картофельные Twister Колбаски гриль с го...     Санду Мадан       3   \n",
       "1      Чипсы картофельные Lays \"Оливье с перепелкой\"  Olga Bogdanova       4   \n",
       "\n",
       "  date_created time_created  \\\n",
       "0   18.10.2025        18:04   \n",
       "1   17.10.2025        18:14   \n",
       "\n",
       "                                               title  \\\n",
       "0                                 Перебор с остротой   \n",
       "1  В пачках Lay's запахло Новым годом🎄⛄, пробую н...   \n",
       "\n",
       "                                         teaser_text  \\\n",
       "0  Я ел эти чипсы очень долго, еще года 2 назад с...   \n",
       "1  Приветствую всех На улицах ещё не закончился з...   \n",
       "\n",
       "                                           full_text     experience  \\\n",
       "0  Я ел эти чипсы очень долго, еще года 2 назад с...  год или более   \n",
       "1  Приветствую всех\\n👋\\nНа улицах ещё не закончил...       149 руб.   \n",
       "\n",
       "                                              pluses  \\\n",
       "0                                          Стоимость   \n",
       "1  В меру соленые | Есть схожесть во вкусе с олив...   \n",
       "\n",
       "                                  minuses         verdict  detailed_rating  \\\n",
       "0   Слишком острые | Химозное послевкусие  не рекомендует              3.0   \n",
       "1  Мало мясной ароматики | Необычный вкус     рекомендует              4.0   \n",
       "\n",
       "                                          review_url           scraped_at  \\\n",
       "0   https://irecommend.ru/content/perebor-s-ostrotoi  2025-10-18 22:15:41   \n",
       "1  https://irecommend.ru/content/v-pachkakh-lays-...  2025-10-18 22:15:41   \n",
       "\n",
       "     combined_created  \n",
       "0 2025-10-18 18:04:00  \n",
       "1 2025-10-17 18:14:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine('postgresql+psycopg2://airflow:airflow@localhost:5433/airflow')\n",
    "\n",
    "df['combined_created'] = pd.to_datetime(df['date_created'] + ' ' + df['time_created'])\n",
    "display(df.head(2))\n",
    "\n",
    "# Now use the engine with to_sql\n",
    "df.to_sql(\n",
    "    name='reviews',\n",
    "    schema='parser', \n",
    "    con=engine,\n",
    "    if_exists='append',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "112c7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql(sql = \"select max(combined_created) from parser.reviews\", con = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1f25559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-18 18:04:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  max\n",
       "0 2025-10-18 18:04:00"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d19cdae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-10-18 18:04:00')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df1['max'].iloc[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faeb6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ef46c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 23:38:06,763 - INFO - 🎯 Запуск парсера iRecommend через кэш\n",
      "2025-10-19 23:38:06,784 - INFO - 🚀 Сбор данных со страницы: https://irecommend.ru/catalog/reviews/939-13393?page=93\n",
      "2025-10-19 23:38:21,402 - ERROR - ❌ Не удалось получить данные со страницы: https://irecommend.ru/catalog/reviews/939-13393?page=93\n",
      "2025-10-19 23:38:21,410 - ERROR - ❌ Не удалось собрать данные\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import logging\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('irecommend_parser.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class IRecommendCacheParser:\n",
    "    def __init__(self, base_url=\"https://irecommend.ru\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.setup_session()\n",
    "        \n",
    "    def setup_session(self):\n",
    "        \"\"\"Настройка сессии\"\"\"\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        })\n",
    "\n",
    "    def get_through_cached_services(self, url):\n",
    "        \"\"\"Получение страницы через кэшированные сервисы\"\"\"\n",
    "        cached_services = [\n",
    "            self._try_google_cache,\n",
    "            self._try_archive_org,\n",
    "        ]\n",
    "        \n",
    "        for service in cached_services:\n",
    "            html = service(url)\n",
    "            if html:\n",
    "                return html\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _try_google_cache(self, url):\n",
    "        \"\"\"Попытка через Google Cache\"\"\"\n",
    "        try:\n",
    "            cache_url = f\"https://webcache.googleusercontent.com/search?q=cache:{url}\"\n",
    "            response = self.session.get(cache_url, timeout=15)\n",
    "            if response.status_code == 200 and self._validate_content(response.text):\n",
    "                logging.info(\"✅ Успех через Google Cache\")\n",
    "                return response.text\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Google Cache не сработал: {e}\")\n",
    "        return None\n",
    "\n",
    "    def _try_archive_org(self, url):\n",
    "        \"\"\"Попытка через Archive.org\"\"\"\n",
    "        try:\n",
    "            # Пробуем разные даты для поиска актуальных данных\n",
    "            dates = [\"20241019\", \"20241018\", \"20241015\", \"20241010\", \"20241001\"]\n",
    "            for date in dates:\n",
    "                archive_url = f\"https://web.archive.org/web/{date}/{url}\"\n",
    "                response = self.session.get(archive_url, timeout=15)\n",
    "                if response.status_code == 200 and self._validate_content(response.text):\n",
    "                    logging.info(f\"✅ Успех через Archive.org ({date})\")\n",
    "                    return response.text\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Archive.org не сработал: {e}\")\n",
    "        return None\n",
    "\n",
    "    def _validate_content(self, html):\n",
    "        \"\"\"Проверка что контент содержит отзывы\"\"\"\n",
    "        required_elements = ['smTeaser', 'productName', 'reviewTitle']\n",
    "        return any(element in html for element in required_elements)\n",
    "\n",
    "    def parse_reviews_list(self, html):\n",
    "        \"\"\"Парсинг списка отзывов\"\"\"\n",
    "        if not html:\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        reviews = []\n",
    "        \n",
    "        # Ищем блоки с отзывами\n",
    "        review_blocks = soup.find_all('div', class_='smTeaser')\n",
    "        \n",
    "        for block in review_blocks:\n",
    "            review_data = self.parse_review_block(block)\n",
    "            if review_data:\n",
    "                reviews.append(review_data)\n",
    "        \n",
    "        logging.info(f\"📝 Найдено отзывов: {len(reviews)}\")\n",
    "        return reviews\n",
    "\n",
    "    def parse_review_block(self, block):\n",
    "        \"\"\"Парсинг одного блока отзыва\"\"\"\n",
    "        try:\n",
    "            # Название продукта\n",
    "            product_elem = block.find('div', class_='productName')\n",
    "            product_name = product_elem.get_text(strip=True) if product_elem else \"Неизвестный продукт\"\n",
    "            \n",
    "            # Автор\n",
    "            author_elem = block.find('div', class_='authorName')\n",
    "            author_name = author_elem.get_text(strip=True) if author_elem else \"Аноним\"\n",
    "            \n",
    "            # Рейтинг\n",
    "            rating = self.extract_rating_from_block(block)\n",
    "            \n",
    "            # Дата и время\n",
    "            date_elem = block.find('span', class_='date-created')\n",
    "            time_elem = block.find('span', class_='time-created')\n",
    "            date_created = date_elem.get_text(strip=True) if date_elem else \"\"\n",
    "            time_created = time_elem.get_text(strip=True) if time_elem else \"\"\n",
    "            \n",
    "            # Заголовок\n",
    "            title_elem = block.find('div', class_='reviewTitle')\n",
    "            title = title_elem.get_text(strip=True) if title_elem else \"\"\n",
    "            \n",
    "            # Текст превью\n",
    "            teaser_elem = block.find('span', class_='reviewTeaserText')\n",
    "            teaser_text = teaser_elem.get_text(strip=True) if teaser_elem else \"\"\n",
    "            \n",
    "            # Ссылка на полный отзыв\n",
    "            link_elem = block.find('a', class_='reviewTextSnippet')\n",
    "            review_url = urljoin(self.base_url, link_elem['href']) if link_elem and link_elem.get('href') else \"\"\n",
    "            \n",
    "            return {\n",
    "                'product_name': product_name,\n",
    "                'author': author_name,\n",
    "                'rating': rating,\n",
    "                'date_created': date_created,\n",
    "                'time_created': time_created,\n",
    "                'title': title,\n",
    "                'teaser_text': teaser_text,\n",
    "                'review_url': review_url,\n",
    "                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка парсинга блока: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_rating_from_block(self, block):\n",
    "        \"\"\"Извлечение рейтинга\"\"\"\n",
    "        try:\n",
    "            rating_elem = block.find('div', class_='starsRating')\n",
    "            if rating_elem:\n",
    "                # Ищем класс с рейтингом в формате fivestarWidgetStatic-X\n",
    "                for cls in rating_elem.get('class', []):\n",
    "                    if 'fivestarWidgetStatic-' in cls:\n",
    "                        return cls.split('-')[-1]\n",
    "                \n",
    "                # Альтернативный метод: считаем заполненные звезды\n",
    "                stars = rating_elem.find_all('div', class_='star')\n",
    "                filled_stars = sum(1 for star in stars if star.find('div', class_='on'))\n",
    "                return str(filled_stars)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Ошибка извлечения рейтинга: {e}\")\n",
    "        return \"0\"\n",
    "\n",
    "    def parse_full_review(self, url):\n",
    "        \"\"\"Парсинг полного отзыва\"\"\"\n",
    "        if not url:\n",
    "            return {}\n",
    "            \n",
    "        logging.info(f\"🔍 Парсим полный отзыв: {url}\")\n",
    "        html = self.get_through_cached_services(url)\n",
    "        \n",
    "        if not html:\n",
    "            logging.warning(f\"Не удалось загрузить полный отзыв: {url}\")\n",
    "            return {}\n",
    "            \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        review_data = {}\n",
    "        \n",
    "        try:\n",
    "            review_block = soup.find('div', class_='reviewBlock')\n",
    "            if not review_block:\n",
    "                return {}\n",
    "            \n",
    "            # Полный текст отзыва\n",
    "            review_body = review_block.find('div', itemprop='reviewBody')\n",
    "            if review_body:\n",
    "                review_data['full_text'] = self.clean_text(review_body)\n",
    "            else:\n",
    "                review_data['full_text'] = \"\"\n",
    "            \n",
    "            # Опыт использования (стоимость и т.д.)\n",
    "            experience_data = self.extract_experience_info(review_block)\n",
    "            review_data['experience'] = experience_data\n",
    "            \n",
    "            # Достоинства\n",
    "            pluses = self.extract_pluses(review_block)\n",
    "            review_data['pluses'] = ' | '.join(pluses) if pluses else \"\"\n",
    "            \n",
    "            # Недостатки\n",
    "            minuses = self.extract_minuses(review_block)\n",
    "            review_data['minuses'] = ' | '.join(minuses) if minuses else \"\"\n",
    "            \n",
    "            # Вердикт\n",
    "            verdict = self.extract_verdict(review_block)\n",
    "            review_data['verdict'] = verdict\n",
    "            \n",
    "            # Дополнительная информация\n",
    "            additional_info = self.extract_additional_info(review_block)\n",
    "            review_data.update(additional_info)\n",
    "            \n",
    "            logging.info(f\"✅ Полный отзыв: {len(review_data.get('full_text', ''))} символов\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка парсинга полного отзыва: {e}\")\n",
    "        \n",
    "        return review_data\n",
    "\n",
    "    def extract_experience_info(self, review_block):\n",
    "        \"\"\"Извлечение информации об опыте использования\"\"\"\n",
    "        try:\n",
    "            extra_info = review_block.find('div', class_='extraInfo')\n",
    "            if extra_info:\n",
    "                item_data = extra_info.find('div', class_='item-data')\n",
    "                if item_data:\n",
    "                    return item_data.get_text(strip=True)\n",
    "        except:\n",
    "            pass\n",
    "        return \"\"\n",
    "\n",
    "    def extract_pluses(self, review_block):\n",
    "        \"\"\"Извлечение достоинств\"\"\"\n",
    "        try:\n",
    "            plus_block = review_block.find('div', class_='plus')\n",
    "            if plus_block:\n",
    "                plus_items = plus_block.find_all('li')\n",
    "                return [item.get_text(strip=True) for item in plus_items]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    def extract_minuses(self, review_block):\n",
    "        \"\"\"Извлечение недостатков\"\"\"\n",
    "        try:\n",
    "            minus_block = review_block.find('div', class_='minus')\n",
    "            if minus_block:\n",
    "                minus_items = minus_block.find_all('li')\n",
    "                return [item.get_text(strip=True) for item in minus_items]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    def extract_verdict(self, review_block):\n",
    "        \"\"\"Извлечение вердикта\"\"\"\n",
    "        try:\n",
    "            conclusion = review_block.find('div', class_='conclusion')\n",
    "            if conclusion:\n",
    "                verdict_elem = conclusion.find('span', class_='verdict')\n",
    "                if verdict_elem:\n",
    "                    return verdict_elem.get_text(strip=True)\n",
    "        except:\n",
    "            pass\n",
    "        return \"\"\n",
    "\n",
    "    def extract_additional_info(self, review_block):\n",
    "        \"\"\"Извлечение дополнительной информации\"\"\"\n",
    "        info = {}\n",
    "        try:\n",
    "            # Дата публикации из полного отзыва\n",
    "            date_elem = review_block.find('span', class_='dtreviewed')\n",
    "            if date_elem:\n",
    "                info['full_date'] = date_elem.get_text(strip=True)\n",
    "            \n",
    "            # Рейтинг из полного отзыва (для проверки)\n",
    "            rating_meta = review_block.find('meta', itemprop='ratingValue')\n",
    "            if rating_meta:\n",
    "                info['rating_verified'] = rating_meta.get('content', '')\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return info\n",
    "\n",
    "    def clean_text(self, element):\n",
    "        \"\"\"Очистка текста\"\"\"\n",
    "        if not element:\n",
    "            return \"\"\n",
    "        \n",
    "        for br in element.find_all(\"br\"):\n",
    "            br.replace_with(\"\\n\")\n",
    "        \n",
    "        text = element.get_text(separator='\\n')\n",
    "        lines = [line.strip() for line in text.split('\\n')]\n",
    "        lines = [line for line in lines if line]\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def scrape_page(self, page_url, max_reviews=10, get_full_reviews=True):\n",
    "        \"\"\"Сбор отзывов с конкретной страницы\"\"\"\n",
    "        logging.info(f\"🚀 Сбор данных со страницы: {page_url}\")\n",
    "        \n",
    "        # Получаем страницу через кэш\n",
    "        html = self.get_through_cached_services(page_url)\n",
    "        \n",
    "        if not html:\n",
    "            logging.error(f\"❌ Не удалось получить данные со страницы: {page_url}\")\n",
    "            return []\n",
    "        \n",
    "        # Парсим список отзывов\n",
    "        preview_reviews = self.parse_reviews_list(html)\n",
    "        \n",
    "        if not preview_reviews:\n",
    "            logging.warning(f\"❌ На странице {page_url} не найдено отзывов\")\n",
    "            return []\n",
    "        \n",
    "        # Ограничиваем количество\n",
    "        preview_reviews = preview_reviews[:max_reviews]\n",
    "        \n",
    "        complete_reviews = []\n",
    "        successful_full = 0\n",
    "        \n",
    "        for i, preview in enumerate(preview_reviews, 1):\n",
    "            logging.info(f\"🔄 Обработка отзыва {i}/{len(preview_reviews)}: {preview['title'][:50]}...\")\n",
    "            \n",
    "            if get_full_reviews and preview['review_url']:\n",
    "                # Получаем полные данные отзыва\n",
    "                full_data = self.parse_full_review(preview['review_url'])\n",
    "                \n",
    "                if full_data:\n",
    "                    complete_review = {**preview, **full_data}\n",
    "                    complete_reviews.append(complete_review)\n",
    "                    successful_full += 1\n",
    "                else:\n",
    "                    # Если не удалось получить полные данные, используем превью\n",
    "                    preview['full_text'] = preview.get('teaser_text', '')\n",
    "                    complete_reviews.append(preview)\n",
    "            else:\n",
    "                # Используем только превью данные\n",
    "                preview['full_text'] = preview.get('teaser_text', '')\n",
    "                complete_reviews.append(preview)\n",
    "            \n",
    "            # Задержка между обработкой отзывов\n",
    "            if i < len(preview_reviews):\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "        \n",
    "        logging.info(f\"🎉 Сбор завершен! Полные отзывы: {successful_full}/{len(complete_reviews)}\")\n",
    "        return complete_reviews\n",
    "\n",
    "    def scrape_multiple_pages(self, base_url, pages, max_reviews_per_page=5):\n",
    "        \"\"\"Сбор отзывов с нескольких страниц\"\"\"\n",
    "        all_reviews = []\n",
    "        \n",
    "        for page in pages:\n",
    "            if page == 1:\n",
    "                page_url = base_url\n",
    "            else:\n",
    "                page_url = f\"{base_url}?page={page}\"\n",
    "            \n",
    "            logging.info(f\"📄 Обработка страницы {page}\")\n",
    "            \n",
    "            page_reviews = self.scrape_page(page_url, max_reviews_per_page, get_full_reviews=True)\n",
    "            all_reviews.extend(page_reviews)\n",
    "            \n",
    "            # Задержка между страницами\n",
    "            if page < pages[-1]:\n",
    "                time.sleep(random.uniform(5, 10))\n",
    "        \n",
    "        # Удаляем дубликаты (на случай если отзывы повторяются на разных страницах)\n",
    "        unique_reviews = self.remove_duplicates(all_reviews)\n",
    "        return unique_reviews\n",
    "\n",
    "    def remove_duplicates(self, reviews):\n",
    "        \"\"\"Удаление дубликатов отзывов\"\"\"\n",
    "        seen = set()\n",
    "        unique = []\n",
    "        \n",
    "        for review in reviews:\n",
    "            # Создаем уникальный ключ на основе заголовка и автора\n",
    "            key = (review['title'], review['author'])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique.append(review)\n",
    "        \n",
    "        return unique\n",
    "\n",
    "    def save_to_csv(self, reviews, filename):\n",
    "        \"\"\"Сохранение в CSV\"\"\"\n",
    "        if not reviews:\n",
    "            logging.error(\"Нет данных для сохранения\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            fieldnames = [\n",
    "                'product_name', 'author', 'rating', 'date_created', 'time_created',\n",
    "                'title', 'teaser_text', 'full_text', 'experience', 'pluses', \n",
    "                'minuses', 'verdict', 'full_date', 'rating_verified', 'review_url', \n",
    "                'scraped_at'\n",
    "            ]\n",
    "            \n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                \n",
    "                for review in reviews:\n",
    "                    row = {field: review.get(field, '') for field in fieldnames}\n",
    "                    writer.writerow(row)\n",
    "            \n",
    "            logging.info(f\"💾 Сохранено {len(reviews)} отзывов в {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка сохранения: {e}\")\n",
    "            return False\n",
    "\n",
    "# Функции для разных сценариев использования\n",
    "def scrape_single_page(page_number=1):\n",
    "    \"\"\"Сбор данных с одной страницы\"\"\"\n",
    "    parser = IRecommendCacheParser()\n",
    "    \n",
    "    if page_number == 1:\n",
    "        url = \"https://irecommend.ru/catalog/reviews/939-13393\"\n",
    "    else:\n",
    "        url = f\"https://irecommend.ru/catalog/reviews/939-13393?page={page_number}\"\n",
    "    \n",
    "    reviews = parser.scrape_page(url, max_reviews=10, get_full_reviews=True)\n",
    "    return reviews\n",
    "\n",
    "def scrape_multiple_pages(page_numbers):\n",
    "    \"\"\"Сбор данных с нескольких страниц\"\"\"\n",
    "    parser = IRecommendCacheParser()\n",
    "    \n",
    "    reviews = parser.scrape_multiple_pages(\n",
    "        base_url=\"https://irecommend.ru/catalog/reviews/939-13393\",\n",
    "        pages=page_numbers,\n",
    "        max_reviews_per_page=5\n",
    "    )\n",
    "    return reviews\n",
    "\n",
    "def main():\n",
    "    \"\"\"Тестирование разных сценариев\"\"\"\n",
    "    logging.info(\"🎯 Запуск парсера iRecommend через кэш\")\n",
    "    \n",
    "    # Сценарий 1: Одна страница\n",
    "    # reviews = scrape_single_page(page_number=1)\n",
    "    \n",
    "    # Сценарий 2: Несколько страниц\n",
    "    #reviews = scrape_multiple_pages(page_numbers=[1, 2, 3])\n",
    "    \n",
    "    # Сценарий 3: Конкретная страница (например, 99)\n",
    "    reviews = scrape_single_page(page_number=93)\n",
    "    \n",
    "    if reviews:\n",
    "        filename = f\"irecommend_reviews_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "        parser = IRecommendCacheParser()\n",
    "        success = parser.save_to_csv(reviews, filename)\n",
    "        \n",
    "        if success:\n",
    "            logging.info(f\"✅ Успешно собрано отзывов: {len(reviews)}\")\n",
    "            \n",
    "            # Статистика\n",
    "            full_text_count = sum(1 for r in reviews if r.get('full_text') and len(r['full_text']) > 500)\n",
    "            pluses_count = sum(1 for r in reviews if r.get('pluses'))\n",
    "            minuses_count = sum(1 for r in reviews if r.get('minuses'))\n",
    "            \n",
    "            logging.info(f\"📊 Статистика:\")\n",
    "            logging.info(f\"   - Полные тексты: {full_text_count}\")\n",
    "            logging.info(f\"   - С плюсами: {pluses_count}\") \n",
    "            logging.info(f\"   - С минусами: {minuses_count}\")\n",
    "            \n",
    "            if reviews:\n",
    "                sample = reviews[0]\n",
    "                logging.info(f\"📄 Пример отзыва: '{sample['title']}'\")\n",
    "        else:\n",
    "            logging.error(\"❌ Ошибка сохранения данных\")\n",
    "    else:\n",
    "        logging.error(\"❌ Не удалось собрать данные\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad76b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudscraper in c:\\users\\smart\\pycharmprojects\\nlp_and_mlops_course_work\\.venv\\lib\\site-packages (1.2.71)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement fp.free-proxy (from versions: none)\n",
      "ERROR: No matching distribution found for fp.free-proxy\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install cloudscraper fp.free-proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e30d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fdd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>experience</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>verdict</th>\n",
       "      <th>full_date</th>\n",
       "      <th>rating_verified</th>\n",
       "      <th>review_url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Чипсы картофельные Lays \"Оливье с перепелкой\"</td>\n",
       "      <td>гелла раньше пела</td>\n",
       "      <td>5</td>\n",
       "      <td>19.10.2025</td>\n",
       "      <td>21:47</td>\n",
       "      <td>Осенний Новый год часть вторая. Удалось ли Lay...</td>\n",
       "      <td>ВСЕМ ПРИВЕТЫ)   Не скажу, что часто ем чипсы, ...</td>\n",
       "      <td>ВСЕМ ПРИВЕТЫ)\\nНе скажу, что часто ем чипсы, е...</td>\n",
       "      <td>149 рублей</td>\n",
       "      <td>В меру соленые | Есть схожесть во вкусе с олив...</td>\n",
       "      <td>Вкус улетучился на завтра, словно прошлый год ...</td>\n",
       "      <td>рекомендует</td>\n",
       "      <td>19 Октябрь, 2025 - 21:47</td>\n",
       "      <td>5</td>\n",
       "      <td>https://irecommend.ru/content/osennii-novyi-go...</td>\n",
       "      <td>2025-10-19 23:02:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    product_name             author  rating  \\\n",
       "0  Чипсы картофельные Lays \"Оливье с перепелкой\"  гелла раньше пела       5   \n",
       "\n",
       "  date_created time_created  \\\n",
       "0   19.10.2025        21:47   \n",
       "\n",
       "                                               title  \\\n",
       "0  Осенний Новый год часть вторая. Удалось ли Lay...   \n",
       "\n",
       "                                         teaser_text  \\\n",
       "0  ВСЕМ ПРИВЕТЫ)   Не скажу, что часто ем чипсы, ...   \n",
       "\n",
       "                                           full_text  experience  \\\n",
       "0  ВСЕМ ПРИВЕТЫ)\\nНе скажу, что часто ем чипсы, е...  149 рублей   \n",
       "\n",
       "                                              pluses  \\\n",
       "0  В меру соленые | Есть схожесть во вкусе с олив...   \n",
       "\n",
       "                                             minuses      verdict  \\\n",
       "0  Вкус улетучился на завтра, словно прошлый год ...  рекомендует   \n",
       "\n",
       "                  full_date  rating_verified  \\\n",
       "0  19 Октябрь, 2025 - 21:47                5   \n",
       "\n",
       "                                          review_url           scraped_at  \n",
       "0  https://irecommend.ru/content/osennii-novyi-go...  2025-10-19 23:02:35  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('irecommend_reviews_20251019_2303.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd2ea261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>title</th>\n",
       "      <th>teaser_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>experience</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>verdict</th>\n",
       "      <th>full_date</th>\n",
       "      <th>rating_verified</th>\n",
       "      <th>review_url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Чипсы картофельные Lava Lava Крабо-ниндзя</td>\n",
       "      <td>XeniumX</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Так ли хороши чипсы от бумажного блогера, как ...</td>\n",
       "      <td>Всем привет! Я думаю, многие родители, у кого ...</td>\n",
       "      <td>Всем привет! Я думаю, многие родители, у кого ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Чипсы картофельные Lorenz Crunchips X-Cut смет...</td>\n",
       "      <td>katerina_5512</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Очень качественные чипсы по невысокой цене!</td>\n",
       "      <td>Чипсы картофельные рифленые «Crunchips X-Cut» ...</td>\n",
       "      <td>Чипсы картофельные рифленые «Crunchips X-Cut» ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Чипсы картофельные Lays Моцарелла с песто</td>\n",
       "      <td>katerina_5512</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Вкусы чипсов с каждым разом становятся менее б...</td>\n",
       "      <td>Чипсы из натурального картофеля Lays со вкусом...</td>\n",
       "      <td>Чипсы из натурального картофеля Lays со вкусом...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Чипсы TWISTER Сыр</td>\n",
       "      <td>Екатерина1703niz</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хрустящие и легкие сырные чипсы🧀</td>\n",
       "      <td>Всем привет! Иногда очень хочется чем-нибудь п...</td>\n",
       "      <td>Всем привет! Иногда очень хочется чем-нибудь п...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Чипсы картофельные Lays Рифленые Острые крылышки</td>\n",
       "      <td>katerina_5512</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Насколько удачна новинка?</td>\n",
       "      <td>Чипсы из натурального картофеля Lays рифленые ...</td>\n",
       "      <td>Чипсы из натурального картофеля Lays рифленые ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-19 23:34:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name            author  \\\n",
       "0          Чипсы картофельные Lava Lava Крабо-ниндзя           XeniumX   \n",
       "1  Чипсы картофельные Lorenz Crunchips X-Cut смет...     katerina_5512   \n",
       "2          Чипсы картофельные Lays Моцарелла с песто     katerina_5512   \n",
       "3                                  Чипсы TWISTER Сыр  Екатерина1703niz   \n",
       "4   Чипсы картофельные Lays Рифленые Острые крылышки     katerina_5512   \n",
       "\n",
       "   rating  date_created  time_created  \\\n",
       "0       3           NaN           NaN   \n",
       "1       5           NaN           NaN   \n",
       "2       5           NaN           NaN   \n",
       "3       5           NaN           NaN   \n",
       "4       5           NaN           NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0  Так ли хороши чипсы от бумажного блогера, как ...   \n",
       "1        Очень качественные чипсы по невысокой цене!   \n",
       "2  Вкусы чипсов с каждым разом становятся менее б...   \n",
       "3                   Хрустящие и легкие сырные чипсы🧀   \n",
       "4                          Насколько удачна новинка?   \n",
       "\n",
       "                                         teaser_text  \\\n",
       "0  Всем привет! Я думаю, многие родители, у кого ...   \n",
       "1  Чипсы картофельные рифленые «Crunchips X-Cut» ...   \n",
       "2  Чипсы из натурального картофеля Lays со вкусом...   \n",
       "3  Всем привет! Иногда очень хочется чем-нибудь п...   \n",
       "4  Чипсы из натурального картофеля Lays рифленые ...   \n",
       "\n",
       "                                           full_text  experience  pluses  \\\n",
       "0  Всем привет! Я думаю, многие родители, у кого ...         NaN     NaN   \n",
       "1  Чипсы картофельные рифленые «Crunchips X-Cut» ...         NaN     NaN   \n",
       "2  Чипсы из натурального картофеля Lays со вкусом...         NaN     NaN   \n",
       "3  Всем привет! Иногда очень хочется чем-нибудь п...         NaN     NaN   \n",
       "4  Чипсы из натурального картофеля Lays рифленые ...         NaN     NaN   \n",
       "\n",
       "   minuses  verdict  full_date  rating_verified  review_url  \\\n",
       "0      NaN      NaN        NaN              NaN         NaN   \n",
       "1      NaN      NaN        NaN              NaN         NaN   \n",
       "2      NaN      NaN        NaN              NaN         NaN   \n",
       "3      NaN      NaN        NaN              NaN         NaN   \n",
       "4      NaN      NaN        NaN              NaN         NaN   \n",
       "\n",
       "            scraped_at  \n",
       "0  2025-10-19 23:34:35  \n",
       "1  2025-10-19 23:34:35  \n",
       "2  2025-10-19 23:34:35  \n",
       "3  2025-10-19 23:34:35  \n",
       "4  2025-10-19 23:34:35  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Всем привет! Иногда очень хочется чем-нибудь похрустеть, чипсами какими-нибудь вкусными Последнее приобретение это чипсы - TWISTER со вкусом сыра от Московского картофеля.Продаются данные чипсы во всех супермаркетах и обычных магазинах, цена за упаковку 70 грамм в районе 60 рублей.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('irecommend_reviews_20251019_2335.csv')\n",
    "display(df)\n",
    "df['full_text'][3]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
